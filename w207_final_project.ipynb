{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project\n",
    "## Jacky Wong, Renjun Cheng\n",
    "\n",
    "### This code run on python 3.8.3 and tensorflow 2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and import following package before runing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow pandas numpy matplotlib yahoo_fin sklearn requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "        \n",
    "        \n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "        \n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    \n",
    "    \n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])       \n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence)\n",
    "    result['last_sequence'] = last_sequence\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                               test_size=test_size, shuffle=shuffle) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            \n",
    "            # first layer\n",
    "            model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            \n",
    "            # last layer\n",
    "            model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            \n",
    "            # hidden layers\n",
    "            model.add(cell(units, return_sequences=True))\n",
    "            \n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # retrieve the column scalers\n",
    "    column_scaler = data[\"column_scaler\"]\n",
    "    # reshape the last sequence\n",
    "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph for predicted price vs real price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(model, data, time_period = 1000):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    plt.plot(y_test[-time_period:], c='b')\n",
    "    plt.plot(y_pred[-time_period:], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
    "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest function to get return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    money = 1\n",
    "    moneylist = []\n",
    "    long_only = 1\n",
    "    long_only_list = []\n",
    "    for i in range(1,len(y_test)):\n",
    "        if y_pred[i] >= y_pred[i-1]:\n",
    "            long_short = 1\n",
    "        else:\n",
    "            long_short = -1\n",
    "        money =  money * (1 + (y_test[i] - y_test[i-1])/y_test[i-1] * long_short) \n",
    "        long_only = long_only * (1 + (y_test[i] - y_test[i-1])/y_test[i-1] * 1) \n",
    "        moneylist.append(money)\n",
    "        long_only_list.append(long_only)\n",
    "    plt.plot(moneylist)\n",
    "    plt.plot(long_only_list)\n",
    "    return money,long_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Nasdaq Index (QQQ) price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which stock to predict\n",
    "ticker = \"QQQ\"\n",
    "\n",
    "# use numbers of previous days to predict\n",
    "N_STEPS = 100\n",
    "\n",
    "# which day to predict, 1 is for next day, 10 is for 10 days after today\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.5\n",
    "\n",
    "### training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 500\n",
    "\n",
    "### other parameters\n",
    "TEST_SIZE = 0.2\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save data to csv locally\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 2/86 [..............................] - ETA: 8s - loss: 0.0149 - mean_absolute_error: 0.1082WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0197s vs `on_train_batch_end` time: 0.1736s). Check your callbacks.\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.0024 - mean_absolute_error: 0.0408\n",
      "Epoch 00001: val_loss improved from inf to 0.00055, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 3s 39ms/step - loss: 0.0024 - mean_absolute_error: 0.0407 - val_loss: 5.5018e-04 - val_mean_absolute_error: 0.0202\n",
      "Epoch 2/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0296\n",
      "Epoch 00002: val_loss did not improve from 0.00055\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 0.0011 - mean_absolute_error: 0.0296 - val_loss: 7.8614e-04 - val_mean_absolute_error: 0.0292\n",
      "Epoch 3/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 9.3492e-04 - mean_absolute_error: 0.0278\n",
      "Epoch 00003: val_loss improved from 0.00055 to 0.00031, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 9.3098e-04 - mean_absolute_error: 0.0278 - val_loss: 3.0985e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 4/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 9.5419e-04 - mean_absolute_error: 0.0278\n",
      "Epoch 00004: val_loss did not improve from 0.00031\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 9.5870e-04 - mean_absolute_error: 0.0279 - val_loss: 3.7746e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 5/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 9.1843e-04 - mean_absolute_error: 0.0268\n",
      "Epoch 00005: val_loss improved from 0.00031 to 0.00028, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 9.1595e-04 - mean_absolute_error: 0.0268 - val_loss: 2.7733e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 6/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 8.4516e-04 - mean_absolute_error: 0.0264\n",
      "Epoch 00006: val_loss did not improve from 0.00028\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 8.4456e-04 - mean_absolute_error: 0.0263 - val_loss: 3.9719e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 7/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 9.0796e-04 - mean_absolute_error: 0.0269\n",
      "Epoch 00007: val_loss did not improve from 0.00028\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 9.0796e-04 - mean_absolute_error: 0.0269 - val_loss: 2.8242e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 8/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 6.9654e-04 - mean_absolute_error: 0.0241\n",
      "Epoch 00008: val_loss did not improve from 0.00028\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 6.9878e-04 - mean_absolute_error: 0.0241 - val_loss: 3.2855e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 9/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 6.8997e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00009: val_loss improved from 0.00028 to 0.00026, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 28ms/step - loss: 6.8880e-04 - mean_absolute_error: 0.0231 - val_loss: 2.5543e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 10/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 6.3903e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00010: val_loss did not improve from 0.00026\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 6.4767e-04 - mean_absolute_error: 0.0222 - val_loss: 3.7478e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 11/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 6.5381e-04 - mean_absolute_error: 0.0222\n",
      "Epoch 00011: val_loss did not improve from 0.00026\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 6.6089e-04 - mean_absolute_error: 0.0223 - val_loss: 3.7428e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 12/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 5.7679e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00012: val_loss improved from 0.00026 to 0.00017, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 5.7426e-04 - mean_absolute_error: 0.0209 - val_loss: 1.7495e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 13/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 5.9466e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00013: val_loss did not improve from 0.00017\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 5.9126e-04 - mean_absolute_error: 0.0216 - val_loss: 3.0591e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 14/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 5.3148e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00014: val_loss improved from 0.00017 to 0.00017, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 5.2889e-04 - mean_absolute_error: 0.0204 - val_loss: 1.7303e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 15/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 4.8308e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00015: val_loss improved from 0.00017 to 0.00016, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 4.8308e-04 - mean_absolute_error: 0.0197 - val_loss: 1.5846e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 16/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 5.6559e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00016: val_loss did not improve from 0.00016\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 5.6646e-04 - mean_absolute_error: 0.0210 - val_loss: 1.8355e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 17/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 5.1333e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00017: val_loss did not improve from 0.00016\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 5.1333e-04 - mean_absolute_error: 0.0202 - val_loss: 3.1913e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 18/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 4.6738e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00018: val_loss improved from 0.00016 to 0.00012, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 4.6738e-04 - mean_absolute_error: 0.0193 - val_loss: 1.2205e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 19/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 4.2202e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00019: val_loss improved from 0.00012 to 0.00009, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 4.2210e-04 - mean_absolute_error: 0.0181 - val_loss: 9.0909e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 20/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.8856e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00020: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 3.8744e-04 - mean_absolute_error: 0.0176 - val_loss: 9.2347e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 21/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.8569e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00021: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 3.8320e-04 - mean_absolute_error: 0.0173 - val_loss: 9.3066e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 22/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.3553e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00022: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 3.3851e-04 - mean_absolute_error: 0.0168 - val_loss: 1.0628e-04 - val_mean_absolute_error: 0.0113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 4.3161e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00023: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 4.2929e-04 - mean_absolute_error: 0.0190 - val_loss: 1.8430e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 24/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.4663e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00024: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 3.4843e-04 - mean_absolute_error: 0.0165 - val_loss: 1.7318e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 25/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.4863e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00025: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.4824e-04 - mean_absolute_error: 0.0169 - val_loss: 1.5027e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 26/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 4.1188e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00026: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 4.1218e-04 - mean_absolute_error: 0.0183 - val_loss: 1.5299e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 27/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 3.7417e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00027: val_loss improved from 0.00009 to 0.00009, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 3.7417e-04 - mean_absolute_error: 0.0171 - val_loss: 8.9405e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 28/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.5117e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00028: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 3.4860e-04 - mean_absolute_error: 0.0170 - val_loss: 1.0706e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 29/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.5280e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00029: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 3.5263e-04 - mean_absolute_error: 0.0168 - val_loss: 1.0195e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 30/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.5705e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00030: val_loss did not improve from 0.00009\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 3.5745e-04 - mean_absolute_error: 0.0171 - val_loss: 2.9580e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 31/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.5988e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00031: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.6835e-04 - mean_absolute_error: 0.0175 - val_loss: 7.8038e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 32/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.3192e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00032: val_loss did not improve from 0.00008\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.3161e-04 - mean_absolute_error: 0.0166 - val_loss: 1.2524e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 33/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 3.5175e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00033: val_loss improved from 0.00008 to 0.00006, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.5175e-04 - mean_absolute_error: 0.0177 - val_loss: 5.5133e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 34/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.1330e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00034: val_loss did not improve from 0.00006\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 3.1467e-04 - mean_absolute_error: 0.0164 - val_loss: 8.3513e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 35/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.6188e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00035: val_loss did not improve from 0.00006\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 3.6005e-04 - mean_absolute_error: 0.0172 - val_loss: 7.1213e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 36/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.1317e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00036: val_loss did not improve from 0.00006\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 3.1591e-04 - mean_absolute_error: 0.0160 - val_loss: 8.6621e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 37/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.1466e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00037: val_loss did not improve from 0.00006\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 3.1429e-04 - mean_absolute_error: 0.0160 - val_loss: 1.3547e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 38/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 3.1263e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00038: val_loss did not improve from 0.00006\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 3.1436e-04 - mean_absolute_error: 0.0168 - val_loss: 9.0747e-05 - val_mean_absolute_error: 0.0110\n",
      "Epoch 39/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.7215e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00039: val_loss did not improve from 0.00006\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.7171e-04 - mean_absolute_error: 0.0154 - val_loss: 5.5424e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 40/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.6899e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00040: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.6789e-04 - mean_absolute_error: 0.0151 - val_loss: 4.9973e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 41/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.9969e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00041: val_loss did not improve from 0.00005\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.9814e-04 - mean_absolute_error: 0.0159 - val_loss: 7.3918e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 42/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 3.0129e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00042: val_loss did not improve from 0.00005\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 3.0129e-04 - mean_absolute_error: 0.0162 - val_loss: 1.0403e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 43/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.7114e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00043: val_loss did not improve from 0.00005\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.7204e-04 - mean_absolute_error: 0.0154 - val_loss: 8.7490e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 44/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.5667e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00044: val_loss did not improve from 0.00005\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.5674e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2210e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 45/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 3.0269e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00045: val_loss did not improve from 0.00005\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 3.0269e-04 - mean_absolute_error: 0.0160 - val_loss: 7.2541e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 46/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 3.5092e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00046: val_loss did not improve from 0.00005\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 3.5092e-04 - mean_absolute_error: 0.0168 - val_loss: 9.6077e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - ETA: 0s - loss: 2.8942e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00047: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.8942e-04 - mean_absolute_error: 0.0162 - val_loss: 4.2897e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 48/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.5634e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00048: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.5707e-04 - mean_absolute_error: 0.0151 - val_loss: 6.8176e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 49/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 3.0860e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00049: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.0676e-04 - mean_absolute_error: 0.0165 - val_loss: 9.5573e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 50/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.7517e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00050: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.7517e-04 - mean_absolute_error: 0.0156 - val_loss: 9.9229e-05 - val_mean_absolute_error: 0.0110\n",
      "Epoch 51/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 3.0643e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00051: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.0534e-04 - mean_absolute_error: 0.0160 - val_loss: 4.2054e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 52/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.7481e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00052: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.7434e-04 - mean_absolute_error: 0.0153 - val_loss: 5.7686e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 53/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.9624e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00053: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.9445e-04 - mean_absolute_error: 0.0161 - val_loss: 5.3336e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 54/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.5934e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00054: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.6134e-04 - mean_absolute_error: 0.0151 - val_loss: 1.8823e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 55/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 3.2927e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00055: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 3.2927e-04 - mean_absolute_error: 0.0174 - val_loss: 1.0202e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 56/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.5302e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00056: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.5149e-04 - mean_absolute_error: 0.0149 - val_loss: 8.9494e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 57/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.3160e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00057: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3281e-04 - mean_absolute_error: 0.0148 - val_loss: 5.8946e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 58/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.3734e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00058: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3734e-04 - mean_absolute_error: 0.0144 - val_loss: 6.1708e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 59/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4516e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00059: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.4363e-04 - mean_absolute_error: 0.0147 - val_loss: 8.0184e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 60/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4210e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00060: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.4054e-04 - mean_absolute_error: 0.0147 - val_loss: 6.1303e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 61/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.6211e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00061: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.6153e-04 - mean_absolute_error: 0.0156 - val_loss: 6.5058e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 62/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.7431e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00062: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.7431e-04 - mean_absolute_error: 0.0157 - val_loss: 7.5930e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 63/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.4667e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00063: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.5006e-04 - mean_absolute_error: 0.0151 - val_loss: 2.8856e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 64/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.8365e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00064: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.8450e-04 - mean_absolute_error: 0.0161 - val_loss: 5.0417e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 65/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.5608e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00065: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.5574e-04 - mean_absolute_error: 0.0151 - val_loss: 4.1184e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 66/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.4999e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00066: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.5051e-04 - mean_absolute_error: 0.0149 - val_loss: 6.6044e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 67/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4556e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00067: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.4355e-04 - mean_absolute_error: 0.0149 - val_loss: 8.0569e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 68/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.4632e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00068: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 2.4632e-04 - mean_absolute_error: 0.0148 - val_loss: 4.5078e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 69/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.5776e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00069: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 2.5672e-04 - mean_absolute_error: 0.0156 - val_loss: 1.3468e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 70/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.8406e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00070: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 2.8358e-04 - mean_absolute_error: 0.0159 - val_loss: 3.9492e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 71/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/86 [============================>.] - ETA: 0s - loss: 2.6894e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00071: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.6818e-04 - mean_absolute_error: 0.0154 - val_loss: 9.9365e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 72/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.6965e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00072: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 2.6896e-04 - mean_absolute_error: 0.0158 - val_loss: 2.0076e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 73/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.3406e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00073: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3406e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2714e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 74/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.3236e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00074: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3585e-04 - mean_absolute_error: 0.0146 - val_loss: 1.3127e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 75/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.8398e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00075: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.8326e-04 - mean_absolute_error: 0.0163 - val_loss: 1.1675e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 76/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 2.7153e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00076: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.7108e-04 - mean_absolute_error: 0.0153 - val_loss: 8.6896e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 77/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.5160e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00077: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.5376e-04 - mean_absolute_error: 0.0152 - val_loss: 4.5814e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 78/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2567e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00078: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2630e-04 - mean_absolute_error: 0.0144 - val_loss: 6.3201e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 79/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.7226e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00079: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.7226e-04 - mean_absolute_error: 0.0157 - val_loss: 1.4245e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 80/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.6165e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00080: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.6165e-04 - mean_absolute_error: 0.0157 - val_loss: 9.6991e-05 - val_mean_absolute_error: 0.0106\n",
      "Epoch 81/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.7954e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00081: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.7954e-04 - mean_absolute_error: 0.0159 - val_loss: 3.6246e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 82/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4225e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00082: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 2.4215e-04 - mean_absolute_error: 0.0148 - val_loss: 9.2002e-05 - val_mean_absolute_error: 0.0106\n",
      "Epoch 83/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4395e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00083: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.4355e-04 - mean_absolute_error: 0.0150 - val_loss: 5.7670e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 84/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.2624e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00084: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 3s 30ms/step - loss: 2.2966e-04 - mean_absolute_error: 0.0146 - val_loss: 3.7352e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 85/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1238e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00085: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2.1317e-04 - mean_absolute_error: 0.0142 - val_loss: 9.6432e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 86/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.7811e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00086: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 2.7573e-04 - mean_absolute_error: 0.0159 - val_loss: 7.6286e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 87/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.4034e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00087: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 3s 31ms/step - loss: 2.4034e-04 - mean_absolute_error: 0.0149 - val_loss: 5.0733e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 88/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.6618e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00088: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 29ms/step - loss: 2.6465e-04 - mean_absolute_error: 0.0157 - val_loss: 6.9844e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 89/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2744e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00089: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.2744e-04 - mean_absolute_error: 0.0145 - val_loss: 7.3507e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 90/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2749e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00090: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.2749e-04 - mean_absolute_error: 0.0147 - val_loss: 9.4140e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 91/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.2418e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00091: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.2342e-04 - mean_absolute_error: 0.0143 - val_loss: 7.2457e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 92/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2872e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00092: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.2796e-04 - mean_absolute_error: 0.0143 - val_loss: 2.3664e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 93/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.3231e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00093: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.3153e-04 - mean_absolute_error: 0.0148 - val_loss: 7.9012e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 94/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1590e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00094: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.1656e-04 - mean_absolute_error: 0.0140 - val_loss: 6.7527e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 95/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4679e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00095: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.4572e-04 - mean_absolute_error: 0.0152 - val_loss: 3.8755e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 96/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/86 [============================>.] - ETA: 0s - loss: 2.2954e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00096: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 2.2918e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2287e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 97/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.6824e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00097: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.6824e-04 - mean_absolute_error: 0.0157 - val_loss: 5.1412e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 98/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2033e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00098: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.2033e-04 - mean_absolute_error: 0.0142 - val_loss: 9.8957e-05 - val_mean_absolute_error: 0.0108\n",
      "Epoch 99/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.1895e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00099: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.1685e-04 - mean_absolute_error: 0.0145 - val_loss: 4.0253e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 100/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3206e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00100: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 2.3190e-04 - mean_absolute_error: 0.0149 - val_loss: 5.0136e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 101/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3617e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00101: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3357e-04 - mean_absolute_error: 0.0146 - val_loss: 4.5930e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 102/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3500e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00102: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.3530e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2236e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 103/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.1786e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00103: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1598e-04 - mean_absolute_error: 0.0146 - val_loss: 9.8846e-05 - val_mean_absolute_error: 0.0120\n",
      "Epoch 104/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1064e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00104: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.1064e-04 - mean_absolute_error: 0.0142 - val_loss: 6.9544e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 105/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.2721e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00105: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.2587e-04 - mean_absolute_error: 0.0142 - val_loss: 3.6565e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 106/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.5503e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00106: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.5503e-04 - mean_absolute_error: 0.0148 - val_loss: 1.1340e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 107/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.6741e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00107: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.6741e-04 - mean_absolute_error: 0.0158 - val_loss: 1.1211e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 108/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.6437e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00108: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.6287e-04 - mean_absolute_error: 0.0152 - val_loss: 5.6772e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 109/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4566e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00109: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.4340e-04 - mean_absolute_error: 0.0148 - val_loss: 6.4562e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 110/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2290e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00110: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.2500e-04 - mean_absolute_error: 0.0148 - val_loss: 6.6575e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 111/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2022e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00111: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.2022e-04 - mean_absolute_error: 0.0145 - val_loss: 4.5080e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 112/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.4073e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00112: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.4266e-04 - mean_absolute_error: 0.0152 - val_loss: 1.5968e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 113/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2924e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00113: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.2924e-04 - mean_absolute_error: 0.0146 - val_loss: 4.0379e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 114/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0389e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00114: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0304e-04 - mean_absolute_error: 0.0143 - val_loss: 3.7058e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 115/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3295e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00115: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.3422e-04 - mean_absolute_error: 0.0147 - val_loss: 8.2048e-05 - val_mean_absolute_error: 0.0099\n",
      "Epoch 116/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2204e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00116: val_loss did not improve from 0.00004\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.2204e-04 - mean_absolute_error: 0.0142 - val_loss: 5.4756e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 117/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0019e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00117: val_loss improved from 0.00004 to 0.00003, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 27ms/step - loss: 1.9988e-04 - mean_absolute_error: 0.0136 - val_loss: 3.2555e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 118/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0686e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00118: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0909e-04 - mean_absolute_error: 0.0141 - val_loss: 6.4034e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 119/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9297e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00119: val_loss improved from 0.00003 to 0.00003, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.9374e-04 - mean_absolute_error: 0.0136 - val_loss: 3.2095e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 120/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1713e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00120: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.1713e-04 - mean_absolute_error: 0.0138 - val_loss: 5.6862e-05 - val_mean_absolute_error: 0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1559e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00121: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1449e-04 - mean_absolute_error: 0.0142 - val_loss: 4.0602e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 122/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1238e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00122: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1170e-04 - mean_absolute_error: 0.0142 - val_loss: 3.9611e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 123/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.5517e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00123: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.5517e-04 - mean_absolute_error: 0.0154 - val_loss: 5.8669e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 124/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0825e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00124: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0862e-04 - mean_absolute_error: 0.0142 - val_loss: 5.4887e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 125/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0711e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00125: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.0554e-04 - mean_absolute_error: 0.0139 - val_loss: 4.7840e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 126/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 2.0112e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00126: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9967e-04 - mean_absolute_error: 0.0140 - val_loss: 3.8900e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 127/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0955e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00127: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0946e-04 - mean_absolute_error: 0.0139 - val_loss: 7.4321e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 128/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2319e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00128: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.2319e-04 - mean_absolute_error: 0.0142 - val_loss: 6.1823e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 129/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3219e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00129: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3349e-04 - mean_absolute_error: 0.0146 - val_loss: 1.5380e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 130/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9963e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00130: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9963e-04 - mean_absolute_error: 0.0139 - val_loss: 1.0886e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 131/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2179e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00131: val_loss improved from 0.00003 to 0.00003, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2082e-04 - mean_absolute_error: 0.0142 - val_loss: 2.7403e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 132/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0414e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00132: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0414e-04 - mean_absolute_error: 0.0137 - val_loss: 8.5397e-05 - val_mean_absolute_error: 0.0109\n",
      "Epoch 133/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0896e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00133: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0896e-04 - mean_absolute_error: 0.0140 - val_loss: 2.9513e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 134/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.7745e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00134: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.7745e-04 - mean_absolute_error: 0.0159 - val_loss: 4.8110e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 135/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1700e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00135: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 2.1620e-04 - mean_absolute_error: 0.0140 - val_loss: 3.6331e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 136/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.1107e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00136: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1150e-04 - mean_absolute_error: 0.0139 - val_loss: 3.0343e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 137/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0757e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00137: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0738e-04 - mean_absolute_error: 0.0138 - val_loss: 4.2402e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 138/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.2475e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00138: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.2443e-04 - mean_absolute_error: 0.0143 - val_loss: 4.1126e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 139/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2785e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00139: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.2916e-04 - mean_absolute_error: 0.0148 - val_loss: 4.7761e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 140/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2575e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00140: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.2552e-04 - mean_absolute_error: 0.0144 - val_loss: 1.0648e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 141/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.2260e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00141: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2201e-04 - mean_absolute_error: 0.0145 - val_loss: 5.2112e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 142/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0665e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00142: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0665e-04 - mean_absolute_error: 0.0138 - val_loss: 6.0656e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 143/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0311e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00143: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0311e-04 - mean_absolute_error: 0.0137 - val_loss: 3.4620e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 144/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.2672e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00144: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 2.2600e-04 - mean_absolute_error: 0.0145 - val_loss: 4.3672e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 145/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.2426e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00145: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2426e-04 - mean_absolute_error: 0.0144 - val_loss: 6.1999e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 146/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/86 [============================>.] - ETA: 0s - loss: 2.0267e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00146: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0281e-04 - mean_absolute_error: 0.0138 - val_loss: 3.6010e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 147/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1762e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00147: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1762e-04 - mean_absolute_error: 0.0143 - val_loss: 3.6793e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 148/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9172e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00148: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9172e-04 - mean_absolute_error: 0.0136 - val_loss: 9.3587e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 149/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.2126e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00149: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2171e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8117e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 150/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.5445e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00150: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.5526e-04 - mean_absolute_error: 0.0155 - val_loss: 4.2552e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 151/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.3174e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00151: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.3174e-04 - mean_absolute_error: 0.0149 - val_loss: 4.3520e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 152/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.3054e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00152: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2976e-04 - mean_absolute_error: 0.0145 - val_loss: 6.0351e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 153/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1676e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00153: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1717e-04 - mean_absolute_error: 0.0141 - val_loss: 4.1357e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 154/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0079e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00154: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0079e-04 - mean_absolute_error: 0.0136 - val_loss: 3.6503e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 155/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0123e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00155: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9990e-04 - mean_absolute_error: 0.0136 - val_loss: 5.4945e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 156/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0537e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00156: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0513e-04 - mean_absolute_error: 0.0138 - val_loss: 3.5053e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 157/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3293e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00157: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.3085e-04 - mean_absolute_error: 0.0146 - val_loss: 8.4583e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 158/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9807e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00158: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9721e-04 - mean_absolute_error: 0.0136 - val_loss: 5.0249e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 159/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1955e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00159: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1955e-04 - mean_absolute_error: 0.0141 - val_loss: 8.6608e-05 - val_mean_absolute_error: 0.0111\n",
      "Epoch 160/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.1961e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00160: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.2318e-04 - mean_absolute_error: 0.0148 - val_loss: 3.4566e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 161/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1562e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00161: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1562e-04 - mean_absolute_error: 0.0141 - val_loss: 5.1444e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 162/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1307e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00162: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.1170e-04 - mean_absolute_error: 0.0140 - val_loss: 1.1466e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 163/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.3241e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00163: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3241e-04 - mean_absolute_error: 0.0149 - val_loss: 6.7415e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 164/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.1831e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00164: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.2137e-04 - mean_absolute_error: 0.0141 - val_loss: 4.7916e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 165/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1011e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00165: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1011e-04 - mean_absolute_error: 0.0140 - val_loss: 4.4509e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 166/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9170e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00166: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9077e-04 - mean_absolute_error: 0.0134 - val_loss: 7.7816e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 167/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0488e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00167: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 2.0488e-04 - mean_absolute_error: 0.0140 - val_loss: 4.8608e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 168/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0230e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00168: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0230e-04 - mean_absolute_error: 0.0140 - val_loss: 6.8852e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 169/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7451e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00169: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7355e-04 - mean_absolute_error: 0.0129 - val_loss: 3.6698e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 170/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8665e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00170: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8631e-04 - mean_absolute_error: 0.0130 - val_loss: 3.9796e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 171/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1256e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00171: val_loss improved from 0.00003 to 0.00003, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1256e-04 - mean_absolute_error: 0.0139 - val_loss: 2.5882e-05 - val_mean_absolute_error: 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9890e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00172: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0040e-04 - mean_absolute_error: 0.0135 - val_loss: 7.6550e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 173/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9976e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00173: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9948e-04 - mean_absolute_error: 0.0135 - val_loss: 3.6835e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 174/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1035e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00174: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.1002e-04 - mean_absolute_error: 0.0138 - val_loss: 5.5041e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 175/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3671e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00175: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.3634e-04 - mean_absolute_error: 0.0148 - val_loss: 3.9543e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 176/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0933e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00176: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0834e-04 - mean_absolute_error: 0.0136 - val_loss: 5.5165e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 177/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0676e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00177: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0712e-04 - mean_absolute_error: 0.0134 - val_loss: 4.9547e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 178/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.1599e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00178: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1599e-04 - mean_absolute_error: 0.0139 - val_loss: 8.7397e-05 - val_mean_absolute_error: 0.0100\n",
      "Epoch 179/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0229e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00179: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0096e-04 - mean_absolute_error: 0.0137 - val_loss: 4.5668e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 180/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8082e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00180: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8032e-04 - mean_absolute_error: 0.0128 - val_loss: 3.5419e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 181/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9553e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00181: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.9556e-04 - mean_absolute_error: 0.0134 - val_loss: 7.6504e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 182/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0232e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00182: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0232e-04 - mean_absolute_error: 0.0137 - val_loss: 5.1536e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 183/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1503e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00183: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.1484e-04 - mean_absolute_error: 0.0143 - val_loss: 3.3234e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 184/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9166e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00184: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9061e-04 - mean_absolute_error: 0.0132 - val_loss: 4.1767e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 185/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9468e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00185: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.9468e-04 - mean_absolute_error: 0.0133 - val_loss: 7.3890e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 186/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0205e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00186: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0054e-04 - mean_absolute_error: 0.0138 - val_loss: 5.9189e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 187/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9017e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00187: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8982e-04 - mean_absolute_error: 0.0132 - val_loss: 5.4149e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 188/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9134e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00188: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9071e-04 - mean_absolute_error: 0.0132 - val_loss: 6.8418e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 189/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0868e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00189: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0868e-04 - mean_absolute_error: 0.0138 - val_loss: 5.6405e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 190/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9005e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00190: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8965e-04 - mean_absolute_error: 0.0133 - val_loss: 5.2011e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 191/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9978e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00191: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0028e-04 - mean_absolute_error: 0.0135 - val_loss: 3.4152e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 192/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9806e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00192: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9762e-04 - mean_absolute_error: 0.0131 - val_loss: 3.4627e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 193/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0370e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00193: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0285e-04 - mean_absolute_error: 0.0138 - val_loss: 4.7722e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 194/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0950e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00194: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1052e-04 - mean_absolute_error: 0.0135 - val_loss: 2.6385e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 195/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9915e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00195: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9877e-04 - mean_absolute_error: 0.0135 - val_loss: 7.8990e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 196/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0384e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00196: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0344e-04 - mean_absolute_error: 0.0139 - val_loss: 4.2088e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 197/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0222e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00197: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0134e-04 - mean_absolute_error: 0.0134 - val_loss: 3.9365e-05 - val_mean_absolute_error: 0.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.8307e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00198: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8307e-04 - mean_absolute_error: 0.0131 - val_loss: 4.2715e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 199/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9455e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00199: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9524e-04 - mean_absolute_error: 0.0137 - val_loss: 5.0616e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 200/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1466e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00200: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1388e-04 - mean_absolute_error: 0.0140 - val_loss: 6.3053e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 201/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0000e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00201: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.9884e-04 - mean_absolute_error: 0.0138 - val_loss: 7.8245e-05 - val_mean_absolute_error: 0.0098\n",
      "Epoch 202/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8899e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00202: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8922e-04 - mean_absolute_error: 0.0133 - val_loss: 1.2258e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 203/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0566e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00203: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0646e-04 - mean_absolute_error: 0.0140 - val_loss: 1.0453e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 204/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0848e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00204: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0884e-04 - mean_absolute_error: 0.0136 - val_loss: 4.7400e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 205/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9961e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00205: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9898e-04 - mean_absolute_error: 0.0136 - val_loss: 3.5316e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 206/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0452e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00206: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0452e-04 - mean_absolute_error: 0.0134 - val_loss: 4.5764e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 207/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9500e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00207: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9500e-04 - mean_absolute_error: 0.0135 - val_loss: 3.0495e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 208/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7346e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00208: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7236e-04 - mean_absolute_error: 0.0130 - val_loss: 4.6350e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 209/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0653e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00209: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0708e-04 - mean_absolute_error: 0.0136 - val_loss: 4.3114e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 210/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6542e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00210: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6801e-04 - mean_absolute_error: 0.0128 - val_loss: 6.0950e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 211/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0771e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00211: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0785e-04 - mean_absolute_error: 0.0139 - val_loss: 5.3637e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 212/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9035e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00212: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9035e-04 - mean_absolute_error: 0.0134 - val_loss: 9.6805e-05 - val_mean_absolute_error: 0.0120\n",
      "Epoch 213/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0000e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00213: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9880e-04 - mean_absolute_error: 0.0134 - val_loss: 4.3073e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 214/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8153e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00214: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8347e-04 - mean_absolute_error: 0.0129 - val_loss: 2.9620e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 215/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9124e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00215: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9111e-04 - mean_absolute_error: 0.0132 - val_loss: 4.5887e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 216/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1143e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00216: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.1126e-04 - mean_absolute_error: 0.0139 - val_loss: 7.7979e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 217/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9082e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00217: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.9068e-04 - mean_absolute_error: 0.0134 - val_loss: 4.0703e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 218/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7317e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00218: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7317e-04 - mean_absolute_error: 0.0126 - val_loss: 3.2028e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 219/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9774e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00219: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9816e-04 - mean_absolute_error: 0.0135 - val_loss: 6.4422e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 220/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0467e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00220: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0467e-04 - mean_absolute_error: 0.0141 - val_loss: 7.9328e-05 - val_mean_absolute_error: 0.0108\n",
      "Epoch 221/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 2.0076e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00221: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0087e-04 - mean_absolute_error: 0.0137 - val_loss: 7.3041e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 222/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0034e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00222: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0034e-04 - mean_absolute_error: 0.0136 - val_loss: 9.2213e-05 - val_mean_absolute_error: 0.0104\n",
      "Epoch 223/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7436e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00223: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7351e-04 - mean_absolute_error: 0.0129 - val_loss: 3.2912e-05 - val_mean_absolute_error: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.8501e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00224: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8501e-04 - mean_absolute_error: 0.0130 - val_loss: 4.3635e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 225/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0521e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00225: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0521e-04 - mean_absolute_error: 0.0136 - val_loss: 8.3590e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 226/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8301e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00226: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8260e-04 - mean_absolute_error: 0.0130 - val_loss: 4.7722e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 227/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.9381e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00227: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.9448e-04 - mean_absolute_error: 0.0132 - val_loss: 4.3182e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 228/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8924e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00228: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.8748e-04 - mean_absolute_error: 0.0132 - val_loss: 1.0096e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 229/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9103e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00229: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8945e-04 - mean_absolute_error: 0.0133 - val_loss: 4.3093e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 230/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0108e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00230: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0077e-04 - mean_absolute_error: 0.0134 - val_loss: 1.8531e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 231/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.9144e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00231: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.8930e-04 - mean_absolute_error: 0.0137 - val_loss: 6.5659e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 232/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9011e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00232: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.8711e-04 - mean_absolute_error: 0.0130 - val_loss: 2.7234e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 233/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7937e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00233: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7937e-04 - mean_absolute_error: 0.0127 - val_loss: 4.3298e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 234/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7901e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00234: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.7875e-04 - mean_absolute_error: 0.0127 - val_loss: 2.7550e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 235/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.3134e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00235: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 2.3041e-04 - mean_absolute_error: 0.0145 - val_loss: 7.7102e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 236/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7322e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00236: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.7179e-04 - mean_absolute_error: 0.0127 - val_loss: 4.4837e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 237/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8585e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00237: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.8539e-04 - mean_absolute_error: 0.0128 - val_loss: 3.7524e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 238/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 2.0983e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00238: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1150e-04 - mean_absolute_error: 0.0140 - val_loss: 6.3280e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 239/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8115e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00239: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8173e-04 - mean_absolute_error: 0.0131 - val_loss: 4.6525e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 240/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8448e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00240: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8516e-04 - mean_absolute_error: 0.0130 - val_loss: 3.2786e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 241/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6781e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00241: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6781e-04 - mean_absolute_error: 0.0125 - val_loss: 4.4864e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 242/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.8107e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00242: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8107e-04 - mean_absolute_error: 0.0131 - val_loss: 7.4118e-05 - val_mean_absolute_error: 0.0099\n",
      "Epoch 243/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7240e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00243: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7351e-04 - mean_absolute_error: 0.0128 - val_loss: 2.9759e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 244/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7944e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00244: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7977e-04 - mean_absolute_error: 0.0133 - val_loss: 3.9979e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 245/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0026e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00245: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0026e-04 - mean_absolute_error: 0.0137 - val_loss: 6.8942e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 246/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9264e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00246: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9271e-04 - mean_absolute_error: 0.0133 - val_loss: 3.1703e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 247/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8224e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00247: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8148e-04 - mean_absolute_error: 0.0126 - val_loss: 3.8315e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 248/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6479e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00248: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6525e-04 - mean_absolute_error: 0.0124 - val_loss: 2.7451e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 249/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.8532e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00249: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8532e-04 - mean_absolute_error: 0.0130 - val_loss: 3.2387e-05 - val_mean_absolute_error: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7746e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00250: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.7839e-04 - mean_absolute_error: 0.0129 - val_loss: 2.9441e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 251/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7125e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00251: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.7092e-04 - mean_absolute_error: 0.0125 - val_loss: 8.6312e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 252/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9212e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00252: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9236e-04 - mean_absolute_error: 0.0136 - val_loss: 7.1505e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 253/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9439e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00253: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9579e-04 - mean_absolute_error: 0.0133 - val_loss: 8.0009e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 254/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7490e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00254: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7490e-04 - mean_absolute_error: 0.0128 - val_loss: 3.8097e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 255/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6539e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00255: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6496e-04 - mean_absolute_error: 0.0124 - val_loss: 2.6773e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 256/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0558e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00256: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0558e-04 - mean_absolute_error: 0.0136 - val_loss: 5.4873e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 257/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9198e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00257: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9198e-04 - mean_absolute_error: 0.0127 - val_loss: 4.0237e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 258/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7921e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00258: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7921e-04 - mean_absolute_error: 0.0130 - val_loss: 8.7029e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 259/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8530e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00259: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8579e-04 - mean_absolute_error: 0.0132 - val_loss: 3.7958e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 260/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5550e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00260: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5550e-04 - mean_absolute_error: 0.0124 - val_loss: 3.5485e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 261/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7914e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00261: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7721e-04 - mean_absolute_error: 0.0127 - val_loss: 4.7707e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 262/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8460e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00262: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8222e-04 - mean_absolute_error: 0.0129 - val_loss: 3.9707e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 263/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9151e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00263: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9119e-04 - mean_absolute_error: 0.0131 - val_loss: 6.4327e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 264/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9018e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00264: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9040e-04 - mean_absolute_error: 0.0133 - val_loss: 7.5682e-05 - val_mean_absolute_error: 0.0104\n",
      "Epoch 265/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8008e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00265: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8168e-04 - mean_absolute_error: 0.0131 - val_loss: 5.5915e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 266/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6352e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00266: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6341e-04 - mean_absolute_error: 0.0124 - val_loss: 4.0500e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 267/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.8192e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00267: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8192e-04 - mean_absolute_error: 0.0128 - val_loss: 3.4503e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 268/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6658e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00268: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.6644e-04 - mean_absolute_error: 0.0127 - val_loss: 2.9623e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 269/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6602e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00269: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6602e-04 - mean_absolute_error: 0.0124 - val_loss: 3.0791e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 270/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6943e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00270: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7062e-04 - mean_absolute_error: 0.0125 - val_loss: 7.2670e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 271/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8897e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00271: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8706e-04 - mean_absolute_error: 0.0131 - val_loss: 4.7412e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 272/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7703e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00272: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7703e-04 - mean_absolute_error: 0.0126 - val_loss: 2.8225e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 273/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.6944e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00273: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6984e-04 - mean_absolute_error: 0.0126 - val_loss: 5.5162e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 274/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9611e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00274: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.9545e-04 - mean_absolute_error: 0.0133 - val_loss: 4.9860e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 275/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6507e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00275: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6528e-04 - mean_absolute_error: 0.0125 - val_loss: 3.4481e-05 - val_mean_absolute_error: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7934e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00276: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8456e-04 - mean_absolute_error: 0.0129 - val_loss: 4.2320e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 277/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6376e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00277: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6291e-04 - mean_absolute_error: 0.0122 - val_loss: 3.3210e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 278/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7888e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00278: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8056e-04 - mean_absolute_error: 0.0131 - val_loss: 6.4047e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 279/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8078e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00279: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8028e-04 - mean_absolute_error: 0.0127 - val_loss: 8.1716e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 280/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7203e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00280: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7203e-04 - mean_absolute_error: 0.0127 - val_loss: 4.0117e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 281/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9393e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00281: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.9239e-04 - mean_absolute_error: 0.0133 - val_loss: 6.5640e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 282/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8908e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00282: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.8838e-04 - mean_absolute_error: 0.0131 - val_loss: 2.9273e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 283/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.7405e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00283: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7302e-04 - mean_absolute_error: 0.0128 - val_loss: 4.1887e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 284/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9286e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00284: val_loss did not improve from 0.00003\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.9286e-04 - mean_absolute_error: 0.0131 - val_loss: 8.7155e-05 - val_mean_absolute_error: 0.0104\n",
      "Epoch 285/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8359e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00285: val_loss improved from 0.00003 to 0.00002, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.8261e-04 - mean_absolute_error: 0.0128 - val_loss: 2.4385e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 286/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8453e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00286: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8377e-04 - mean_absolute_error: 0.0130 - val_loss: 6.8152e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 287/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.7950e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00287: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7819e-04 - mean_absolute_error: 0.0130 - val_loss: 5.6715e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 288/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6835e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00288: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6835e-04 - mean_absolute_error: 0.0124 - val_loss: 6.8662e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 289/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7080e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00289: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7000e-04 - mean_absolute_error: 0.0125 - val_loss: 2.5833e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 290/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7723e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00290: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7773e-04 - mean_absolute_error: 0.0129 - val_loss: 3.7438e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 291/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.9396e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00291: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9279e-04 - mean_absolute_error: 0.0133 - val_loss: 3.4028e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 292/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0814e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00292: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.0814e-04 - mean_absolute_error: 0.0138 - val_loss: 9.5713e-05 - val_mean_absolute_error: 0.0098\n",
      "Epoch 293/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 2.0224e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00293: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 2.0224e-04 - mean_absolute_error: 0.0136 - val_loss: 9.6782e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 294/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7113e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00294: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7393e-04 - mean_absolute_error: 0.0124 - val_loss: 2.9191e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 295/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7414e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00295: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7422e-04 - mean_absolute_error: 0.0124 - val_loss: 5.5742e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 296/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.0018e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00296: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9888e-04 - mean_absolute_error: 0.0135 - val_loss: 5.4137e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 297/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7854e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00297: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 1.7854e-04 - mean_absolute_error: 0.0127 - val_loss: 4.5483e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 298/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8002e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00298: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7976e-04 - mean_absolute_error: 0.0128 - val_loss: 5.2314e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 299/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9540e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00299: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9540e-04 - mean_absolute_error: 0.0133 - val_loss: 5.0283e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 300/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7157e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00300: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 1.7157e-04 - mean_absolute_error: 0.0126 - val_loss: 3.0062e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 301/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/86 [============================>.] - ETA: 0s - loss: 1.7907e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00301: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.7921e-04 - mean_absolute_error: 0.0130 - val_loss: 4.9752e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 302/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7717e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00302: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.7597e-04 - mean_absolute_error: 0.0129 - val_loss: 4.3099e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 303/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7411e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00303: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7592e-04 - mean_absolute_error: 0.0126 - val_loss: 6.3293e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 304/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7409e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00304: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7445e-04 - mean_absolute_error: 0.0127 - val_loss: 4.3065e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 305/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6521e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00305: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6521e-04 - mean_absolute_error: 0.0123 - val_loss: 5.4887e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 306/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.8772e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00306: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8772e-04 - mean_absolute_error: 0.0130 - val_loss: 3.7540e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 307/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8480e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00307: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8362e-04 - mean_absolute_error: 0.0130 - val_loss: 2.6581e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 308/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7770e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00308: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.7718e-04 - mean_absolute_error: 0.0124 - val_loss: 3.6151e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 309/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7480e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00309: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 1.7519e-04 - mean_absolute_error: 0.0127 - val_loss: 3.0039e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 310/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8285e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00310: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.8290e-04 - mean_absolute_error: 0.0133 - val_loss: 3.2029e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 311/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9768e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00311: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9642e-04 - mean_absolute_error: 0.0135 - val_loss: 6.0173e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 312/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7751e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00312: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7751e-04 - mean_absolute_error: 0.0128 - val_loss: 7.1188e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 313/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7142e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00313: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7036e-04 - mean_absolute_error: 0.0124 - val_loss: 3.0123e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 314/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5817e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00314: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5936e-04 - mean_absolute_error: 0.0123 - val_loss: 2.7088e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 315/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8969e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00315: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9106e-04 - mean_absolute_error: 0.0131 - val_loss: 5.4259e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 316/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.6146e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00316: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6065e-04 - mean_absolute_error: 0.0121 - val_loss: 6.9192e-05 - val_mean_absolute_error: 0.0099\n",
      "Epoch 317/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 2.1546e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00317: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 2.1429e-04 - mean_absolute_error: 0.0138 - val_loss: 6.7909e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 318/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6877e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00318: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 25ms/step - loss: 1.6980e-04 - mean_absolute_error: 0.0126 - val_loss: 2.5354e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 319/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.4870e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00319: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5050e-04 - mean_absolute_error: 0.0120 - val_loss: 4.8264e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 320/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.6896e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00320: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6925e-04 - mean_absolute_error: 0.0126 - val_loss: 5.2209e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 321/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6662e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00321: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6752e-04 - mean_absolute_error: 0.0124 - val_loss: 3.5665e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 322/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7100e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00322: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7242e-04 - mean_absolute_error: 0.0127 - val_loss: 9.6362e-05 - val_mean_absolute_error: 0.0102\n",
      "Epoch 323/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6974e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00323: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6895e-04 - mean_absolute_error: 0.0126 - val_loss: 2.5405e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 324/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5684e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00324: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5644e-04 - mean_absolute_error: 0.0122 - val_loss: 3.9733e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 325/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8203e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00325: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8136e-04 - mean_absolute_error: 0.0128 - val_loss: 4.3913e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 326/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6030e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00326: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5990e-04 - mean_absolute_error: 0.0123 - val_loss: 5.7010e-05 - val_mean_absolute_error: 0.0079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 327/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8094e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00327: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8533e-04 - mean_absolute_error: 0.0130 - val_loss: 5.0138e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 328/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8186e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00328: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8122e-04 - mean_absolute_error: 0.0130 - val_loss: 4.6151e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 329/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4885e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00329: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.4870e-04 - mean_absolute_error: 0.0119 - val_loss: 3.8634e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 330/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7347e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00330: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7316e-04 - mean_absolute_error: 0.0124 - val_loss: 3.0391e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 331/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6933e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00331: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6871e-04 - mean_absolute_error: 0.0124 - val_loss: 3.9074e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 332/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6923e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00332: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6923e-04 - mean_absolute_error: 0.0124 - val_loss: 3.4568e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 333/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6311e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00333: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6311e-04 - mean_absolute_error: 0.0127 - val_loss: 9.3966e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 334/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6847e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00334: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.7014e-04 - mean_absolute_error: 0.0126 - val_loss: 2.5745e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 335/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6104e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00335: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6104e-04 - mean_absolute_error: 0.0123 - val_loss: 2.9395e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 336/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6592e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00336: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6553e-04 - mean_absolute_error: 0.0122 - val_loss: 8.5123e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 337/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8997e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00337: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.9035e-04 - mean_absolute_error: 0.0132 - val_loss: 8.8049e-05 - val_mean_absolute_error: 0.0103\n",
      "Epoch 338/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6616e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00338: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.6512e-04 - mean_absolute_error: 0.0124 - val_loss: 2.4651e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 339/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6267e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00339: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6267e-04 - mean_absolute_error: 0.0124 - val_loss: 2.9586e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 340/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6925e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00340: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6925e-04 - mean_absolute_error: 0.0123 - val_loss: 7.6676e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 341/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9272e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00341: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9086e-04 - mean_absolute_error: 0.0134 - val_loss: 4.0881e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 342/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6434e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00342: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6396e-04 - mean_absolute_error: 0.0122 - val_loss: 3.7680e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 343/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7406e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00343: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7541e-04 - mean_absolute_error: 0.0123 - val_loss: 3.0103e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 344/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6043e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00344: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6019e-04 - mean_absolute_error: 0.0123 - val_loss: 2.7748e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 345/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6237e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00345: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.6237e-04 - mean_absolute_error: 0.0122 - val_loss: 5.2857e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 346/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6053e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00346: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6038e-04 - mean_absolute_error: 0.0122 - val_loss: 3.0532e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 347/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.9288e-04 - mean_absolute_error: 0.0132\n",
      "Epoch 00347: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.9774e-04 - mean_absolute_error: 0.0132 - val_loss: 2.8799e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 348/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5872e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00348: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5872e-04 - mean_absolute_error: 0.0120 - val_loss: 7.9648e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 349/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7742e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00349: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.7596e-04 - mean_absolute_error: 0.0125 - val_loss: 4.4911e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 350/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5475e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00350: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5777e-04 - mean_absolute_error: 0.0123 - val_loss: 3.4867e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 351/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6322e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00351: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.6538e-04 - mean_absolute_error: 0.0124 - val_loss: 4.8115e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 352/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7545e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00352: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7566e-04 - mean_absolute_error: 0.0126 - val_loss: 6.8533e-05 - val_mean_absolute_error: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.8014e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00353: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8198e-04 - mean_absolute_error: 0.0130 - val_loss: 4.3919e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 354/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5820e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00354: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5746e-04 - mean_absolute_error: 0.0120 - val_loss: 2.6678e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 355/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8928e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00355: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8885e-04 - mean_absolute_error: 0.0131 - val_loss: 3.6942e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 356/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6902e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00356: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6803e-04 - mean_absolute_error: 0.0126 - val_loss: 3.5177e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 357/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5355e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00357: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5378e-04 - mean_absolute_error: 0.0121 - val_loss: 3.1023e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 358/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7104e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00358: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7104e-04 - mean_absolute_error: 0.0124 - val_loss: 2.9775e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 359/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7481e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00359: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7536e-04 - mean_absolute_error: 0.0125 - val_loss: 3.4851e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 360/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7591e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00360: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7448e-04 - mean_absolute_error: 0.0124 - val_loss: 4.5352e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 361/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.6856e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00361: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6792e-04 - mean_absolute_error: 0.0124 - val_loss: 4.1272e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 362/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5912e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00362: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5912e-04 - mean_absolute_error: 0.0123 - val_loss: 7.0295e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 363/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8548e-04 - mean_absolute_error: 0.0131\n",
      "Epoch 00363: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8764e-04 - mean_absolute_error: 0.0131 - val_loss: 6.6729e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 364/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7142e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00364: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7113e-04 - mean_absolute_error: 0.0127 - val_loss: 5.4582e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 365/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7010e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00365: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6920e-04 - mean_absolute_error: 0.0125 - val_loss: 2.5189e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 366/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7408e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00366: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7559e-04 - mean_absolute_error: 0.0126 - val_loss: 4.8990e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 367/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5855e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00367: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 1.5855e-04 - mean_absolute_error: 0.0120 - val_loss: 4.6326e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 368/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4555e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00368: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.4501e-04 - mean_absolute_error: 0.0118 - val_loss: 2.4879e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 369/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5869e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00369: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5792e-04 - mean_absolute_error: 0.0121 - val_loss: 6.3021e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 370/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6540e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00370: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6540e-04 - mean_absolute_error: 0.0121 - val_loss: 4.5201e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 371/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7606e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00371: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7647e-04 - mean_absolute_error: 0.0130 - val_loss: 7.8037e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 372/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6002e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00372: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6002e-04 - mean_absolute_error: 0.0123 - val_loss: 3.2299e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 373/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7234e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00373: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7142e-04 - mean_absolute_error: 0.0128 - val_loss: 4.0143e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 374/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6933e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00374: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6933e-04 - mean_absolute_error: 0.0124 - val_loss: 2.6324e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 375/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5608e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00375: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5608e-04 - mean_absolute_error: 0.0121 - val_loss: 3.1829e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 376/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8172e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00376: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8058e-04 - mean_absolute_error: 0.0124 - val_loss: 3.4485e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 377/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8074e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00377: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8067e-04 - mean_absolute_error: 0.0128 - val_loss: 3.5079e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 378/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7106e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00378: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7106e-04 - mean_absolute_error: 0.0126 - val_loss: 4.6088e-05 - val_mean_absolute_error: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6634e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00379: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6587e-04 - mean_absolute_error: 0.0123 - val_loss: 2.5488e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 380/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6287e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00380: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6424e-04 - mean_absolute_error: 0.0121 - val_loss: 2.8782e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 381/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6704e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00381: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6681e-04 - mean_absolute_error: 0.0124 - val_loss: 3.2361e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 382/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5501e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00382: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5573e-04 - mean_absolute_error: 0.0120 - val_loss: 4.2157e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 383/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7377e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00383: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7364e-04 - mean_absolute_error: 0.0125 - val_loss: 5.8377e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 384/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6341e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00384: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6341e-04 - mean_absolute_error: 0.0121 - val_loss: 3.4496e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 385/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7583e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00385: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7538e-04 - mean_absolute_error: 0.0125 - val_loss: 4.1207e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 386/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6737e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00386: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6737e-04 - mean_absolute_error: 0.0124 - val_loss: 3.2197e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 387/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7380e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00387: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7369e-04 - mean_absolute_error: 0.0127 - val_loss: 3.0727e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 388/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6152e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00388: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6152e-04 - mean_absolute_error: 0.0121 - val_loss: 3.5338e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 389/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.4961e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00389: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.4961e-04 - mean_absolute_error: 0.0120 - val_loss: 4.3976e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 390/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6442e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00390: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6597e-04 - mean_absolute_error: 0.0125 - val_loss: 2.6065e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 391/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7776e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00391: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7814e-04 - mean_absolute_error: 0.0125 - val_loss: 8.6040e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 392/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7041e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00392: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6973e-04 - mean_absolute_error: 0.0124 - val_loss: 3.5154e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 393/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7046e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00393: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7046e-04 - mean_absolute_error: 0.0124 - val_loss: 3.7378e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 394/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.8400e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00394: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.8588e-04 - mean_absolute_error: 0.0128 - val_loss: 3.3961e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 395/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7903e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00395: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7886e-04 - mean_absolute_error: 0.0129 - val_loss: 4.1920e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 396/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7215e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00396: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7204e-04 - mean_absolute_error: 0.0127 - val_loss: 5.1122e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 397/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5578e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00397: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5741e-04 - mean_absolute_error: 0.0120 - val_loss: 3.1952e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 398/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7555e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00398: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7603e-04 - mean_absolute_error: 0.0127 - val_loss: 7.3050e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 399/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7780e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00399: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7726e-04 - mean_absolute_error: 0.0129 - val_loss: 7.5022e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 400/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5681e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00400: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5701e-04 - mean_absolute_error: 0.0122 - val_loss: 4.1647e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 401/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7607e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00401: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 1.7607e-04 - mean_absolute_error: 0.0122 - val_loss: 3.0972e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 402/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4065e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00402: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.4096e-04 - mean_absolute_error: 0.0115 - val_loss: 4.3033e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 403/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.7894e-04 - mean_absolute_error: 0.0127\n",
      "Epoch 00403: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7838e-04 - mean_absolute_error: 0.0127 - val_loss: 4.2106e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 404/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5974e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00404: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6066e-04 - mean_absolute_error: 0.0121 - val_loss: 3.2866e-05 - val_mean_absolute_error: 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5934e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00405: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5880e-04 - mean_absolute_error: 0.0121 - val_loss: 3.8906e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 406/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6406e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00406: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6371e-04 - mean_absolute_error: 0.0121 - val_loss: 3.6270e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 407/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5156e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00407: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5173e-04 - mean_absolute_error: 0.0121 - val_loss: 3.3669e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 408/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4987e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00408: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.4923e-04 - mean_absolute_error: 0.0116 - val_loss: 2.3574e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 409/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5435e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00409: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5488e-04 - mean_absolute_error: 0.0122 - val_loss: 3.4635e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 410/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5628e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00410: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5758e-04 - mean_absolute_error: 0.0122 - val_loss: 2.6654e-05 - val_mean_absolute_error: 0.0044\n",
      "Epoch 411/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6853e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00411: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.6853e-04 - mean_absolute_error: 0.0124 - val_loss: 3.9098e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 412/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5213e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00412: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5151e-04 - mean_absolute_error: 0.0119 - val_loss: 5.1504e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 413/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5974e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00413: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5974e-04 - mean_absolute_error: 0.0121 - val_loss: 6.0580e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 414/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6510e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00414: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6574e-04 - mean_absolute_error: 0.0122 - val_loss: 3.5054e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 415/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7433e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00415: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7433e-04 - mean_absolute_error: 0.0122 - val_loss: 3.1854e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 416/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5272e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00416: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5272e-04 - mean_absolute_error: 0.0120 - val_loss: 2.5263e-05 - val_mean_absolute_error: 0.0045\n",
      "Epoch 417/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7072e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00417: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6998e-04 - mean_absolute_error: 0.0121 - val_loss: 6.8085e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 418/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7402e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00418: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 26ms/step - loss: 1.7402e-04 - mean_absolute_error: 0.0125 - val_loss: 1.1685e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 419/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7627e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00419: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7514e-04 - mean_absolute_error: 0.0125 - val_loss: 2.9613e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 420/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6749e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00420: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.6582e-04 - mean_absolute_error: 0.0122 - val_loss: 3.1072e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 421/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6745e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00421: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6750e-04 - mean_absolute_error: 0.0123 - val_loss: 3.3661e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 422/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4392e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00422: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.4225e-04 - mean_absolute_error: 0.0115 - val_loss: 3.0711e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 423/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6620e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00423: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6620e-04 - mean_absolute_error: 0.0123 - val_loss: 4.3858e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 424/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6400e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00424: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6508e-04 - mean_absolute_error: 0.0124 - val_loss: 2.4745e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 425/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5226e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00425: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5226e-04 - mean_absolute_error: 0.0118 - val_loss: 2.3210e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 426/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7382e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00426: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.7382e-04 - mean_absolute_error: 0.0123 - val_loss: 3.9896e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 427/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7707e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00427: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7707e-04 - mean_absolute_error: 0.0126 - val_loss: 3.9941e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 428/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5425e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00428: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5655e-04 - mean_absolute_error: 0.0123 - val_loss: 3.0901e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 429/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3904e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00429: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3860e-04 - mean_absolute_error: 0.0115 - val_loss: 2.3388e-05 - val_mean_absolute_error: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7100e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00430: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.7156e-04 - mean_absolute_error: 0.0125 - val_loss: 2.8412e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 431/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6248e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00431: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6248e-04 - mean_absolute_error: 0.0123 - val_loss: 2.5198e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 432/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5268e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00432: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5409e-04 - mean_absolute_error: 0.0118 - val_loss: 4.5483e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 433/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4943e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00433: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4986e-04 - mean_absolute_error: 0.0117 - val_loss: 3.8812e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 434/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4437e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00434: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4380e-04 - mean_absolute_error: 0.0118 - val_loss: 3.1106e-05 - val_mean_absolute_error: 0.0043\n",
      "Epoch 435/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5548e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00435: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 24ms/step - loss: 1.5491e-04 - mean_absolute_error: 0.0121 - val_loss: 4.4373e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 436/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4660e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00436: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.4590e-04 - mean_absolute_error: 0.0114 - val_loss: 3.0551e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 437/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5542e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00437: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5515e-04 - mean_absolute_error: 0.0120 - val_loss: 5.3373e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 438/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.5235e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00438: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5368e-04 - mean_absolute_error: 0.0119 - val_loss: 7.1901e-05 - val_mean_absolute_error: 0.0099\n",
      "Epoch 439/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5447e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00439: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5452e-04 - mean_absolute_error: 0.0120 - val_loss: 2.3118e-05 - val_mean_absolute_error: 0.0041\n",
      "Epoch 440/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.8377e-04 - mean_absolute_error: 0.0129\n",
      "Epoch 00440: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.8282e-04 - mean_absolute_error: 0.0129 - val_loss: 6.2669e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 441/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6518e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00441: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6379e-04 - mean_absolute_error: 0.0127 - val_loss: 2.2852e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 442/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5449e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00442: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5449e-04 - mean_absolute_error: 0.0120 - val_loss: 3.2691e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 443/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6887e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00443: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6887e-04 - mean_absolute_error: 0.0122 - val_loss: 8.5680e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 444/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7269e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00444: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.7300e-04 - mean_absolute_error: 0.0124 - val_loss: 5.7064e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 445/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5796e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00445: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5943e-04 - mean_absolute_error: 0.0119 - val_loss: 2.8947e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 446/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5475e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00446: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5463e-04 - mean_absolute_error: 0.0119 - val_loss: 5.9211e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 447/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5012e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00447: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5012e-04 - mean_absolute_error: 0.0120 - val_loss: 3.0030e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 448/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5218e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00448: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5224e-04 - mean_absolute_error: 0.0120 - val_loss: 3.6921e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 449/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5643e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00449: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5578e-04 - mean_absolute_error: 0.0120 - val_loss: 3.3973e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 450/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.5864e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00450: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5648e-04 - mean_absolute_error: 0.0120 - val_loss: 2.3883e-05 - val_mean_absolute_error: 0.0042\n",
      "Epoch 451/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6557e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00451: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6557e-04 - mean_absolute_error: 0.0121 - val_loss: 3.4615e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 452/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5725e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00452: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5626e-04 - mean_absolute_error: 0.0120 - val_loss: 3.1659e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 453/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4828e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00453: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4808e-04 - mean_absolute_error: 0.0116 - val_loss: 2.6877e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 454/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.7332e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00454: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7271e-04 - mean_absolute_error: 0.0122 - val_loss: 3.8038e-05 - val_mean_absolute_error: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6095e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00455: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6095e-04 - mean_absolute_error: 0.0121 - val_loss: 5.9787e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 456/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.5704e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00456: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5696e-04 - mean_absolute_error: 0.0119 - val_loss: 3.4563e-05 - val_mean_absolute_error: 0.0051\n",
      "Epoch 457/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.9004e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00457: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.9004e-04 - mean_absolute_error: 0.0130 - val_loss: 4.8605e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 458/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.4688e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00458: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4688e-04 - mean_absolute_error: 0.0118 - val_loss: 3.2958e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 459/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5899e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00459: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6049e-04 - mean_absolute_error: 0.0121 - val_loss: 4.5071e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 460/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6094e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00460: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6094e-04 - mean_absolute_error: 0.0121 - val_loss: 6.1206e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 461/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6977e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00461: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6868e-04 - mean_absolute_error: 0.0124 - val_loss: 6.0774e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 462/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6613e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00462: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6547e-04 - mean_absolute_error: 0.0122 - val_loss: 3.4198e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 463/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6759e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00463: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6759e-04 - mean_absolute_error: 0.0124 - val_loss: 3.7835e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 464/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6529e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00464: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6529e-04 - mean_absolute_error: 0.0125 - val_loss: 5.0191e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 465/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5803e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00465: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5728e-04 - mean_absolute_error: 0.0120 - val_loss: 2.7326e-05 - val_mean_absolute_error: 0.0047\n",
      "Epoch 466/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5170e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00466: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5186e-04 - mean_absolute_error: 0.0121 - val_loss: 4.1340e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 467/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4860e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00467: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4850e-04 - mean_absolute_error: 0.0118 - val_loss: 3.1724e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 468/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3893e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00468: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3942e-04 - mean_absolute_error: 0.0114 - val_loss: 3.3551e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 469/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5451e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00469: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 23ms/step - loss: 1.5375e-04 - mean_absolute_error: 0.0117 - val_loss: 4.0834e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 470/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5701e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00470: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5664e-04 - mean_absolute_error: 0.0123 - val_loss: 4.0094e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 471/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5733e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00471: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5709e-04 - mean_absolute_error: 0.0121 - val_loss: 4.7970e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 472/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5371e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00472: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5232e-04 - mean_absolute_error: 0.0120 - val_loss: 2.7145e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 473/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5487e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00473: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5487e-04 - mean_absolute_error: 0.0118 - val_loss: 2.3999e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 474/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4037e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00474: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4187e-04 - mean_absolute_error: 0.0116 - val_loss: 3.7600e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 475/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.6879e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00475: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.6773e-04 - mean_absolute_error: 0.0122 - val_loss: 2.8125e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 476/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4885e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00476: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4846e-04 - mean_absolute_error: 0.0117 - val_loss: 3.0188e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 477/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5611e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00477: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5611e-04 - mean_absolute_error: 0.0120 - val_loss: 3.7069e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 478/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4866e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00478: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4884e-04 - mean_absolute_error: 0.0117 - val_loss: 3.1016e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 479/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6996e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00479: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6996e-04 - mean_absolute_error: 0.0124 - val_loss: 3.7109e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 480/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.6151e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00480: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.6014e-04 - mean_absolute_error: 0.0121 - val_loss: 3.9328e-05 - val_mean_absolute_error: 0.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.5780e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00481: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.5653e-04 - mean_absolute_error: 0.0120 - val_loss: 6.5846e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 482/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5563e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00482: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.5436e-04 - mean_absolute_error: 0.0119 - val_loss: 3.2072e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 483/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4213e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00483: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4237e-04 - mean_absolute_error: 0.0114 - val_loss: 3.5572e-05 - val_mean_absolute_error: 0.0049\n",
      "Epoch 484/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5337e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00484: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5569e-04 - mean_absolute_error: 0.0121 - val_loss: 4.6042e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 485/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4242e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00485: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4614e-04 - mean_absolute_error: 0.0118 - val_loss: 3.3379e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 486/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.6335e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00486: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6335e-04 - mean_absolute_error: 0.0122 - val_loss: 3.6889e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 487/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5270e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00487: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5404e-04 - mean_absolute_error: 0.0119 - val_loss: 4.3871e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 488/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5576e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00488: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5576e-04 - mean_absolute_error: 0.0119 - val_loss: 6.4248e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 489/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.7588e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00489: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.7588e-04 - mean_absolute_error: 0.0125 - val_loss: 3.4149e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 490/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.8405e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00490: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.8495e-04 - mean_absolute_error: 0.0130 - val_loss: 3.1824e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 491/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5384e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00491: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5364e-04 - mean_absolute_error: 0.0118 - val_loss: 3.8612e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 492/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5255e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00492: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5255e-04 - mean_absolute_error: 0.0119 - val_loss: 4.7898e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 493/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4620e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00493: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4782e-04 - mean_absolute_error: 0.0115 - val_loss: 3.4350e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 494/500\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.5519e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00494: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.5519e-04 - mean_absolute_error: 0.0120 - val_loss: 3.6931e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 495/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.6112e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00495: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.6079e-04 - mean_absolute_error: 0.0120 - val_loss: 2.7607e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 496/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4089e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00496: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4262e-04 - mean_absolute_error: 0.0116 - val_loss: 3.3042e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 497/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5571e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00497: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5606e-04 - mean_absolute_error: 0.0119 - val_loss: 4.7546e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 498/500\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.5846e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00498: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5863e-04 - mean_absolute_error: 0.0121 - val_loss: 6.2889e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 499/500\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.5878e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00499: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.6061e-04 - mean_absolute_error: 0.0122 - val_loss: 4.0033e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 500/500\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.5485e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00500: val_loss did not improve from 0.00002\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.5406e-04 - mean_absolute_error: 0.0121 - val_loss: 6.0181e-05 - val_mean_absolute_error: 0.0079\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 307.55$\n"
     ]
    }
   ],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)\n",
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Accuracy Score: 0.5637209302325581\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.836297535443189, 2.6730144037635877)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_return(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 Tesla price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which stock to predict\n",
    "ticker = \"TSLA\"\n",
    "\n",
    "# use numbers of previous days to predict\n",
    "N_STEPS = 100\n",
    "\n",
    "# which day to predict, 1 is for next day, 10 is for 10 days after today\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.5\n",
    "\n",
    "### training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 200\n",
    "\n",
    "### other parameters\n",
    "TEST_SIZE = 0.2\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save data to csv locally\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 2/21 [=>............................] - ETA: 1s - loss: 0.0041 - mean_absolute_error: 0.0521WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0347s vs `on_train_batch_end` time: 0.1407s). Check your callbacks.\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.0353\n",
      "Epoch 00001: val_loss improved from inf to 0.00067, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 2s 110ms/step - loss: 0.0017 - mean_absolute_error: 0.0352 - val_loss: 6.6571e-04 - val_mean_absolute_error: 0.0218\n",
      "Epoch 2/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 9.8943e-04 - mean_absolute_error: 0.0259\n",
      "Epoch 00002: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.0011 - mean_absolute_error: 0.0263 - val_loss: 7.5426e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 3/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 7.5096e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00003: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 7.4273e-04 - mean_absolute_error: 0.0216 - val_loss: 8.0440e-04 - val_mean_absolute_error: 0.0245\n",
      "Epoch 4/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 7.9976e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00004: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 7.9976e-04 - mean_absolute_error: 0.0217 - val_loss: 6.7205e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 5/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 8.4121e-04 - mean_absolute_error: 0.0242\n",
      "Epoch 00005: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 8.4724e-04 - mean_absolute_error: 0.0243 - val_loss: 0.0014 - val_mean_absolute_error: 0.0219\n",
      "Epoch 6/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0272\n",
      "Epoch 00006: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.0013 - mean_absolute_error: 0.0274 - val_loss: 0.0012 - val_mean_absolute_error: 0.0238\n",
      "Epoch 7/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.0015 - mean_absolute_error: 0.0325\n",
      "Epoch 00007: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.0014 - mean_absolute_error: 0.0321 - val_loss: 7.0672e-04 - val_mean_absolute_error: 0.0248\n",
      "Epoch 8/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.4456e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00008: val_loss did not improve from 0.00067\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 5.4456e-04 - mean_absolute_error: 0.0182 - val_loss: 6.9092e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 9/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 5.5066e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00009: val_loss improved from 0.00067 to 0.00040, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 5.5066e-04 - mean_absolute_error: 0.0188 - val_loss: 3.9764e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 10/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 6.6432e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00010: val_loss did not improve from 0.00040\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 6.6432e-04 - mean_absolute_error: 0.0198 - val_loss: 7.1197e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 11/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 6.3129e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00011: val_loss did not improve from 0.00040\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 6.3044e-04 - mean_absolute_error: 0.0184 - val_loss: 4.3136e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 12/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.4854e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00012: val_loss did not improve from 0.00040\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 4.4854e-04 - mean_absolute_error: 0.0163 - val_loss: 4.9913e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 13/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.7342e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00013: val_loss did not improve from 0.00040\n",
      "21/21 [==============================] - 1s 39ms/step - loss: 4.7342e-04 - mean_absolute_error: 0.0163 - val_loss: 4.9629e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 14/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 4.1048e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00014: val_loss did not improve from 0.00040\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 4.1048e-04 - mean_absolute_error: 0.0158 - val_loss: 5.4424e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 15/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 4.5536e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00015: val_loss improved from 0.00040 to 0.00033, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 4.5294e-04 - mean_absolute_error: 0.0164 - val_loss: 3.3330e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 16/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 4.2739e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00016: val_loss did not improve from 0.00033\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 4.1820e-04 - mean_absolute_error: 0.0149 - val_loss: 3.3364e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 17/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.5068e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00017: val_loss did not improve from 0.00033\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 3.5074e-04 - mean_absolute_error: 0.0151 - val_loss: 4.2202e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 18/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 5.3797e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00018: val_loss did not improve from 0.00033\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 5.1480e-04 - mean_absolute_error: 0.0177 - val_loss: 3.9151e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 19/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 4.3862e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00019: val_loss did not improve from 0.00033\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 4.2231e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0013 - val_mean_absolute_error: 0.0212\n",
      "Epoch 20/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 5.6103e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00020: val_loss did not improve from 0.00033\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 5.4110e-04 - mean_absolute_error: 0.0187 - val_loss: 4.4854e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 21/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.9224e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00021: val_loss did not improve from 0.00033\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 3.9224e-04 - mean_absolute_error: 0.0144 - val_loss: 4.2169e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 22/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.6300e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00022: val_loss improved from 0.00033 to 0.00030, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 3.6056e-04 - mean_absolute_error: 0.0141 - val_loss: 3.0251e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 23/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.0904e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00023: val_loss did not improve from 0.00030\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 2.9927e-04 - mean_absolute_error: 0.0133 - val_loss: 3.3744e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 24/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.0148e-04 - mean_absolute_error: 0.0130\n",
      "Epoch 00024: val_loss did not improve from 0.00030\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.9716e-04 - mean_absolute_error: 0.0131 - val_loss: 6.0152e-04 - val_mean_absolute_error: 0.0182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 4.2694e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00025: val_loss did not improve from 0.00030\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 4.2422e-04 - mean_absolute_error: 0.0177 - val_loss: 4.8171e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 26/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.1246e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00026: val_loss improved from 0.00030 to 0.00027, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 3.1149e-04 - mean_absolute_error: 0.0146 - val_loss: 2.6987e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 27/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.7619e-04 - mean_absolute_error: 0.0125\n",
      "Epoch 00027: val_loss did not improve from 0.00027\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.7619e-04 - mean_absolute_error: 0.0125 - val_loss: 2.9835e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 28/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.5394e-04 - mean_absolute_error: 0.0113\n",
      "Epoch 00028: val_loss did not improve from 0.00027\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.5229e-04 - mean_absolute_error: 0.0113 - val_loss: 3.2142e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 29/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.7562e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00029: val_loss did not improve from 0.00027\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.7173e-04 - mean_absolute_error: 0.0123 - val_loss: 3.3546e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 30/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.8011e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00030: val_loss did not improve from 0.00027\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.8446e-04 - mean_absolute_error: 0.0137 - val_loss: 2.9292e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 31/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.0523e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00031: val_loss did not improve from 0.00027\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 3.0087e-04 - mean_absolute_error: 0.0134 - val_loss: 4.2510e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 32/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.6387e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00032: val_loss did not improve from 0.00027\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.6133e-04 - mean_absolute_error: 0.0137 - val_loss: 3.1092e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 33/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 3.0155e-04 - mean_absolute_error: 0.0133\n",
      "Epoch 00033: val_loss improved from 0.00027 to 0.00024, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 3.0155e-04 - mean_absolute_error: 0.0133 - val_loss: 2.3550e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 34/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.6797e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00034: val_loss did not improve from 0.00024\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.6512e-04 - mean_absolute_error: 0.0122 - val_loss: 2.9703e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 35/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.2872e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00035: val_loss improved from 0.00024 to 0.00022, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.2961e-04 - mean_absolute_error: 0.0108 - val_loss: 2.1718e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 36/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0400e-04 - mean_absolute_error: 0.0110\n",
      "Epoch 00036: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.1685e-04 - mean_absolute_error: 0.0111 - val_loss: 3.6169e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 37/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.6029e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00037: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.5798e-04 - mean_absolute_error: 0.0117 - val_loss: 2.7714e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 38/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.5126e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00038: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.4746e-04 - mean_absolute_error: 0.0116 - val_loss: 3.0960e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 39/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.1537e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 00039: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.1378e-04 - mean_absolute_error: 0.0107 - val_loss: 2.3508e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 40/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.4792e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00040: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 2.4886e-04 - mean_absolute_error: 0.0120 - val_loss: 2.7017e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 41/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0276e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00041: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.9963e-04 - mean_absolute_error: 0.0104 - val_loss: 2.2474e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 42/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.3813e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00042: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 2.3813e-04 - mean_absolute_error: 0.0120 - val_loss: 3.1655e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 43/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.3729e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00043: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 2.3155e-04 - mean_absolute_error: 0.0119 - val_loss: 6.7391e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 44/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.7054e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00044: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.6798e-04 - mean_absolute_error: 0.0134 - val_loss: 4.3022e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 45/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.8106e-04 - mean_absolute_error: 0.0126\n",
      "Epoch 00045: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.7459e-04 - mean_absolute_error: 0.0124 - val_loss: 3.5806e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 46/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.5629e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00046: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 2.5629e-04 - mean_absolute_error: 0.0115 - val_loss: 3.6625e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 47/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.6378e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00047: val_loss did not improve from 0.00022\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.6346e-04 - mean_absolute_error: 0.0141 - val_loss: 3.3340e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 48/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.5699e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00048: val_loss improved from 0.00022 to 0.00019, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 2.5699e-04 - mean_absolute_error: 0.0121 - val_loss: 1.9467e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - ETA: 0s - loss: 2.7283e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00049: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 2.7283e-04 - mean_absolute_error: 0.0139 - val_loss: 3.8053e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 50/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.4954e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00050: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.4778e-04 - mean_absolute_error: 0.0119 - val_loss: 2.5899e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 51/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.9346e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00051: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.9150e-04 - mean_absolute_error: 0.0100 - val_loss: 2.1627e-04 - val_mean_absolute_error: 0.0089\n",
      "Epoch 52/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.1143e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00052: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.0780e-04 - mean_absolute_error: 0.0104 - val_loss: 2.9815e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 53/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.9321e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00053: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.9321e-04 - mean_absolute_error: 0.0102 - val_loss: 2.8088e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 54/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.4521e-04 - mean_absolute_error: 0.0106\n",
      "Epoch 00054: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 2.4521e-04 - mean_absolute_error: 0.0106 - val_loss: 3.2661e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 55/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.2103e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00055: val_loss did not improve from 0.00019\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 2.1957e-04 - mean_absolute_error: 0.0115 - val_loss: 3.0907e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 56/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.0636e-04 - mean_absolute_error: 0.0115\n",
      "Epoch 00056: val_loss improved from 0.00019 to 0.00018, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.1035e-04 - mean_absolute_error: 0.0116 - val_loss: 1.8102e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 57/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0472e-04 - mean_absolute_error: 0.0111\n",
      "Epoch 00057: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.0136e-04 - mean_absolute_error: 0.0110 - val_loss: 2.0822e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 58/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.3049e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00058: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.5239e-04 - mean_absolute_error: 0.0131 - val_loss: 2.9873e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 59/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.1081e-04 - mean_absolute_error: 0.0120\n",
      "Epoch 00059: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.1081e-04 - mean_absolute_error: 0.0120 - val_loss: 2.1190e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 60/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.1293e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00060: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.1274e-04 - mean_absolute_error: 0.0104 - val_loss: 3.2288e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 61/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.9404e-04 - mean_absolute_error: 0.0106\n",
      "Epoch 00061: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.9273e-04 - mean_absolute_error: 0.0106 - val_loss: 1.8415e-04 - val_mean_absolute_error: 0.0088\n",
      "Epoch 62/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.0570e-04 - mean_absolute_error: 0.0105\n",
      "Epoch 00062: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.0704e-04 - mean_absolute_error: 0.0105 - val_loss: 1.8626e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 63/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.9631e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00063: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.9631e-04 - mean_absolute_error: 0.0103 - val_loss: 2.7450e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 64/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.9451e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00064: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.8533e-04 - mean_absolute_error: 0.0144 - val_loss: 2.1890e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 65/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6946e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00065: val_loss did not improve from 0.00018\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.6946e-04 - mean_absolute_error: 0.0098 - val_loss: 2.0535e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 66/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.6207e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00066: val_loss improved from 0.00018 to 0.00015, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.6557e-04 - mean_absolute_error: 0.0096 - val_loss: 1.4760e-04 - val_mean_absolute_error: 0.0069\n",
      "Epoch 67/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.6698e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00067: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.6722e-04 - mean_absolute_error: 0.0096 - val_loss: 2.5763e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 68/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.8934e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00068: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.9023e-04 - mean_absolute_error: 0.0100 - val_loss: 2.0615e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 69/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.2406e-04 - mean_absolute_error: 0.0116\n",
      "Epoch 00069: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.2406e-04 - mean_absolute_error: 0.0116 - val_loss: 1.6418e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 70/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.3860e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00070: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.3807e-04 - mean_absolute_error: 0.0118 - val_loss: 2.1025e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 71/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0972e-04 - mean_absolute_error: 0.0124\n",
      "Epoch 00071: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.0997e-04 - mean_absolute_error: 0.0124 - val_loss: 2.2811e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 72/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.8073e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00072: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.8546e-04 - mean_absolute_error: 0.0100 - val_loss: 1.8613e-04 - val_mean_absolute_error: 0.0067\n",
      "Epoch 73/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5005e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00073: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.6474e-04 - mean_absolute_error: 0.0098 - val_loss: 1.7891e-04 - val_mean_absolute_error: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4979e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00074: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.5342e-04 - mean_absolute_error: 0.0092 - val_loss: 1.9037e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 75/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5373e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00075: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.5361e-04 - mean_absolute_error: 0.0095 - val_loss: 1.7017e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 76/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.9228e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00076: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.9418e-04 - mean_absolute_error: 0.0114 - val_loss: 2.1280e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 77/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.7623e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 00077: val_loss improved from 0.00015 to 0.00015, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.7456e-04 - mean_absolute_error: 0.0107 - val_loss: 1.4655e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 78/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4539e-04 - mean_absolute_error: 0.0096\n",
      "Epoch 00078: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 1.4689e-04 - mean_absolute_error: 0.0096 - val_loss: 1.8918e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 79/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4973e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00079: val_loss did not improve from 0.00015\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.4516e-04 - mean_absolute_error: 0.0097 - val_loss: 1.5086e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 80/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.8307e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00080: val_loss improved from 0.00015 to 0.00014, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.8177e-04 - mean_absolute_error: 0.0097 - val_loss: 1.3911e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 81/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.0475e-04 - mean_absolute_error: 0.0110\n",
      "Epoch 00081: val_loss did not improve from 0.00014\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.0475e-04 - mean_absolute_error: 0.0110 - val_loss: 3.9155e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 82/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.2341e-04 - mean_absolute_error: 0.0128\n",
      "Epoch 00082: val_loss did not improve from 0.00014\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.2149e-04 - mean_absolute_error: 0.0128 - val_loss: 1.9596e-04 - val_mean_absolute_error: 0.0068\n",
      "Epoch 83/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.7861e-04 - mean_absolute_error: 0.0104\n",
      "Epoch 00083: val_loss did not improve from 0.00014\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.7861e-04 - mean_absolute_error: 0.0104 - val_loss: 1.4366e-04 - val_mean_absolute_error: 0.0069\n",
      "Epoch 84/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.8474e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00084: val_loss did not improve from 0.00014\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.8416e-04 - mean_absolute_error: 0.0108 - val_loss: 1.6287e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 85/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.0477e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00085: val_loss improved from 0.00014 to 0.00012, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.0252e-04 - mean_absolute_error: 0.0121 - val_loss: 1.1753e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 86/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6678e-04 - mean_absolute_error: 0.0110\n",
      "Epoch 00086: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.6678e-04 - mean_absolute_error: 0.0110 - val_loss: 2.1288e-04 - val_mean_absolute_error: 0.0082\n",
      "Epoch 87/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4724e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00087: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.4151e-04 - mean_absolute_error: 0.0091 - val_loss: 1.6656e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 88/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5545e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00088: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.4955e-04 - mean_absolute_error: 0.0094 - val_loss: 1.4882e-04 - val_mean_absolute_error: 0.0066\n",
      "Epoch 89/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4752e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00089: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.4752e-04 - mean_absolute_error: 0.0090 - val_loss: 1.5017e-04 - val_mean_absolute_error: 0.0083\n",
      "Epoch 90/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0621e-04 - mean_absolute_error: 0.0117\n",
      "Epoch 00090: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.0083e-04 - mean_absolute_error: 0.0116 - val_loss: 1.6126e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 91/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.1434e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00091: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.1631e-04 - mean_absolute_error: 0.0117 - val_loss: 1.9998e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 92/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.4133e-04 - mean_absolute_error: 0.0121\n",
      "Epoch 00092: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.4133e-04 - mean_absolute_error: 0.0121 - val_loss: 2.8019e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 93/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 3.0256e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00093: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 3.0043e-04 - mean_absolute_error: 0.0148 - val_loss: 2.1388e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 94/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 2.3290e-04 - mean_absolute_error: 0.0118\n",
      "Epoch 00094: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 2.3392e-04 - mean_absolute_error: 0.0118 - val_loss: 1.6381e-04 - val_mean_absolute_error: 0.0078\n",
      "Epoch 95/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.2633e-04 - mean_absolute_error: 0.0119\n",
      "Epoch 00095: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.1707e-04 - mean_absolute_error: 0.0117 - val_loss: 1.4679e-04 - val_mean_absolute_error: 0.0064\n",
      "Epoch 96/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.7013e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00096: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.7441e-04 - mean_absolute_error: 0.0101 - val_loss: 1.8986e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 97/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0564e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00097: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 2.0452e-04 - mean_absolute_error: 0.0095 - val_loss: 2.8683e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 98/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.4189e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00098: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 2.4189e-04 - mean_absolute_error: 0.0134 - val_loss: 2.6749e-04 - val_mean_absolute_error: 0.0095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.0902e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00099: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 2.1389e-04 - mean_absolute_error: 0.0116 - val_loss: 1.7629e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 100/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.7122e-04 - mean_absolute_error: 0.0098\n",
      "Epoch 00100: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.6966e-04 - mean_absolute_error: 0.0097 - val_loss: 1.3203e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 101/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4254e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00101: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.3820e-04 - mean_absolute_error: 0.0093 - val_loss: 1.6937e-04 - val_mean_absolute_error: 0.0068\n",
      "Epoch 102/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.3480e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00102: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.4216e-04 - mean_absolute_error: 0.0090 - val_loss: 1.7032e-04 - val_mean_absolute_error: 0.0069\n",
      "Epoch 103/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.3053e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00103: val_loss did not improve from 0.00012\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 1.3069e-04 - mean_absolute_error: 0.0084 - val_loss: 1.4209e-04 - val_mean_absolute_error: 0.0066\n",
      "Epoch 104/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.8889e-04 - mean_absolute_error: 0.0109\n",
      "Epoch 00104: val_loss improved from 0.00012 to 0.00011, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 1.8740e-04 - mean_absolute_error: 0.0108 - val_loss: 1.1096e-04 - val_mean_absolute_error: 0.0064\n",
      "Epoch 105/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.2802e-04 - mean_absolute_error: 0.0113\n",
      "Epoch 00105: val_loss did not improve from 0.00011\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 2.2220e-04 - mean_absolute_error: 0.0112 - val_loss: 2.4150e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 106/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.7556e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00106: val_loss improved from 0.00011 to 0.00010, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.6859e-04 - mean_absolute_error: 0.0107 - val_loss: 9.8025e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 107/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4699e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00107: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.4699e-04 - mean_absolute_error: 0.0094 - val_loss: 1.2997e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 108/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.6456e-04 - mean_absolute_error: 0.0106\n",
      "Epoch 00108: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.6379e-04 - mean_absolute_error: 0.0106 - val_loss: 1.6170e-04 - val_mean_absolute_error: 0.0103\n",
      "Epoch 109/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.4155e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00109: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.4071e-04 - mean_absolute_error: 0.0093 - val_loss: 1.3436e-04 - val_mean_absolute_error: 0.0064\n",
      "Epoch 110/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.4333e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00110: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.4417e-04 - mean_absolute_error: 0.0092 - val_loss: 2.5527e-04 - val_mean_absolute_error: 0.0087\n",
      "Epoch 111/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.9464e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00111: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.9373e-04 - mean_absolute_error: 0.0108 - val_loss: 1.3527e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 112/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.5018e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00112: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.4884e-04 - mean_absolute_error: 0.0091 - val_loss: 1.2760e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 113/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.5541e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00113: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.5386e-04 - mean_absolute_error: 0.0097 - val_loss: 1.1494e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 114/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4709e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00114: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.4709e-04 - mean_absolute_error: 0.0092 - val_loss: 1.1458e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 115/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5701e-04 - mean_absolute_error: 0.0102\n",
      "Epoch 00115: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.5328e-04 - mean_absolute_error: 0.0101 - val_loss: 1.1592e-04 - val_mean_absolute_error: 0.0110\n",
      "Epoch 116/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.6579e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 00116: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.6451e-04 - mean_absolute_error: 0.0107 - val_loss: 1.2135e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 117/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3216e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00117: val_loss did not improve from 0.00010\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.3094e-04 - mean_absolute_error: 0.0100 - val_loss: 1.4398e-04 - val_mean_absolute_error: 0.0064\n",
      "Epoch 118/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0815e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00118: val_loss improved from 0.00010 to 0.00009, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.1295e-04 - mean_absolute_error: 0.0083 - val_loss: 9.3188e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 119/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.2961e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00119: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.2614e-04 - mean_absolute_error: 0.0089 - val_loss: 1.1428e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 120/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2539e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00120: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.2786e-04 - mean_absolute_error: 0.0085 - val_loss: 2.4442e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 121/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.7340e-04 - mean_absolute_error: 0.0122\n",
      "Epoch 00121: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.7599e-04 - mean_absolute_error: 0.0122 - val_loss: 1.2352e-04 - val_mean_absolute_error: 0.0065\n",
      "Epoch 122/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.4669e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00122: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.4526e-04 - mean_absolute_error: 0.0097 - val_loss: 1.0608e-04 - val_mean_absolute_error: 0.0059\n",
      "Epoch 123/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.9105e-04 - mean_absolute_error: 0.0107\n",
      "Epoch 00123: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.9202e-04 - mean_absolute_error: 0.0106 - val_loss: 1.9989e-04 - val_mean_absolute_error: 0.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4828e-04 - mean_absolute_error: 0.0099\n",
      "Epoch 00124: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.4333e-04 - mean_absolute_error: 0.0098 - val_loss: 1.0507e-04 - val_mean_absolute_error: 0.0066\n",
      "Epoch 125/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5416e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00125: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.5220e-04 - mean_absolute_error: 0.0091 - val_loss: 1.4108e-04 - val_mean_absolute_error: 0.0084\n",
      "Epoch 126/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.5376e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00126: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.5296e-04 - mean_absolute_error: 0.0094 - val_loss: 1.2287e-04 - val_mean_absolute_error: 0.0069\n",
      "Epoch 127/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.7257e-04 - mean_absolute_error: 0.0103\n",
      "Epoch 00127: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.7085e-04 - mean_absolute_error: 0.0103 - val_loss: 2.1584e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 128/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.7278e-04 - mean_absolute_error: 0.0111\n",
      "Epoch 00128: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.6959e-04 - mean_absolute_error: 0.0110 - val_loss: 1.6866e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 129/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.1779e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00129: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.1490e-04 - mean_absolute_error: 0.0092 - val_loss: 1.1991e-04 - val_mean_absolute_error: 0.0055\n",
      "Epoch 130/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0271e-04 - mean_absolute_error: 0.0080\n",
      "Epoch 00130: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 9.8203e-05 - mean_absolute_error: 0.0079 - val_loss: 9.5885e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 131/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.2206e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00131: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.2703e-04 - mean_absolute_error: 0.0082 - val_loss: 1.1766e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 132/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.7127e-04 - mean_absolute_error: 0.0123\n",
      "Epoch 00132: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.7031e-04 - mean_absolute_error: 0.0122 - val_loss: 1.3712e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 133/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3377e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00133: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.3246e-04 - mean_absolute_error: 0.0093 - val_loss: 1.4411e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 134/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.1685e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00134: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.2109e-04 - mean_absolute_error: 0.0084 - val_loss: 1.3543e-04 - val_mean_absolute_error: 0.0076\n",
      "Epoch 135/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.6164e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00135: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.7208e-04 - mean_absolute_error: 0.0100 - val_loss: 1.4077e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 136/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5303e-04 - mean_absolute_error: 0.0101\n",
      "Epoch 00136: val_loss did not improve from 0.00009\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.5725e-04 - mean_absolute_error: 0.0101 - val_loss: 1.3909e-04 - val_mean_absolute_error: 0.0063\n",
      "Epoch 137/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0756e-04 - mean_absolute_error: 0.0079\n",
      "Epoch 00137: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.1301e-04 - mean_absolute_error: 0.0080 - val_loss: 7.8748e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 138/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.4636e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00138: val_loss did not improve from 0.00008\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.4537e-04 - mean_absolute_error: 0.0093 - val_loss: 1.0958e-04 - val_mean_absolute_error: 0.0081\n",
      "Epoch 139/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.4510e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00139: val_loss did not improve from 0.00008\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.4388e-04 - mean_absolute_error: 0.0091 - val_loss: 2.8987e-04 - val_mean_absolute_error: 0.0101\n",
      "Epoch 140/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6695e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00140: val_loss did not improve from 0.00008\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.6695e-04 - mean_absolute_error: 0.0095 - val_loss: 1.1049e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 141/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.3100e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00141: val_loss did not improve from 0.00008\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.2911e-04 - mean_absolute_error: 0.0088 - val_loss: 1.6400e-04 - val_mean_absolute_error: 0.0066\n",
      "Epoch 142/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2191e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00142: val_loss did not improve from 0.00008\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.2097e-04 - mean_absolute_error: 0.0087 - val_loss: 8.9213e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 143/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2357e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00143: val_loss did not improve from 0.00008\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.2357e-04 - mean_absolute_error: 0.0082 - val_loss: 9.4908e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 144/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.1996e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00144: val_loss improved from 0.00008 to 0.00006, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.1921e-04 - mean_absolute_error: 0.0084 - val_loss: 6.3966e-05 - val_mean_absolute_error: 0.0050\n",
      "Epoch 145/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3160e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00145: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.3463e-04 - mean_absolute_error: 0.0091 - val_loss: 1.1198e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 146/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.8382e-04 - mean_absolute_error: 0.0114\n",
      "Epoch 00146: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.7801e-04 - mean_absolute_error: 0.0113 - val_loss: 2.5670e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 147/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5002e-04 - mean_absolute_error: 0.0108\n",
      "Epoch 00147: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.5492e-04 - mean_absolute_error: 0.0109 - val_loss: 8.2379e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 148/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0824e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00148: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.1899e-04 - mean_absolute_error: 0.0083 - val_loss: 1.2556e-04 - val_mean_absolute_error: 0.0064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3765e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00149: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.3661e-04 - mean_absolute_error: 0.0082 - val_loss: 7.9235e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 150/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0418e-04 - mean_absolute_error: 0.0077\n",
      "Epoch 00150: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 1.0152e-04 - mean_absolute_error: 0.0077 - val_loss: 1.1139e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 151/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5587e-04 - mean_absolute_error: 0.0094\n",
      "Epoch 00151: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.5773e-04 - mean_absolute_error: 0.0095 - val_loss: 1.1552e-04 - val_mean_absolute_error: 0.0077\n",
      "Epoch 152/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3258e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00152: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.3258e-04 - mean_absolute_error: 0.0091 - val_loss: 9.5065e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 153/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.1160e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00153: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.1052e-04 - mean_absolute_error: 0.0084 - val_loss: 1.0493e-04 - val_mean_absolute_error: 0.0066\n",
      "Epoch 154/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.0019e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00154: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 9.9212e-05 - mean_absolute_error: 0.0085 - val_loss: 8.2120e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 155/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.2625e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00155: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.2611e-04 - mean_absolute_error: 0.0085 - val_loss: 8.8457e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 156/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0924e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00156: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.0709e-04 - mean_absolute_error: 0.0086 - val_loss: 1.6228e-04 - val_mean_absolute_error: 0.0060\n",
      "Epoch 157/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1174e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00157: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.1174e-04 - mean_absolute_error: 0.0083 - val_loss: 1.6683e-04 - val_mean_absolute_error: 0.0064\n",
      "Epoch 158/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3329e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00158: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.3198e-04 - mean_absolute_error: 0.0088 - val_loss: 9.9213e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 159/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.5094e-04 - mean_absolute_error: 0.0100\n",
      "Epoch 00159: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.5094e-04 - mean_absolute_error: 0.0100 - val_loss: 1.2714e-04 - val_mean_absolute_error: 0.0067\n",
      "Epoch 160/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.7573e-04 - mean_absolute_error: 0.0112\n",
      "Epoch 00160: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.7459e-04 - mean_absolute_error: 0.0111 - val_loss: 1.2252e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 161/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.2725e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00161: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.3625e-04 - mean_absolute_error: 0.0094 - val_loss: 1.1638e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 162/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3902e-04 - mean_absolute_error: 0.0097\n",
      "Epoch 00162: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.3757e-04 - mean_absolute_error: 0.0096 - val_loss: 9.1011e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 163/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.1941e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00163: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.2096e-04 - mean_absolute_error: 0.0083 - val_loss: 1.5315e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 164/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2579e-04 - mean_absolute_error: 0.0083\n",
      "Epoch 00164: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.2579e-04 - mean_absolute_error: 0.0083 - val_loss: 1.7250e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 165/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0652e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00165: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0765e-04 - mean_absolute_error: 0.0085 - val_loss: 1.1776e-04 - val_mean_absolute_error: 0.0063\n",
      "Epoch 166/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5294e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00166: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.5829e-04 - mean_absolute_error: 0.0090 - val_loss: 1.1688e-04 - val_mean_absolute_error: 0.0063\n",
      "Epoch 167/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2401e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00167: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.2398e-04 - mean_absolute_error: 0.0092 - val_loss: 1.6413e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 168/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3084e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00168: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.3329e-04 - mean_absolute_error: 0.0091 - val_loss: 1.0210e-04 - val_mean_absolute_error: 0.0071\n",
      "Epoch 169/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1699e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00169: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.1699e-04 - mean_absolute_error: 0.0087 - val_loss: 1.1006e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 170/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.0763e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00170: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.0665e-04 - mean_absolute_error: 0.0085 - val_loss: 1.1869e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 171/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2429e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00171: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.2429e-04 - mean_absolute_error: 0.0082 - val_loss: 1.7638e-04 - val_mean_absolute_error: 0.0075\n",
      "Epoch 172/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0711e-04 - mean_absolute_error: 0.0081\n",
      "Epoch 00172: val_loss improved from 0.00006 to 0.00006, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0711e-04 - mean_absolute_error: 0.0081 - val_loss: 6.2903e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 173/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2346e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00173: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.2233e-04 - mean_absolute_error: 0.0086 - val_loss: 9.5749e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/21 [==========================>...] - ETA: 0s - loss: 1.1275e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00174: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.1665e-04 - mean_absolute_error: 0.0085 - val_loss: 1.0153e-04 - val_mean_absolute_error: 0.0053\n",
      "Epoch 175/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.0335e-04 - mean_absolute_error: 0.0080\n",
      "Epoch 00175: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0257e-04 - mean_absolute_error: 0.0080 - val_loss: 1.2869e-04 - val_mean_absolute_error: 0.0072\n",
      "Epoch 176/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.0382e-04 - mean_absolute_error: 0.0084\n",
      "Epoch 00176: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0282e-04 - mean_absolute_error: 0.0084 - val_loss: 6.7785e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 177/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.4480e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00177: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.4288e-04 - mean_absolute_error: 0.0091 - val_loss: 1.9522e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 178/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2666e-04 - mean_absolute_error: 0.0090\n",
      "Epoch 00178: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.2573e-04 - mean_absolute_error: 0.0089 - val_loss: 7.0698e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 179/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.1430e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00179: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.1318e-04 - mean_absolute_error: 0.0082 - val_loss: 1.0011e-04 - val_mean_absolute_error: 0.0060\n",
      "Epoch 180/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0281e-04 - mean_absolute_error: 0.0078\n",
      "Epoch 00180: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.0303e-04 - mean_absolute_error: 0.0078 - val_loss: 8.3876e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 181/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 9.1449e-05 - mean_absolute_error: 0.0082\n",
      "Epoch 00181: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 9.0811e-05 - mean_absolute_error: 0.0081 - val_loss: 1.5138e-04 - val_mean_absolute_error: 0.0073\n",
      "Epoch 182/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.3164e-04 - mean_absolute_error: 0.0088\n",
      "Epoch 00182: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.3068e-04 - mean_absolute_error: 0.0089 - val_loss: 1.1071e-04 - val_mean_absolute_error: 0.0079\n",
      "Epoch 183/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0823e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00183: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0935e-04 - mean_absolute_error: 0.0084 - val_loss: 1.0160e-04 - val_mean_absolute_error: 0.0070\n",
      "Epoch 184/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.1716e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00184: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.1611e-04 - mean_absolute_error: 0.0086 - val_loss: 1.4294e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 185/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3447e-04 - mean_absolute_error: 0.0093\n",
      "Epoch 00185: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.3314e-04 - mean_absolute_error: 0.0092 - val_loss: 1.3240e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 186/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2153e-04 - mean_absolute_error: 0.0091\n",
      "Epoch 00186: val_loss did not improve from 0.00006\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.2177e-04 - mean_absolute_error: 0.0091 - val_loss: 8.7668e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 187/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2625e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00187: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-12-10_TSLA-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.2498e-04 - mean_absolute_error: 0.0082 - val_loss: 5.0503e-05 - val_mean_absolute_error: 0.0046\n",
      "Epoch 188/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 9.0964e-05 - mean_absolute_error: 0.0073\n",
      "Epoch 00188: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0094e-04 - mean_absolute_error: 0.0075 - val_loss: 5.3188e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 189/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.1727e-04 - mean_absolute_error: 0.0089\n",
      "Epoch 00189: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.1280e-04 - mean_absolute_error: 0.0087 - val_loss: 8.5118e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 190/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.1606e-04 - mean_absolute_error: 0.0087\n",
      "Epoch 00190: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 1.1826e-04 - mean_absolute_error: 0.0087 - val_loss: 6.3616e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 191/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 9.3971e-05 - mean_absolute_error: 0.0088\n",
      "Epoch 00191: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 9.6302e-05 - mean_absolute_error: 0.0088 - val_loss: 6.4076e-05 - val_mean_absolute_error: 0.0048\n",
      "Epoch 192/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.3837e-04 - mean_absolute_error: 0.0085\n",
      "Epoch 00192: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.3708e-04 - mean_absolute_error: 0.0084 - val_loss: 1.3548e-04 - val_mean_absolute_error: 0.0062\n",
      "Epoch 193/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 8.4572e-05 - mean_absolute_error: 0.0077\n",
      "Epoch 00193: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 8.2628e-05 - mean_absolute_error: 0.0077 - val_loss: 9.5393e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 194/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.0389e-04 - mean_absolute_error: 0.0076\n",
      "Epoch 00194: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0375e-04 - mean_absolute_error: 0.0076 - val_loss: 7.4634e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 195/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.1356e-04 - mean_absolute_error: 0.0086\n",
      "Epoch 00195: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 1.1128e-04 - mean_absolute_error: 0.0086 - val_loss: 9.0276e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 196/200\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.2157e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00196: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 40ms/step - loss: 1.2036e-04 - mean_absolute_error: 0.0082 - val_loss: 1.5877e-04 - val_mean_absolute_error: 0.0068\n",
      "Epoch 197/200\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.5733e-04 - mean_absolute_error: 0.0095\n",
      "Epoch 00197: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 1.5430e-04 - mean_absolute_error: 0.0096 - val_loss: 8.2611e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 198/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2415e-04 - mean_absolute_error: 0.0092\n",
      "Epoch 00198: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 1.2415e-04 - mean_absolute_error: 0.0092 - val_loss: 8.9488e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/21 [==========================>...] - ETA: 0s - loss: 1.0280e-04 - mean_absolute_error: 0.0082\n",
      "Epoch 00199: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.0700e-04 - mean_absolute_error: 0.0082 - val_loss: 2.0323e-04 - val_mean_absolute_error: 0.0063\n",
      "Epoch 200/200\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2193e-04 - mean_absolute_error: 0.0079\n",
      "Epoch 00200: val_loss did not improve from 0.00005\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 1.2193e-04 - mean_absolute_error: 0.0079 - val_loss: 9.1286e-05 - val_mean_absolute_error: 0.0066\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 566.68$\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Accuracy Score: 0.5474308300395256\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.995669922302485, 8.324794586756878)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_return(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 S&P 500 price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which stock to predict\n",
    "ticker = \"SPY\"\n",
    "\n",
    "# use numbers of previous days to predict\n",
    "N_STEPS = 100\n",
    "\n",
    "# which day to predict, 1 is for next day, 10 is for 10 days after today\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.5\n",
    "\n",
    "### training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 200\n",
    "\n",
    "### other parameters\n",
    "TEST_SIZE = 0.2\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save data to csv locally\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 2/56 [>.............................] - ETA: 5s - loss: 0.0332 - mean_absolute_error: 0.1654WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0326s vs `on_train_batch_end` time: 0.1863s). Check your callbacks.\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 0.0051 - mean_absolute_error: 0.0617\n",
      "Epoch 00001: val_loss improved from inf to 0.00042, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 3s 49ms/step - loss: 0.0050 - mean_absolute_error: 0.0610 - val_loss: 4.2054e-04 - val_mean_absolute_error: 0.0189\n",
      "Epoch 2/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0330\n",
      "Epoch 00002: val_loss improved from 0.00042 to 0.00033, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 0.0013 - mean_absolute_error: 0.0330 - val_loss: 3.2922e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 3/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0326\n",
      "Epoch 00003: val_loss did not improve from 0.00033\n",
      "56/56 [==============================] - 1s 27ms/step - loss: 0.0012 - mean_absolute_error: 0.0326 - val_loss: 4.5429e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 4/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0303\n",
      "Epoch 00004: val_loss improved from 0.00033 to 0.00031, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 0.0010 - mean_absolute_error: 0.0303 - val_loss: 3.0676e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 5/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 8.6148e-04 - mean_absolute_error: 0.0272\n",
      "Epoch 00005: val_loss did not improve from 0.00031\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 8.5977e-04 - mean_absolute_error: 0.0273 - val_loss: 3.2315e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 6/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 8.8805e-04 - mean_absolute_error: 0.0272\n",
      "Epoch 00006: val_loss did not improve from 0.00031\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 8.8805e-04 - mean_absolute_error: 0.0272 - val_loss: 6.3859e-04 - val_mean_absolute_error: 0.0258\n",
      "Epoch 7/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 9.6715e-04 - mean_absolute_error: 0.0290\n",
      "Epoch 00007: val_loss did not improve from 0.00031\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 9.6563e-04 - mean_absolute_error: 0.0290 - val_loss: 5.2160e-04 - val_mean_absolute_error: 0.0241\n",
      "Epoch 8/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 8.7442e-04 - mean_absolute_error: 0.0272\n",
      "Epoch 00008: val_loss improved from 0.00031 to 0.00029, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 8.7576e-04 - mean_absolute_error: 0.0272 - val_loss: 2.9495e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 9/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 6.7979e-04 - mean_absolute_error: 0.0241\n",
      "Epoch 00009: val_loss improved from 0.00029 to 0.00027, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 6.8029e-04 - mean_absolute_error: 0.0241 - val_loss: 2.6832e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 10/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 7.0556e-04 - mean_absolute_error: 0.0247\n",
      "Epoch 00010: val_loss improved from 0.00027 to 0.00018, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 7.0556e-04 - mean_absolute_error: 0.0247 - val_loss: 1.7926e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 11/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 6.1943e-04 - mean_absolute_error: 0.0230\n",
      "Epoch 00011: val_loss did not improve from 0.00018\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 6.1943e-04 - mean_absolute_error: 0.0230 - val_loss: 1.8539e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 12/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 8.6227e-04 - mean_absolute_error: 0.0274\n",
      "Epoch 00012: val_loss did not improve from 0.00018\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 8.6317e-04 - mean_absolute_error: 0.0274 - val_loss: 5.5397e-04 - val_mean_absolute_error: 0.0236\n",
      "Epoch 13/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 7.9142e-04 - mean_absolute_error: 0.0259\n",
      "Epoch 00013: val_loss did not improve from 0.00018\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 7.8923e-04 - mean_absolute_error: 0.0259 - val_loss: 4.3443e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 14/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 6.1584e-04 - mean_absolute_error: 0.0230\n",
      "Epoch 00014: val_loss did not improve from 0.00018\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 6.1669e-04 - mean_absolute_error: 0.0230 - val_loss: 2.4567e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 15/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 6.6794e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00015: val_loss did not improve from 0.00018\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 6.6614e-04 - mean_absolute_error: 0.0239 - val_loss: 1.7995e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 16/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 6.0373e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 00016: val_loss improved from 0.00018 to 0.00015, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 6.0373e-04 - mean_absolute_error: 0.0227 - val_loss: 1.5446e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 17/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 5.6264e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00017: val_loss improved from 0.00015 to 0.00015, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 5.7272e-04 - mean_absolute_error: 0.0217 - val_loss: 1.4654e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 18/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 5.6001e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00018: val_loss did not improve from 0.00015\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 5.5865e-04 - mean_absolute_error: 0.0217 - val_loss: 1.5141e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 19/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 5.1774e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00019: val_loss did not improve from 0.00015\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 5.1774e-04 - mean_absolute_error: 0.0208 - val_loss: 1.6054e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 20/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 6.0081e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00020: val_loss did not improve from 0.00015\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 6.0005e-04 - mean_absolute_error: 0.0221 - val_loss: 2.4426e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 21/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 5.1839e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00021: val_loss improved from 0.00015 to 0.00010, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 5.1787e-04 - mean_absolute_error: 0.0209 - val_loss: 9.8533e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 22/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 4.7586e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00022: val_loss did not improve from 0.00010\n",
      "56/56 [==============================] - 1s 27ms/step - loss: 4.7238e-04 - mean_absolute_error: 0.0200 - val_loss: 3.3732e-04 - val_mean_absolute_error: 0.0182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 4.6077e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00023: val_loss did not improve from 0.00010\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 4.6077e-04 - mean_absolute_error: 0.0199 - val_loss: 2.5353e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 24/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 4.6622e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00024: val_loss did not improve from 0.00010\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 4.6445e-04 - mean_absolute_error: 0.0202 - val_loss: 1.3277e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 25/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 5.1601e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00025: val_loss improved from 0.00010 to 0.00009, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 5.1143e-04 - mean_absolute_error: 0.0213 - val_loss: 8.7542e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 26/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 4.7317e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 00026: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 4.7607e-04 - mean_absolute_error: 0.0198 - val_loss: 3.4489e-04 - val_mean_absolute_error: 0.0195\n",
      "Epoch 27/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 4.8534e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00027: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 4.8128e-04 - mean_absolute_error: 0.0205 - val_loss: 1.2433e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 28/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 4.0111e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00028: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 3.9843e-04 - mean_absolute_error: 0.0187 - val_loss: 1.4048e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 29/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 4.5399e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00029: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 4.5282e-04 - mean_absolute_error: 0.0200 - val_loss: 9.5923e-05 - val_mean_absolute_error: 0.0099\n",
      "Epoch 30/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 4.1496e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00030: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 4.1375e-04 - mean_absolute_error: 0.0192 - val_loss: 1.0266e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 31/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 4.0803e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00031: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 4.0978e-04 - mean_absolute_error: 0.0186 - val_loss: 8.9183e-05 - val_mean_absolute_error: 0.0090\n",
      "Epoch 32/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 4.2295e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 00032: val_loss did not improve from 0.00009\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 4.2234e-04 - mean_absolute_error: 0.0192 - val_loss: 3.1494e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 33/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.7252e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00033: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 3.7154e-04 - mean_absolute_error: 0.0179 - val_loss: 7.8821e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 34/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 3.7788e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00034: val_loss did not improve from 0.00008\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 3.7583e-04 - mean_absolute_error: 0.0181 - val_loss: 1.4230e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 35/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 3.7406e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00035: val_loss did not improve from 0.00008\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 3.7609e-04 - mean_absolute_error: 0.0178 - val_loss: 1.1770e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 36/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 4.2474e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00036: val_loss did not improve from 0.00008\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 4.2474e-04 - mean_absolute_error: 0.0194 - val_loss: 1.0005e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 37/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 3.7780e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00037: val_loss did not improve from 0.00008\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 3.7780e-04 - mean_absolute_error: 0.0183 - val_loss: 8.1419e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 38/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.6811e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00038: val_loss improved from 0.00008 to 0.00007, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3.6634e-04 - mean_absolute_error: 0.0176 - val_loss: 7.4728e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 39/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.4718e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00039: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3.4669e-04 - mean_absolute_error: 0.0176 - val_loss: 1.3856e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 40/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.5120e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00040: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 3.5121e-04 - mean_absolute_error: 0.0173 - val_loss: 2.3635e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 41/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 3.3884e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00041: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3.3776e-04 - mean_absolute_error: 0.0171 - val_loss: 7.8167e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 42/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 3.3797e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00042: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3.3797e-04 - mean_absolute_error: 0.0175 - val_loss: 1.7414e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 43/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 3.7079e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00043: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 3.7079e-04 - mean_absolute_error: 0.0182 - val_loss: 3.7488e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 44/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.8393e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00044: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 3.8360e-04 - mean_absolute_error: 0.0183 - val_loss: 1.4911e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 45/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 3.2716e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00045: val_loss improved from 0.00007 to 0.00007, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 3.2716e-04 - mean_absolute_error: 0.0166 - val_loss: 7.3849e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 46/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.3746e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00046: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3.3630e-04 - mean_absolute_error: 0.0174 - val_loss: 9.3245e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/56 [===========================>..] - ETA: 0s - loss: 3.3227e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00047: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 3.2854e-04 - mean_absolute_error: 0.0169 - val_loss: 1.2297e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 48/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.1245e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00048: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 3.1323e-04 - mean_absolute_error: 0.0166 - val_loss: 1.3084e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 49/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 3.2269e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00049: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3.2206e-04 - mean_absolute_error: 0.0169 - val_loss: 1.2810e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 50/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 3.4287e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00050: val_loss did not improve from 0.00007\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3.4287e-04 - mean_absolute_error: 0.0171 - val_loss: 1.3495e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 51/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 3.0751e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00051: val_loss improved from 0.00007 to 0.00006, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3.0751e-04 - mean_absolute_error: 0.0161 - val_loss: 6.3933e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 52/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.5417e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00052: val_loss did not improve from 0.00006\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 3.5363e-04 - mean_absolute_error: 0.0184 - val_loss: 8.1772e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 53/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.7533e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00053: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.7459e-04 - mean_absolute_error: 0.0154 - val_loss: 5.3102e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 54/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.8103e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00054: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 1s 27ms/step - loss: 2.8103e-04 - mean_absolute_error: 0.0155 - val_loss: 1.4719e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 55/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.0129e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00055: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 3.0177e-04 - mean_absolute_error: 0.0162 - val_loss: 7.2522e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 56/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.7671e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00056: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.7671e-04 - mean_absolute_error: 0.0154 - val_loss: 5.5028e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 57/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.9187e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00057: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.9114e-04 - mean_absolute_error: 0.0161 - val_loss: 1.3604e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 58/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.6667e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00058: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.6448e-04 - mean_absolute_error: 0.0154 - val_loss: 7.3668e-05 - val_mean_absolute_error: 0.0090\n",
      "Epoch 59/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.0126e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00059: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3.0114e-04 - mean_absolute_error: 0.0158 - val_loss: 6.6787e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 60/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 3.3789e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00060: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3.3697e-04 - mean_absolute_error: 0.0173 - val_loss: 1.1444e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 61/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.5456e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00061: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.5456e-04 - mean_absolute_error: 0.0149 - val_loss: 5.2505e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 62/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.7098e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00062: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.7098e-04 - mean_absolute_error: 0.0152 - val_loss: 1.0104e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 63/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.9393e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00063: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.9311e-04 - mean_absolute_error: 0.0162 - val_loss: 7.8468e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 64/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.8813e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00064: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.8813e-04 - mean_absolute_error: 0.0156 - val_loss: 6.1925e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 65/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 3.0496e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00065: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3.0561e-04 - mean_absolute_error: 0.0164 - val_loss: 6.2336e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 66/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.6691e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00066: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.6805e-04 - mean_absolute_error: 0.0155 - val_loss: 4.9210e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 67/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5449e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00067: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5338e-04 - mean_absolute_error: 0.0150 - val_loss: 6.6841e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 68/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.6822e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00068: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.6822e-04 - mean_absolute_error: 0.0154 - val_loss: 1.3869e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 69/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.8508e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00069: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.8362e-04 - mean_absolute_error: 0.0157 - val_loss: 5.5448e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 70/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5564e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00070: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.5526e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3160e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/56 [============================>.] - ETA: 0s - loss: 2.5733e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00071: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 2.5775e-04 - mean_absolute_error: 0.0153 - val_loss: 6.0557e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 72/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4716e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00072: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.4606e-04 - mean_absolute_error: 0.0148 - val_loss: 9.1687e-05 - val_mean_absolute_error: 0.0105\n",
      "Epoch 73/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.5362e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00073: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.5103e-04 - mean_absolute_error: 0.0148 - val_loss: 4.6099e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 74/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4193e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00074: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 2.4167e-04 - mean_absolute_error: 0.0145 - val_loss: 5.5637e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 75/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.2420e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00075: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.2420e-04 - mean_absolute_error: 0.0145 - val_loss: 7.1562e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 76/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.9706e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00076: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.9706e-04 - mean_absolute_error: 0.0165 - val_loss: 1.6805e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 77/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.6269e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00077: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.6302e-04 - mean_absolute_error: 0.0155 - val_loss: 8.0419e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 78/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.9347e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00078: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.9251e-04 - mean_absolute_error: 0.0162 - val_loss: 5.4782e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 79/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.3610e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00079: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.3610e-04 - mean_absolute_error: 0.0143 - val_loss: 5.7757e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 80/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5225e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00080: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.5200e-04 - mean_absolute_error: 0.0144 - val_loss: 5.6014e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 81/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.5143e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00081: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5239e-04 - mean_absolute_error: 0.0153 - val_loss: 6.7509e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 82/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5407e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00082: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5339e-04 - mean_absolute_error: 0.0147 - val_loss: 6.3417e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 83/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.2682e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00083: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2682e-04 - mean_absolute_error: 0.0143 - val_loss: 5.5730e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 84/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.7077e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00084: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.7067e-04 - mean_absolute_error: 0.0156 - val_loss: 1.6629e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 85/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.4396e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00085: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.4392e-04 - mean_absolute_error: 0.0147 - val_loss: 6.6659e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 86/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.7703e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00086: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.7940e-04 - mean_absolute_error: 0.0157 - val_loss: 9.2906e-05 - val_mean_absolute_error: 0.0110\n",
      "Epoch 87/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.6433e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00087: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.6641e-04 - mean_absolute_error: 0.0156 - val_loss: 8.5361e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 88/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.7889e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00088: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.7834e-04 - mean_absolute_error: 0.0162 - val_loss: 6.2302e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 89/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5322e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00089: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5353e-04 - mean_absolute_error: 0.0154 - val_loss: 1.1252e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 90/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1858e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00090: val_loss did not improve from 0.00005\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.2018e-04 - mean_absolute_error: 0.0140 - val_loss: 7.8032e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 91/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.5063e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00091: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 2.5063e-04 - mean_absolute_error: 0.0150 - val_loss: 4.0929e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 92/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.5039e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00092: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.4919e-04 - mean_absolute_error: 0.0152 - val_loss: 4.9546e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 93/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5211e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00093: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5211e-04 - mean_absolute_error: 0.0156 - val_loss: 6.7399e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 94/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4066e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00094: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.4015e-04 - mean_absolute_error: 0.0145 - val_loss: 3.9361e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 95/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.5172e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00095: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5066e-04 - mean_absolute_error: 0.0153 - val_loss: 3.7936e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 96/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.2242e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00096: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2126e-04 - mean_absolute_error: 0.0140 - val_loss: 5.6579e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 97/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3773e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00097: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.3731e-04 - mean_absolute_error: 0.0147 - val_loss: 4.9523e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 98/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0839e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00098: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0954e-04 - mean_absolute_error: 0.0138 - val_loss: 4.4092e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 99/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.2822e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00099: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2813e-04 - mean_absolute_error: 0.0145 - val_loss: 9.4906e-05 - val_mean_absolute_error: 0.0109\n",
      "Epoch 100/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4409e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00100: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.4328e-04 - mean_absolute_error: 0.0147 - val_loss: 7.1759e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 101/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.6268e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00101: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.6146e-04 - mean_absolute_error: 0.0160 - val_loss: 4.1978e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 102/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.6563e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00102: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.6563e-04 - mean_absolute_error: 0.0158 - val_loss: 4.3008e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 103/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3267e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00103: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.3468e-04 - mean_absolute_error: 0.0146 - val_loss: 9.3267e-05 - val_mean_absolute_error: 0.0104\n",
      "Epoch 104/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.6565e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00104: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.6280e-04 - mean_absolute_error: 0.0155 - val_loss: 1.4086e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 105/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.2191e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00105: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2195e-04 - mean_absolute_error: 0.0141 - val_loss: 6.1782e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 106/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.3480e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00106: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.3517e-04 - mean_absolute_error: 0.0145 - val_loss: 4.5470e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 107/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.4877e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00107: val_loss did not improve from 0.00004\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.5313e-04 - mean_absolute_error: 0.0155 - val_loss: 1.5439e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 108/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4162e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00108: val_loss improved from 0.00004 to 0.00003, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.4285e-04 - mean_absolute_error: 0.0151 - val_loss: 2.9979e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 109/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.3309e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00109: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 2.3413e-04 - mean_absolute_error: 0.0143 - val_loss: 7.8730e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 110/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3435e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00110: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.3409e-04 - mean_absolute_error: 0.0149 - val_loss: 8.8670e-05 - val_mean_absolute_error: 0.0108\n",
      "Epoch 111/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3671e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00111: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.3680e-04 - mean_absolute_error: 0.0150 - val_loss: 4.3356e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 112/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1078e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00112: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.1043e-04 - mean_absolute_error: 0.0140 - val_loss: 4.7154e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 113/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3132e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00113: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.3103e-04 - mean_absolute_error: 0.0145 - val_loss: 6.8166e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 114/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.1087e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00114: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.1087e-04 - mean_absolute_error: 0.0138 - val_loss: 6.0873e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 115/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1037e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00115: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.1026e-04 - mean_absolute_error: 0.0140 - val_loss: 4.8821e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 116/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.2143e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00116: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.2240e-04 - mean_absolute_error: 0.0144 - val_loss: 7.5150e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 117/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.4815e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00117: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.4579e-04 - mean_absolute_error: 0.0153 - val_loss: 4.9718e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 118/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1967e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00118: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1995e-04 - mean_absolute_error: 0.0145 - val_loss: 5.2234e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 119/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1181e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00119: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.1326e-04 - mean_absolute_error: 0.0143 - val_loss: 4.2474e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 120/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.2163e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00120: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.2129e-04 - mean_absolute_error: 0.0142 - val_loss: 5.0000e-05 - val_mean_absolute_error: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.2478e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00121: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.2487e-04 - mean_absolute_error: 0.0144 - val_loss: 2.7424e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 122/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.6469e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00122: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.6411e-04 - mean_absolute_error: 0.0163 - val_loss: 9.9427e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 123/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4902e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00123: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.5251e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3845e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 124/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1171e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00124: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 2.1340e-04 - mean_absolute_error: 0.0141 - val_loss: 3.5646e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 125/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 1.9911e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00125: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.0171e-04 - mean_absolute_error: 0.0137 - val_loss: 7.7268e-05 - val_mean_absolute_error: 0.0098\n",
      "Epoch 126/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1511e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00126: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.1638e-04 - mean_absolute_error: 0.0141 - val_loss: 1.0503e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 127/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.3885e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00127: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.3885e-04 - mean_absolute_error: 0.0146 - val_loss: 5.6450e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 128/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1094e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00128: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.1144e-04 - mean_absolute_error: 0.0142 - val_loss: 4.1831e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 129/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.6105e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00129: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.6040e-04 - mean_absolute_error: 0.0154 - val_loss: 1.5184e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 130/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.3064e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00130: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.3064e-04 - mean_absolute_error: 0.0147 - val_loss: 6.2270e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 131/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.2718e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00131: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.2718e-04 - mean_absolute_error: 0.0145 - val_loss: 5.3295e-05 - val_mean_absolute_error: 0.0062\n",
      "Epoch 132/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5136e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00132: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.5248e-04 - mean_absolute_error: 0.0150 - val_loss: 1.1512e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 133/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3966e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00133: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.4022e-04 - mean_absolute_error: 0.0151 - val_loss: 4.9006e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 134/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0849e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00134: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.0885e-04 - mean_absolute_error: 0.0139 - val_loss: 4.2584e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 135/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0623e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00135: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.0638e-04 - mean_absolute_error: 0.0139 - val_loss: 3.9816e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 136/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1731e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00136: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1697e-04 - mean_absolute_error: 0.0140 - val_loss: 3.6655e-05 - val_mean_absolute_error: 0.0058\n",
      "Epoch 137/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3943e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00137: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.4190e-04 - mean_absolute_error: 0.0151 - val_loss: 4.5639e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 138/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.6543e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00138: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 2.6543e-04 - mean_absolute_error: 0.0160 - val_loss: 3.3965e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 139/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1680e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00139: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1540e-04 - mean_absolute_error: 0.0140 - val_loss: 4.7099e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 140/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.4980e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00140: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.4907e-04 - mean_absolute_error: 0.0153 - val_loss: 3.6698e-05 - val_mean_absolute_error: 0.0061\n",
      "Epoch 141/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1116e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00141: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.1146e-04 - mean_absolute_error: 0.0143 - val_loss: 6.3062e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 142/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.0676e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00142: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.0779e-04 - mean_absolute_error: 0.0138 - val_loss: 4.9817e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 143/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1203e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00143: val_loss improved from 0.00003 to 0.00003, saving model to results/2020-12-10_SPY-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1150e-04 - mean_absolute_error: 0.0141 - val_loss: 2.9830e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 144/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1290e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00144: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1361e-04 - mean_absolute_error: 0.0140 - val_loss: 1.2354e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 145/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1672e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00145: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.1719e-04 - mean_absolute_error: 0.0142 - val_loss: 3.5848e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/56 [===========================>..] - ETA: 0s - loss: 2.0469e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00146: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.0552e-04 - mean_absolute_error: 0.0140 - val_loss: 8.1790e-05 - val_mean_absolute_error: 0.0100\n",
      "Epoch 147/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.4451e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00147: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.4475e-04 - mean_absolute_error: 0.0156 - val_loss: 4.8940e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 148/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3331e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00148: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.3395e-04 - mean_absolute_error: 0.0147 - val_loss: 4.9058e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 149/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1105e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00149: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.1084e-04 - mean_absolute_error: 0.0141 - val_loss: 1.0196e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 150/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.7089e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00150: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.7089e-04 - mean_absolute_error: 0.0162 - val_loss: 5.1856e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 151/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.3550e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00151: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.3550e-04 - mean_absolute_error: 0.0151 - val_loss: 5.1386e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 152/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1665e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00152: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1730e-04 - mean_absolute_error: 0.0139 - val_loss: 4.0146e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 153/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0342e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00153: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0287e-04 - mean_absolute_error: 0.0139 - val_loss: 4.9437e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 154/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1441e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00154: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1377e-04 - mean_absolute_error: 0.0141 - val_loss: 5.3631e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 155/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.1603e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00155: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1603e-04 - mean_absolute_error: 0.0141 - val_loss: 3.2530e-05 - val_mean_absolute_error: 0.0056\n",
      "Epoch 156/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.5237e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00156: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.5162e-04 - mean_absolute_error: 0.0155 - val_loss: 7.7008e-05 - val_mean_absolute_error: 0.0102\n",
      "Epoch 157/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0341e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00157: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0395e-04 - mean_absolute_error: 0.0142 - val_loss: 6.0331e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 158/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1970e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00158: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2026e-04 - mean_absolute_error: 0.0144 - val_loss: 4.1774e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 159/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1457e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00159: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.1446e-04 - mean_absolute_error: 0.0139 - val_loss: 9.9292e-05 - val_mean_absolute_error: 0.0118\n",
      "Epoch 160/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1860e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00160: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.1814e-04 - mean_absolute_error: 0.0143 - val_loss: 3.3968e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 161/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.9772e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00161: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 1.9796e-04 - mean_absolute_error: 0.0138 - val_loss: 6.8874e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 162/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0628e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00162: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0602e-04 - mean_absolute_error: 0.0141 - val_loss: 4.0240e-05 - val_mean_absolute_error: 0.0059\n",
      "Epoch 163/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0689e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00163: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0686e-04 - mean_absolute_error: 0.0137 - val_loss: 5.0844e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 164/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.2678e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00164: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2464e-04 - mean_absolute_error: 0.0146 - val_loss: 3.3572e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 165/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1020e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00165: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.1129e-04 - mean_absolute_error: 0.0140 - val_loss: 6.8873e-05 - val_mean_absolute_error: 0.0100\n",
      "Epoch 166/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.0341e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00166: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 2.0341e-04 - mean_absolute_error: 0.0139 - val_loss: 1.3434e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 167/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.0944e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00167: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0875e-04 - mean_absolute_error: 0.0141 - val_loss: 4.0671e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 168/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.5082e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00168: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.5188e-04 - mean_absolute_error: 0.0156 - val_loss: 5.6800e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 169/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.2005e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00169: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2050e-04 - mean_absolute_error: 0.0143 - val_loss: 6.8985e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 170/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1741e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00170: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.1674e-04 - mean_absolute_error: 0.0141 - val_loss: 4.8035e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 171/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.3259e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00171: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.3203e-04 - mean_absolute_error: 0.0146 - val_loss: 3.6343e-05 - val_mean_absolute_error: 0.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.2790e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00172: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.2562e-04 - mean_absolute_error: 0.0145 - val_loss: 5.4055e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 173/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0354e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00173: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 2.0362e-04 - mean_absolute_error: 0.0139 - val_loss: 4.6619e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 174/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0934e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00174: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0884e-04 - mean_absolute_error: 0.0141 - val_loss: 4.5896e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 175/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.9143e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00175: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 1.9209e-04 - mean_absolute_error: 0.0137 - val_loss: 6.5655e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 176/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1560e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00176: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.1486e-04 - mean_absolute_error: 0.0141 - val_loss: 7.8143e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 177/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1335e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00177: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1323e-04 - mean_absolute_error: 0.0143 - val_loss: 6.8124e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 178/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1524e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00178: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.1381e-04 - mean_absolute_error: 0.0143 - val_loss: 3.0535e-05 - val_mean_absolute_error: 0.0054\n",
      "Epoch 179/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.0580e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00179: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.0645e-04 - mean_absolute_error: 0.0139 - val_loss: 7.5395e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 180/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.0393e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00180: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.0421e-04 - mean_absolute_error: 0.0139 - val_loss: 3.3834e-05 - val_mean_absolute_error: 0.0057\n",
      "Epoch 181/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.9894e-04 - mean_absolute_error: 0.0135\n",
      "Epoch 00181: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 1.9894e-04 - mean_absolute_error: 0.0135 - val_loss: 3.2142e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 182/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.9890e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00182: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 1.9911e-04 - mean_absolute_error: 0.0136 - val_loss: 5.9006e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 183/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.2806e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00183: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2783e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3415e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 184/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.3501e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00184: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.3342e-04 - mean_absolute_error: 0.0149 - val_loss: 7.5580e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 185/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 1.9693e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00185: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0032e-04 - mean_absolute_error: 0.0138 - val_loss: 8.8348e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 186/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.9900e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00186: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 1.9891e-04 - mean_absolute_error: 0.0136 - val_loss: 3.1568e-05 - val_mean_absolute_error: 0.0053\n",
      "Epoch 187/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.9842e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00187: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 1.9889e-04 - mean_absolute_error: 0.0138 - val_loss: 9.0404e-05 - val_mean_absolute_error: 0.0098\n",
      "Epoch 188/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.2346e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00188: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.2346e-04 - mean_absolute_error: 0.0146 - val_loss: 9.3317e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 189/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0069e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00189: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.0013e-04 - mean_absolute_error: 0.0137 - val_loss: 4.2201e-05 - val_mean_absolute_error: 0.0055\n",
      "Epoch 190/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.1830e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00190: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.1636e-04 - mean_absolute_error: 0.0142 - val_loss: 3.2011e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 191/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 1.9430e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00191: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 1.9739e-04 - mean_absolute_error: 0.0138 - val_loss: 3.3648e-05 - val_mean_absolute_error: 0.0052\n",
      "Epoch 192/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.0363e-04 - mean_absolute_error: 0.0137\n",
      "Epoch 00192: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.0435e-04 - mean_absolute_error: 0.0137 - val_loss: 8.6423e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 193/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.2318e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00193: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 2.2318e-04 - mean_absolute_error: 0.0146 - val_loss: 6.9887e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 194/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1043e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00194: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.0990e-04 - mean_absolute_error: 0.0140 - val_loss: 6.8116e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 195/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 1.9647e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00195: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 1.9652e-04 - mean_absolute_error: 0.0135 - val_loss: 5.2346e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 196/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.1520e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00196: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 2.1466e-04 - mean_absolute_error: 0.0143 - val_loss: 9.0810e-05 - val_mean_absolute_error: 0.0109\n",
      "Epoch 197/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 1.9533e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00197: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 1.9527e-04 - mean_absolute_error: 0.0138 - val_loss: 5.7376e-05 - val_mean_absolute_error: 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      "54/56 [===========================>..] - ETA: 0s - loss: 2.0903e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00198: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 2.0937e-04 - mean_absolute_error: 0.0140 - val_loss: 5.9360e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 199/200\n",
      "56/56 [==============================] - ETA: 0s - loss: 2.0326e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00199: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 2.0326e-04 - mean_absolute_error: 0.0140 - val_loss: 4.4484e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 200/200\n",
      "55/56 [============================>.] - ETA: 0s - loss: 2.2004e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00200: val_loss did not improve from 0.00003\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 2.2008e-04 - mean_absolute_error: 0.0142 - val_loss: 3.1937e-05 - val_mean_absolute_error: 0.0053\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 352.10$\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data, 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Accuracy Score: 0.5430224150397687\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6490368319261997, 1.9549996584710554)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABIKklEQVR4nO2dd3hUVdrAfyc9IaQACb33Il0FBSkqIuqirr27utZde++r7qdr17Vid9e2dgULKiIWQAHpNfQWEgKk9znfH+fO3JlkJpmEaUne3/Pw3HPPPffedy6Td859z1uU1hpBEASheRAVbgEEQRCEwCFKXRAEoRkhSl0QBKEZIUpdEAShGSFKXRAEoRkRE64bt2vXTvfo0SNctxcEQWiSLF68eK/WOsPX8bAp9R49erBo0aJw3V4QBKFJopTaWtdxMb8IgiA0I+pV6kqprkqpH5RSq5VSq5RS13oZo5RSzyilspRSy5VSI4MjriAIglAX/phfqoAbtdZLlFKtgcVKqW+11qvdxhwP9LX+HQ68YG0FQRCEEFLvTF1rvVtrvcRqFwJrgM41hk0H3tKGBUCaUqpjwKUVBEEQ6qRBNnWlVA9gBLCwxqHOwHa3/R3UVvwopS5TSi1SSi3Kzc1toKiCIAhCffit1JVSycBHwHVa64LG3ExrPUNrPVprPTojw6dHjiAIgtBI/FLqSqlYjEJ/W2v9sZchO4GubvtdrD5BEAQhhPjj/aKAV4E1WusnfAz7HLjA8oIZA+RrrXcHUE5BEISmT3kh/PwkVJYG7Rb+eL8cCZwPrFBKLbX67gC6AWitXwS+BKYBWUAJcHHAJRUEQWjqPNTFaigYd11QblGvUtda/2wkqHOMBq4OlFCCIAjNGhW8uE+JKBUEQQgFDofdrioL2m1EqQuCIISCnYvtdnzroN0mbAm9BEEQWhQHrDxcVy2EzAFBu43M1AVBEIKNoxo+usS0E9OCeitR6oIgCMHk12fh/jb2fkJqUG8nSl0QBCGYzHvEcz82Mai3E6UuCIIQTNxn5pd8G/TbiVIXBEEIJirabscmBf12otQFQRCCiXugUVyroN9OlLogCEJDWfAi7FxS/7iCXbBvo72f1Mb32AAhfuqCIAgN5etbzfa+/LrHbfrRcz/Ini8gM3VBEISGUV3p/9ivbrHbp7wUeFm8IEpdEAShIZQX+j62bSGU7rf303uYbVJbGHZWUMVyIkpdEAShIfhS6o5qeG0KvDXd7kvONNsrfw2+XBai1AVBEBqCU6lHxXr2VxSb7e5ldl/Wd9BjPLTuEBrZEKUuCILQMNZ/ZbY1My1WlnjuO5V8eaNKOjcaUeqCIAgNYc6DZlsz3N+pxAG0hgpLyY84PzRyWYhSFwRBaAxp3T33K4rsdtb39sw9BFGk7ohSFwRB8JfKMoiOM+3oGjb10gN2+72zYdsC0w5yAq+aiFIXBEHwl5K9UF1h2o6q2secVFfAJ5eZtszUBUEQIpSSfXa7stTzWPFevCIzdUEQhAilKMdsU7t5zszB0/wCEG+lBBClLgiCEKHsWWG2vSbYCt5JeYExtXQcZnVos4k0pa6Uek0plaOUWunjeKpS6gul1DKl1Cql1MWBF1MQBCECKMqBuGRI7QpVZVBt2dW1hvnPGpPM5fOgXX/bPz0CbepvAFPrOH41sFprPQyYCDyulIo7eNEEQRAijJJ9kNgGYiwV9/Ik2L8F9qyyBliz84QU+5xIm6lrrecB++oaArRWSikg2RpbVcd4QRCEpkdZPix/D1q1s5N2ZS+Hb++tHTWaOdBuR5pS94NngYHALmAFcK3W2uFtoFLqMqXUIqXUotzc3ADcWhAEIQRUV8LD3UxbOwBlH9MOzyReAHFuKQQi0PxSH8cBS4FOwHDgWaVUireBWusZWuvRWuvRGRkZAbi1IAhCCMj6zm6PurDGQW37rjtxBiapKDtYKUQEQqlfDHysDVnAZmBAAK4rCIJQNzuXNKxoRWP59h6zPfpeGHUx6Gr72IHtdju5vdk6lXpMIii3WX0ICIRS3wYcDaCUag/0BzYF4LqCIAi+eXGcWaj87r7g3qe8CPauN+0hfzZK2uFmYd691G5f8YvZOmfnMaH3GfHHpfFdYD7QXym1Qyl1iVLqCqXUFdaQB4AjlFIrgO+BW7XWPkKrBEEQAkBFMWRbPuNZ3wf3Xrlr7bYz3W58cu1xGQMh2TIrO2fqUaEvA13vHbXWZ9dzfBcwJWASCYIg1Mf+rXa7be/g3uvANrvtLBw97nqzAFqwC36zao+eOsMe55ypq9DHd0pEqSAITY//XWC2yR1qh+cHmnzLZn7bNoiKNu3YRBh3HZS6eXs7FT5A3kazLdoTXNm8IEpdEISmRWUZ5G0w7c6jPAs9B4P9W4zCdlfa3ohzM8mEQZk7EaUuCELTYOYNcF8qbJht98UmQHV5cO+bvQLaH+L9mHtgkbudPcQeL+6IUhcEoWmw6FXP7dSHje26qsL3OQfDvk2wZibk74D0Ht7HTLrLbrv7ow8/z2w7jw6ObHUQ+qVZQRCEhlJeaLdL8sy2/WDIWV078CdQPD/WJO1CQUon72Nat7fb7rPz/lPhrlzbBh9CRKkLghD5rP7cbjtdGRNSITo+eOaXqjKroW1XxoYQBh91EPOLIAhNgc+uqt0Xn2JMHsGIKK15zcS0wN8jSMhMXRCEyCZ/p/f+hFQzG646iJm6wwFF2bXNK3s3eO4PP9f3NS751mRujBBkpi4IQuSSvwOeHGTaIy/wPOacqTsqTZGKxjD/WXhioO1X7uSFsZ77ddnGux4GbXo17v5BQJS6IAiRy0+P2+1x13sei46xPU4aa4LZPM9sc9f5HtPBhztjhCLmF0EQIpPiPFj0mr2f2tWYOl491u5zKfXyxi1MOhdAS2vUAepwCCS1hQs+a/g1w4zM1AVBiEyWvGm3T3/DJMnqehik94S07qY/Jt5sGztTdwYM5e+w+3LWGg+bpMDZybXWfLViN2WV1fUPPkhkpi4IQmSydpbZTn0YBp9i91/zh21Dd2ZDbOxiqbNwdIk1U9+5GF6ebNpO00wAWL+niCvfXsKUQe2ZcUFwA5JEqQuCEHmUF8HORTDmahhzpecxpexAn2jnTL2RAUhVpWbrLHqRu94+FsDaogVl5k1i9urg54QR84sgCJGHs/BFl3pmtS6bemOVujXDd1gzdufMH2D0xY27pheKyqtc7b1F5ejGeuv4gSh1QRAij7wN0CoDBp1c97iYg1XqVtSoo9pzC3DkdY27Zg1KK6pZsSPftT/6we/4NogzdjG/CIIQWVSWwdZfYeBJEFXPvPNgZ+qVllL/4z/mX9u+Zv+mDQHLtHjTB8uYtWK3R9/6PYVMGdwhINeviSh1QRAig6Ic+PAvZlG0ugJ6Taz/HKdSb0ymRkc1bPvVsy9vg7HTB9DzZeHmfbX6qh1eBgYIMb8IghAZLH0HtvwEs24w+6071n+OU6m/NgXm/qth93Nme6xJevf63xAaQFKcHY16eM82AHROD9wibE1kpi4IQmRQM6qz04j6z0ntYrfn/h9MvNX/+5Xle+/3lTu9kbhbcd6/fKzvgQFClLogCOHnfxfAarfozTFX+5ckyxmE1Bh8KfXWgbN15xSUsTWvBIC46NAYRsT8IghCeCnL91ToUDt5ly8OxkxSUtvWDUBcI3Kn++DjP+wMkx9eGfxZOohSFwQh3Pz0RO2+dn2Df9/C3d77A1jc4pnv7RS+Q7ukBey6dVGv+UUp9RpwIpCjtR7iY8xE4CkgFtirtZ4QOBEFQWi2ZK+EX54y7Zs3QtEek3clFGXgDmwFFW1HkzpxRqkGgFHd0/lpw14uOyp0qXn9mam/AUz1dVAplQY8D/xJaz0YOD0gkgmC0LxxVMOLR5r2n181NvT2g2HYWaG590+PQ2K63ec0+QRwpr42u5DR3dO5Y9rAgF2zPupV6lrreYAP4xMA5wAfa623WeNzAiSbIAjNmbwss80YAIecFtp7564125K9dl+clbExQDP18qpqcgvLKa8KolO6FwJhU+8HpCul5iqlFiulfK5wKKUuU0otUkotys3NDcCtBUFosmz/zWxPfCr09y4rMNtTX3HrtHwPA5TIq6jM5Hs5bnD7gFzPXwLh0hgDjAKOBhKB+UqpBVrr9TUHaq1nADMARo8eHbyMNoIgRDYOB3z+N9NOzgz9/Z3ujG3dbN0TbjaRrMPPCcgtSiqMrb59SkJArucvgVDqO4A8rXUxUKyUmgcMA2opdUEQBAD2udUEPRhf88ZSdsBsE9LghjUmS2NiOpzwWMBu4VTqreJDGw4UCPPLZ8A4pVSMUioJOBxYE4DrCoIQ6ZQXwpqZDS/8vHuZ2f71B1NrNNQ4Z+oJaZDSCdK6BfwWxRXG/OKeJiAU+OPS+C4wEWinlNoB3ItxXURr/aLWeo1S6mtgOeAAXtFarwyeyIIghJ3SA7BjEbz9Z7N/zv+g33GeY8qLTD3Rohw4/xPoONT0782Cjy4x7Y7DAidTjJ9mDkc1fHWLaSekBO7+NSgpNzP1pLjQ/mjVezet9dl+jHkUeDQgEgmCEPl8cgWs/8re3/hDbaW+9RfIWW3aP/4LznrbFKV4dpQ9JpD+6FVlpjxdfTP//O12270oRoApCdNMXSJKBUFoGN/c6anQwQQN1WTLT3Z77UxY/j/PKM7Lfgy8bA4/ClC/EwI/eGybuih1QRAiF4cD5j9bu7+4hoty/k749d+Q2AZ6H236Pv4rLH3XtI97CDoND4xM1y6HPsdY8lV7H5O9Eh4fCLuXQ6615HfUzYG5vw+cNvWmuFAqCEJLodhLbGHvo43d3B2n2eW4//MsHL12ptkOmh44mdK7Q69Jpl0z5N/JghegcBe8NN7ucwYbBYlSa6aeKDN1QRAikooSYycHmPIg3JcP9+yHNj1rK3tnBsQuh0LfY2HCbWZ/z0po2wdSOwdWNqdt3tdMPSG1dt9hlwVWhhoUOxdKYyPM+0UQBAGAZ0dDgZVK1pknJSoKkjtA6X5Y8aEd7l9qKfUkU+nHw9SigjCXVJbi1D5C8mtGicYlQ1xS4OWwqKx28MrPmwCICVEedSei1AVBMH7mC1+CNV/AGW9Bq7aexx0OW6EDxLu5AjqrD310Caz6BDIHwjwriMc5Q3YfHx24hFkunHnVfc3Ua+KoCrwMbjw7J4vCsuDewxei1AVBMJWH1nxu2jOvgzP/43ncfSE0tpVnjbbkDLu9dqZtNwfbLOJu/ohrFRCRPXDO/kv3QesauVZ2LIY//uvZF2Slvi67MKjXrwuxqQtCS0drW6GD54zcybb5drvHOM9j/gT9uAf5xAbB7FFZZrbvnOHZn78DXpkMRdme/UFW6gGsW93we4fv1oIghB2tYc4Dnn3eFhWd/uWHXgrH3Ot5zFeq2hMet9vu5pdDglBywWlLP7DNs7+i2Pv45OBmThzeNQ2AE4Z2DOp9vCHmF0EIN45q+Po24+ZXXQk9jwpN5R+tYeGLplgEwMVfw7tnQkyNRcXc9UY+gOMfrT0NjfGh1J1uhgCJaXDKS8Ybpm3vgIjvgbs5yJ0PLvben+RHUeuDoLLa5MJ5/PQApkHwE5mpC0K42b8FfpsBb5wA/znZRF6GgjVf2Mr68Cug+1ho1x/WzYIFL5qQe4APLcUYm+TdruDL/JLa1XN/2FnBUejgO6FYzirv/b5+iAJEhVUYIy7Eni8gSl0QwkPWd/BABhTl2orVyZ4Q5cPbvtBuT33YbKtKzfbrW82sHaBgl9ne7JYu1x1f5d8CWBauXny5Mroz7Gy44DPT7jwyqOJUVDuIiVJERfl4gwgiotQFIRx8fYcpyPDmibBhtunrP83YnvduqPvcQFCcZzxC4lPg+tW2+eKkp+0xWd9B4R7jgz7hVt9+3VFerLjdjwy8zHXhj1LvMQ56TYQLZ5pI1yBSUeUgLiY86lVs6oIQDiqKzNZZKxPg7HfhvXONS2D+zsBHXTr56FLYttAUijjjLc/7dB7lOXbzj4CGlDpkiW/tuX/Fz5DeM1DS+oev9ADuOBdre46ve1wA2F9cEfKcL05EqQtCqKkqNyaNLofBDqtO5y2bzXbomUap714WPKW+4gO73X5I3WM3/mC2dcmSkAq3bYO41ka5BjGdrU+8zdR3Lvbc735EaGQB1mYXMrhT8HK114UodUEIFaUHzALdt/cC2oTadx9rbL3OcPp2fc3WadsONOu+9tx33tcXy94x27pm6uDmBhkmi663ddJ9mz33E+v5rAHguR+yWLJ1P6t3F9Cvfaeg388botQFIRQ4HPDieJPv2+nz3XkkjDzfc5zTk6SqPDhyLHvXcz8hzb/z6lPq4cbbTL2mrT/IEUHlVdU8+s06136lo4El/gKEKHWh5bFvEzwzAi6aVTs6Mhh8cqVROvlugTGnvwHtB9ce60w8VRngmbpz8XX1p5793vy7pzxo3BHXf2Nm6jGJQS37FhC8KXXnukWIqJnrZWNOaO/vRJS60PLYbtmxf38l+Epda9uE4U7vyd7HO2fq7ko9byPkZdUuF+cvqz+H/51fu//Y+72PP+LvZrvgBbMNlikokHhV6iUhFaGg1LPq0tAuXiJzQ4AodaHlkZdltqH4o1/0mvd+b6H4YM/UZ98JlSUw4RbjrbJrCdyUZZJnae07grImOWtrK/RrlgIa2vSq+9xRF8L2Bf7dJ9wMORXmPQJt3IKbqqx8MOd/AmUFQRfBfaZ+83H9uWRciD2ALESpCy2LyjL4+SnTLtwV3HvtWgqzbvDs+/sSs2DqC/e0tHMfMkp973qzf2CrUer/SDMh+Bd8Wr8ML3lx32vjp7LpP82/cZFA5kDoOAxau+Vaca5L9BgfEo+cnEJzv+fPHcnUwR3CEngEEnwktDT2b7GLE2evMME1waAwG2ZMsPeTO8BVC0yYfJdRvs9zn4E7TQpOP/BCt0yDm37wTw7nuX9b5N94j3Mj3I5eExXlmU+9utz0eQuOCgLZBebNYFT39LApdBClLrQ0Ns4x2yFWhZ6aWf0afd0f4L5UswgLnpkP79wDN60zs8mGorU9ey874DvHiTfyNkJJnvmsTlfJhhAVZWbrY65u+LnhwFFl//+CMb9Ex/tvqjpI8orMTL1tqxCmR/BCvUpdKfWaUipHKVVnQgql1KFKqSql1GmBE08QAsw3t5uts+xaZY3UrPMehfnPNeyaRbkmERcYr5rqKii3iiQccgbE+pFv3Bf7t9gFIMoLaxd49kXJPph9l2l3HGq2HYZC9wYuDJ/9LkwNbkh9wMheYYKfsleY/aqKoCfucmfx1v0kxUWHvHxdTfx5L3kDeBZ4y9cApVQ08C9gdmDEEoQgkL/DbrfuYLbOxdJXjoXkTLtqz+FX+u/XvOYzz/0fH7ZrZp70VKPFBWDLz7DfCqL5bQYs+U/d45189jdY96VpH3GN2V7x08HJ0lRwrllUlYVMqe88UMpPG/aG5F71Ue+3Vms9D9hXz7C/Ax8Bfk4jBCEM7PjdbNv0NiXZwHiYgAnXdy/DdmCr/9fds8qYSK630rzOe9Qo4t6TD7502+d/s9v7NtmpZOvzXFk3y26HyPwQMTg/b+662nlparB9Xwm/ba5PvdXP6l3B967xl4N+T1BKdQZOAV7wY+xlSqlFSqlFubm59Q0XhMCyw1osvGiWnXHQV4DKzOvrv17JPsheCRu+NYmwnAWYwZhN0nschLCq7gW+ukrIleXb7bjkg5ChiVOUTXXmYMoqfSf7mvLkPM54ab7P4/7y17casRAdJAJh/HkKuFXr+nNfaq1naK1Ha61HZ2Rk1DdcEAJHRQnMf9a0Uzq6heNXeCpBJ3V5l5QXGXPNIz3hxSMhfzv0m2qO9TnGbEv3Q1Lbxsl66xa4bWvddTTL64hW3Pqr2XY4BC6f1zgZmjTKZLksL+TzVQcY+g/fVuFSS+HXpfjro7Laj7S/ISQQSn008J5SagtwGvC8UurkAFxXEAKHU0kf+lezdfotF+fAw908x/oKDHJda66dXdGJ00PFPWdL2z6NEpXEdE8Zuo+D8z7yHJO/DYp92HCd/We+HbxKQ5HM9oXw5CAoyaPcoVxViOoit7DxuXbmrLWtzm9fenijrxMoDlqpa617aq17aK17AB8CV2mtPz3Y6wpCQMm1Ei0dfbfZOpWw0wXRnS6Hmu2aL8zW4YAvroXlVspab/b2zAFm6ywykdTWpNENBEnp5g2g/SGe/TXfMBwO+O9p8L0V/t8quHU4I5bv/+FqVuFfrdc9lo95Y4i2bPgju6VxZJ/wP3N/XBrfBeYD/ZVSO5RSlyilrlBKXRF88QQhQOxcbAo3OGfATqXuHtADJpHVMZZScP4QHNgCi9+AT680+wW7jP/z3Xn2ec5cLm17w5/+DVctDNwCZbTlweFc9EuxbPeVNdIclO6HrG/N2wcc/CJtM8Bfpb4rv/FKvbjCmMkeOS30Raa9Ua9Lo9b6bH8vprW+6KCkEYRAU1Zg8qnkrjU2ZidR1h/71l9qnKBM9sSoGFtpFlipcp2RqAW7TNGI6BiY/jyU1vCeGHlBYD+DK3WAFXiU3h0KdsB758CV8yHeWgwt3R/Y+zYDqutQ6oVldgKurD2Fjb6HM5FXamIYioN4QSJKhebNw13hgXYmiVdye9/jzn7PbFu3NzPs2Fa2D3ueW83QN06EVR/b+cVHnGtnNQw0yZYvvfPHZZvlpeHM4HhgG+SstseXuL05tG1EBGkzpMpSccXltRed9xVXuNr5NTIsNoQCK5FX64TISKUlSl1ovtScuToDjrzRcwKMugj+/IrZd1TCby8ZL5cvrrXHbbECeHylzg0kf7VC3oedZbYD/2S27kmryt1mmPnbzfboe1pOoFE9OGfqm/cW1zq2v8RW5FUHUdDiQEkFsdGKhFj/TD3BJjJ+WgQhGLx8tNlOvN2YVPrWkY88Lsle5ATPoCRv9J4UGBnrIrUz3Oe2GHr6m7DoVVP+bt1X8PGlnkp952KIioUxV9kpfFs4dc3Uc9wWR6uqG6/UX/5pc/2DQojM1IXmx75NsPAl2LfR7PebCgNPghgfiZbqK74M0K1G0eKkMHg5REXBYX81NvSuh5k+Z/BUcR4seN543bRwhZ6n7SjSam1mz6Ve/NC37zdmrMTY6IOaqUcaMlMXmh8fXAy7l5r2uOtNnm1vZA4yNml/vEROfBJWfQIFO40Jpi77fChwesI4Z+oLrYDuomzv41sonZXx2S+tqK3Un/x2Pa0TYkhPiqPK0bgAomrrxyBcBTG8IUpdaF5obSt0gMn3+HYtnHQHvH8exCbVf93EdJh0e0BEDAg1lbqzmlOgfOObMG2VbZKaFL0UqqDEUurfr9nD3qJyDu/ZliLLJJPROr7RM/VXfzZxDgs359UzMnSI+UVoXrgvjl7xc92ZFp2ZLfyZqSdnHpxcgSY61hSEdin1jaYa0ikvhVeuCENZbqBO88slby7i1o9WkGvlPv/nKUOIjYqiqpGh/qusRF77ixvvPRNoZKYuNC++u89sT3vd0y/dG06Xxfpm6pd8G5mZDuOTjY+81mYdodvYyJQzjOy37OulFdV8u9qucuVMCzCsSxrRUdtcZhR/+W71HpbtOECslTv9+CF1eFaFGFHqQtOnZB+8cQKMOA+WvGn6uo2p/zxngYw4L0r9wpnw5omm3Xl0YOQMNKldTebJohyzYNoS87zUw/VRtwDG/OKeSXHbPvOD3qZVHLHRisoGeL9orbm0RlbGW48fEABpA4OYX4SmS1W5KSP3SE+z4PnNHab/qFsgpVP95w+cDhkD7CIS7vQcD/fsg9t3+F8sI9R0Gg7FuXZJvrTuYRUn7Aw+tVbX3tguxMVEUVLp6dL48FdrAUhPiiM6SjVopl7Tk2Zol1TXjD0SiBxJBKGhPNrHLiPnnuxqzJX+nZ+cAVcv9D3DjYqut8hCWElIM0m9di42+/EtOHc6UD3qL672ekdnJpc/Rsf0RJLioinz4v0CkGiVn2uI98uBEk/7eUZy6Erm+YOYX4SmR/5Ok4mv3K3azF+/hx/+aZRwUpvwyRZKElJNzvWvbzX7MS3bP3359n2MsNq7dDs26U6c2rYV2fllzFy+2+d5MVH+ped1siHHM5d9dFRkrWOIUheaFvu3wNOW33lqVxNoc/wjphblsfeHVbSQUzPvewiLLEci2mHPxoswRVBiohWJcdHs9pKF8ZqjTX6c+JgoCsr891658DXPKOPemZH1hiRKXWharP/GbONT4KoFLdvkUEup11HirgWwKaeAkVbbmfMlOiqKpDg7J8vgTikuN8QxPc0bXbvkeFbv9q/GqNaetvd/nz2C4wZHjucLiE1daGosfQcyB8OtW1u2QgdISPHcb+Ez9VnLdrja1ZZqi4lSJLol2nryzOGu9hFWQYvMlHj2FlX4tVia55bZEeCkYZ2Ii4ksNRpZ0giCL/I2wsIZJlp02JmR65ESShLSPPdVy34m0dh2cad6NuYX2yDRo23tQLPM1glUO7RHKl5f7D7Q+GIaoULML0Lks+oT+OAie/+Q08MmSkThbn6Jim18oetmwi+Owa52jKXgHQ5NkttM3dusOrO1ecPZtq+YDxZv57LxvYjx4aK4cpfJmjnz7+Po2S4yK0u17J92IfIpL4KPL7P3x93gnw96S8Bdqd+zF2Jbtk29lATuqzRVp2IwfumlldUeNnWAfu2TaZdsZ+zMTDFK/d7PV/HI1+v4eMlOn/fYsKeIpLhoBndKoVV8ZM6JI1MqoWVRnAeb58Lu5bBnJZz2mlFYZfnwn1OgusLkEh98crgljSziU+of0wKoqHK4AoLKMSXlYjH7JRXVpCcZBX75Ub0AmHXNeNydEDNbmx/DbXkmynR/SQUFZZWkJHiWp6usdvDZ0p10SE1ARXA6BlHqQnjZsxpeGOvZ93A3OPpeqCqzA2tkdl6bFj4zd3L2ywtYvNUkcivXRhGP6NwKtkBZZTVON/L0Vka514z+zLDML86ydA99tZaHvlrLl9eMZ1An88NZVF7FkHu/8RgfqYhSF0JLeZEppOwsWPHJ5Wbb5xiTNXGjVcLt+39AtNsfT+ag0MopNBmcCh2gHPO9ctrUSyqqKbcCi5J9mEsSYqO9pgpYtSvfpdT/PceuU3v/dD+KqoQRsakLoaOqAh7qDO+cAQe2Q+56yF5u6n2e9xGc/4nn+Opy6Hc83J0n7ouCX5RZ5henTb2koprOaSbStq4ZdqKX+qIV1Q4qqx18vXI3L/1o8qbPumYch/WM7IhlmakLwWdvlsnR4iyMvOkHeMpttnPSM3Z72Dmw7B17/5QXIFq+poJ/OGfq0ZZNvayymkvH9+KQLqmM6+O7BOH1x/bjgZmrPfru/GQld36y0qOvS7ofBVXCTL0zdaXUa0qpHKXUSh/Hz1VKLVdKrVBK/aqU8lE7TGixLH3bVujeSOtqt09+Hs56FzqNhBvXm4pDguAnTpt6nDJK/aRhnUiMi2Zi/0yfbooAh9eYfacmxtYac8e0AV77Iw1/zC9vAFPrOL4ZmKC1PgR4AJgRALmE5sLmefDzE6Z9yksmle2Yq+zjI87zHK8UDJgGl/0ArcNcB7QpkNwB2kgedSdlTpu6rmLtA1O5aqJ/z6Zfezsb523HD6BNK88i5XdOG8hlRzWN51zve63Wep5Sqkcdx391210AdAmAXEJTprLUzM7XfgnFOabvmH/AsLNMe+pDMPlu+PlJUxhaaDw3rpVqR25UOlWao4oEL3ZyX8TFRDH7+qNon5JAamIsb/26xeN4r4zIDDTyRqAXSi8BvvJ1UCl1mVJqkVJqUW5uboBvLQSd3HWw6Uffx7WG31+Ff3aAWTfCxu8hewUceimMu85zbFwSTL7Te9UhwX9EoXtQaSXyorr+kP+a9Gvf2mVeueekQXRvm8T9002U6qERvjjqTsBWoJRSkzBKfZyvMVrrGVjmmdGjRzeufLcQPp47zGwvnAm/PAWnvuyZu3zzjzDrhtrnDTwpJOIJ4cHh0Nz56QouPKIHAzqENyCqyqnUHVV1D6yHqUM6MnVIRwAuGNvjIKUKLQGZqSulhgKvANO11nmBuKYQwbx5ImR9B7PvhsVvmGRbYGbyAONvhLv32uM7ytp5c2Z3QRnv/radS95YVP/gAFOzuEWVds7UD06pN2UOeqaulOoGfAycr7Vef/AiCRHJGyfW7lv6X/MPIL2HKWABxl6ulEky5agUD5ZmQm5hOSUVVXSvkenQYQXtFFeEXpHWzIO+h3RKW/cg8YSHQy5LpFCvUldKvQtMBNoppXYA94Lx8NdavwjcA7QFnrfyIVRprSO0/LrQKNbOgi0/mfafnjVl5OY9CqV2JJ9LoYNt571uhYkSFZoFZ7w0n817i1n7wFSPRcgSq/5nzdqdoaCg1POeVcTwx8nfc0Rv3z7pzR1/vF/Oruf4pcClAZNIiDzmPWq21yyFNj1NOy4ZvrgGeh8Np86ARy13r2P+YZ+X0jGkYgrBo7Lawea9xYCZHY/sZr99OZNphQNv9+7aBAKEgomE6gl1U1lmkm6N/Zut0AFGXWj+aW1m5gNPMrlajrgmfLIKQWP5jnxX+5WfNvHoacNYtauAFTvzSY63Z+1a65BmMCzzptTbiFIXBN/sXGxysHQ/0vtx5x/wmf8NnUxCyFmx4wAAKQkx/LHtADf+bxlfr8quNa6s0kFinP/+4QdLaUX43hIiFUnoJXhn63w4sA1W/M/sdxsTXnmEkJKVU8h/F2wF4Itlu7jvC5MX5YKxPdidX+ZVoQMs2BRa57dwmn4iFZmpC7XRGl53ywyR1s3TH11o9lz73lJW7SqgX/vW3PzhMgCmD+/kqhLki4vf+J3bjh/ApeN61plrJVCIUq+NKHXBJncdzH24dpTnn18NjzxCWMgpLGPVLuMqeMZL8wE4fVQXHj19GAVllXz6x06GdU3j8J5tKaus5v6Zqz2KNj/81Vr6t2/NpAGZQZe1TMwvtRCl3pKpKIEZE2Cvj/CC1K5wzH3Q9bCQiiWEl+OenFerb1R34+2SkhDLx1d5rq8cO6g9ZZXVjHrwO1dfYXlofNa/WbUnJPdpSjRrpZ6VU8jS7fmcNkpyjHnl5yd8K3SAq3+T3CwtkP1e/M17tPOd0KpVfEytIswlQVbqDoem1x1fBvUeTZVmrdSPecLMOESp+2Dr/Np9l/8EHYeGXhYhoulVh1L3xpvztzK+X4ar6lCgqRlJKti0CO8Xh0Nyh9XC4YDtC00GxXsPmMIUvSZCxoBwSyaEEeffyumjuvDieaNc/Q0ttrxmdwFHPjyH93/fRlZOIWfPWEBBWWAiTh0OzYn//tnrseOHdAjIPZoyzXqm7uRf36zl9uMHhluMyKIkz+RladfPLkwxYFq4pRLCzCPfmKRsW/KKmTqkA5sfmkZltX8BRT/cNBGtNZMft9Mz3/rRCnpltGJTbjFD75vN6vuPIynu4NTOzgOlrvabfzmM/cUVXPf+UgAePDmyi0KHghYxU3cWjRUs3j4dHutj2q1lZiPYzFtv6hycN6Y7AEop4mL8UxM927WiV0btAuGbcotd7avfXnLQMuYUlgNw70mDmNAvg5NHdHYd81fW5ow8gZbGprmwYba930U8WwSb1bsLOGZgJtOHd65/sA+8mWqcfT+sO/jiODv2lwBwaI/asRMxUaLSWoT5pdlRWQblhRDXyninFOz2L3lWdSW8Nd20VTT0PVaSbgkulm0/AMB3a3IO6joz/z6OjTlFHN6rLRtyCiksq+KrFdm89svmes8tq6z2yAD52dKdDOiQQv8Odg3Ra99bCuA1ECo2WipBiVJvaqz7Ct49q3b/MfdBzwmwb5NJrhXjZWHrpyfs9r37giai0DTZus/MgG+Z2v+grtM+JYH2KQkArkpIxeVVLqVeWlHNxMd+YE9BOc+dM5IThpqJxbLtB5j+3C8AXHN0XxwOzbM/ZAFw05R+XD2pDxXVdirntq1qf8dDEcUa6bQYpV5Z7SC2qf6Hb5prZthp3eHAVu9jtvwC393n2Xfx19B9rGkX58Hc/zPtG9YES1KhCTN3bQ6pibH85cie9Q9uIBP7Z3LO4d14Z+E2Bt7ztav/6neWMLH/cbSKj+Gdhdtc/c98v8Hj/Mdmr+ex2es9SrJGR8ms3BtNVMv5R2JsNDHWf3xxiCLcAs6WX2yTiVOh95oIN22AM9+GRMuumPVt7XPfPAm+uA5+fwVy15q+w6+AlE7BllpoYny2dCcf/7GT0d3TPcwfgWR8H++FK5Zs289NHyzj/UXbax27+TjPtwZteSdfPqGXR/8tU/tzwdjugRG0idNslPrWvGJ63DaLH9Yae6DWmrKqatomxwHw8k9N1APmPatGSUpn6Hc8xLYyyjw5EwaeCLfWYad0VMLi12HWjbDNCjQ67LLgyyyEncVb9/PyvE1o7TtGo9qhWbgpj2qHdtmpb5jSL2gytU9NcLW/vGY8391wFADnv/obHy7eAcDE/hlce3RfAA7pnMrVk/qw6K5juGZyH49rTRnk6bV11cQ+3D9d3BmhGZlffttsbMQfLtnBpAGZlFRUozV0SEkwtrsfNnLzcU0ssKayFMryoedRcOEXps9ZlMIX05+D4efC69Ng2692/5wHICEN2vTyearQfPj7O0vYlV/G0QMzXW6GJRVVKBSJcdEUlVcx5N5vPM4Z3jWNwZ1SgyaT084OMKhTClprJvbPYK7lEXPZUb24+bj+xEZHMX14J5ITjHpqlxzPDVP6c8OU/kx+fC5H9G7rykUj1KZZKPXCskre+928usVZdvMiy9xivkj5vk6NLBzVEOX26rvFLBpxxLV2nzeFftz/wTd3wCkvwTBrEfXiL+HLm6D/NGNrz14OSW3r/kEQmgXZ+WXsyi8DYM7aHDbkFPHE7PWs21PI4T3bcOrIzizcVHuh/OULgltaOLOGq6NSijcuPoxqh65lH/fm7w4w58aJwRKv2dAslPq17y1l8VZTBPmTP3ayv6SCu08cBEDPDDtnxRPfricpLporJvQOi5x1UlkGLx5p7OXDz4H130Bry90ws543jLFXm3/uKAUnPG7ahdnw2VVQfPA+wkLk4/xbAHhwluei+MLN+1i42Vbod584iHXZBTx06tCgLzzGRkdx3OD2TK6RklcWPANLk1fq+4srmGPZ0ZPioimpqGbuulymDjZf3EO7t2FZrwP8vmW/a0U9bEq9othUE8qskbKgOA8etcwieVlmYdOdVhkHd9+uh5ttTELd44Rmwe58E0Z/4tCOzFy+u86xF47tHlI3wJfOD+7bgNCUF0rLjEnlx/W5dFV7mDlsAavvnsApVsiwM4AiOSGG8X0zqHZL6lXl5utqX6/A/Asmb58Oz4+B548wCt6J07vFF958zhtC294w8XaY+tDBXUdoEuw8UEpibDTDuqQBMLJbGpsfmsb/nXJIrbHi1938qPd/VCn1mlIqRym10sdxpZR6RimVpZRarpQaGXgxa3BfKjzcDXYsZvu+El6OfYIh656B+c/y2LROxEYrvluzB9Akx8eQlhTrefoXq2pf84Uj4elhrh8LfnnG9FUHyBVy+++w1bKR56wy97ovFfZvNfsxiXDGW7XPu2zuwd9bKZh4Gxxy2sFfS4hoFmzK4/VftlBaWc2fR3Uhs3U8V03sg1KKsb3beoz97GofxcSFJo0/5pc3gGcBLxoHgOOBvta/w4EXrG1wcLiVr1r6NqVFUxgQZfm3znmQ6DkPsiEWiIUPqo4iOX4Sma09zQ7z1u/1vOamHyHfCnz49h5Y/IZ97O0/wwWfQVU57FwM3Y9omLz7NpmZeKVZuGLy3cYTxWnfftrKXX78w9Dn2NrndxrRsPsJLZaSiirOmrEAgAEdWtOmVRy/3XmM63jPdq148sxh9M1szZDOwfNyEcJLvTN1rfU8oK6Y8unAW9qwAEhTSgUvocjKj23ZVnxA3gqrhFan2i8Ip8fMIzk+msN72Yl/Th3Z2cMUQ2UpvPUne99doYOJ5nx6ODzWF14/HvZ4meX7oqoCnhlh7OjFOdB5NBx1k/exbXqZPC73HoATnzR9cd49AATBySNfr3WtFWVbHi8Atx3vfXH9lBFdRKE3cwJhUOsMuIeC7bD6gkJ1zwls6XsRetyNqPIC/hX7sjkw4jyv49tFl5CSYJtfUhJiXe6OAKydZbc7uy3inPSM3d6/2TbL7PMziGnfZvhXjQi3KpMylEvn1B6fOdhslYJWlndAXMOqzQgtj+fnbuSJb01JQvcMiEMte7rQ8gip94tS6jLgMoBu3bo16hrvry7jjhVTeK7wZ05wP5BhhRNPuA3SukFVGcy6AcoOQFIbfr/zGMoqq3n/9+3kl1aSV1RO2+R4KLFeQqY/Z3KL//fPZr/9YO8C7N3gvb8ma2dCZYln36gLzbbLKLh+FWgHfHw5DD0DWrnZOxNMEiSi4/y7l9DiOVBSwQMzVwPw/Y0TaNNKvjstlUDM1HcCXd32u1h9tdBaz9Baj9Zaj87IaJybntO75aftFZ4HeoyDC2fCUTfDiHMhub3pLy8ETD7nrm2SSLe+7KMe/I5Zy3bBOqt47bBzTCi+61OMhpNfqC3A/vrTh5K7HmbfZdpj/wZ37IL78uGwv9pjUruYH5+/fAWjL/Y835nPRQKFBD+ZMc9+g+ztI3BHaBkEQql/DlxgecGMAfK11nU7xx4EiXEm4vLT6iO5suJaynUspUMvMAd7jodo6+Uj3sq/bCl1J+Pckgr9/L/HYNMPZicqqnaiq+Hn1BZgia/1YjecybVGnA/H/bPhZhSn7ClSMFvwjXtelxU7jXnwuXOC73wmRDb+uDS+C8wH+iuldiilLlFKXaGUusIa8iWwCcgCXgauCpq0FnefOIgy4vnKcTi/nr6ExFP/XXuQD6Webrk3Hhf1Ow/FvlrjnBSTN+X8T73f2Bm8U5c/+5K3TMg+2BGdDSW9u1ksPf2Nxp0vtAhKKmxPsFW7CjhzdFdXbnKh5VKvTV1rfXY9xzVwdV1jAs0l43rSs10SReXVTBriI41svGWXdlfqDofL/PJA7Ot2/7XLzFYpOPl5z+tcu8woaq0hcxB8fCkU7LTt3jVxml3g4IKGRv+l8ecKLYKCskpXe19xBUM6+/hOCi2KJpsmYPKA9nUPcM7U186EHuOhogiePZTYc95n7e2HkfDkAQD2qXTapPfwfZ30HnD0Paa9yaqSXrzX+9iC3baXzBHX+PMxBKHRFJZ5Bsb1bCe2dKEJK/V6cSr11Z9B/k5r5qthyVskpHwPwMrW47ko9xxmFZR5pAX1SYLl31vuw/ziLGJx2usw+JSDk18Q6mF/saezQFJ8cIpbCE2L5pv4ITbRbu9cZLIUgpm5J6YBsO/YJ9hLKv+e46ebotPk4s2mnr8TXjvOtFtliOeKEHRyi8o99pPiRKkLzVmp16VUC3dDYjpHDe3HkX3asmKHn/nWE9LM9tMrTLqCRa/D7LtN37tn2uOi5I9LCD65hTWUemzzffEW/Kf5KvW6WPmxK2qzU2oi2QVl9ZxgkehWbWXnEph5Hfz6DBTlQvYK+1iHoYGTVRB8kFtYTkyUol2yWZCPi2mZf86CJy3rW3BTFqDMommyUeodUk25u+37Suo+Fzxn/6/aiZL4z8l2+7qVEC8LVkLwWbWrgHbJ8Tx2+lAGdUxx1eMVWjYtS6knZ5gAJTDuidh1E8c/8gO7DpS6hhaXV3Hyc7+wcmcN08yfa/i2A+yxshJf/DWkda19XBCCwKpd+YzsnsbE/pl8ee14YiU3ukBLUuo3mqRHxFgLqB1M5fGubZJcQ454eA7PztlASUUVz8zZwNLtB/jX12s9r+P0qvHGwVYoEgQ/cTg0+0sq6SVujEINWsbKym3bbHdEbUXhJZl0AeP6tGNCvwx+XG8y3D02ez2PzV7vOvWnDTV80mvkPNetMlDO3Oit2iEIoaCgrJJqh3YF0wmCk5YxU491y70y7GyIjndldYyOUrz5l8MY0S3N5+mlbuHYv209wKfVplDGQscAcvucYQ9MkDzVQnDJzi+jvKqaPMtHva0odaEGzVupt7PS8Ua7vZAcchrcnWPqdrrxyVVH8vRZw137Uwd3cLV/22LXCNmdX8rLVSbp7x2Vl7Azyh4nvulCMMnOL2PMQ9/T/66v+dHKnS4zdaEmzVupXzIbrvzV7+EnDjV5ZNKTYnnu3JEsvceYWpZtP+Aac+17S1mle9Kj7B026s58FXN0QEWuyZrdBfS4bRZ/e2cJWTmFVFQ5eOWnTTw/Nyuo9xUij/NeXehq32/lTu+SnuhruNBCad429cQ0V/SoP0RHKTb883i0Nu20pDhaJ8Swz3rVdbiVwbtj2gCe+2EjBWXVxpzTCHt6QVklreJiiI7yPcM//9XfAJi5fDczl3tmNI5Wir+O78Xdn63k7YXbmNg/g2uP7suIbuneLiU0cbJyijz2R3dPl9zpQi2a90y9EcRGR3kEcbRpFeeyX27aa/6okuNjuHRcLzJax5NfWgm3baXsyt/ZsKeQr1dmM+3pn9jrFsK9NruAsspqj/vkFZUz9L7ZPDhrNeVV1fyStddVZu/NX7dw5MNz+Hb1HvYWlfucjT301Vpu+N9S3l5oimbPXZfLKc/7/2YiNB3KKqtrpQG4+bj+YZJGiGSa90w9APRo24rVu/Ipq6zmmCfmAXD/9MFERSlSEmL4elU2xI7izk+W8dGSHa7zFmzK48ShnZj61DzWZhfy98l9OLRHG575fgPJCTGcMdr4s7/+yxZW7Srgt821a3v/9a1FAPz77BGM6JbOptwinv5+A3dOG0hReRWTH/+RT5fuqnVeWWU1CbGSqqA5cd17Sz3ypwOkJsX6GC20ZGSmXg/DuqaxMbfYo1xYRZUDgL6ZrdHaZMtzV+gAT3+3gV+y9rI22+Rzn7sulwte+41FW/czd10uV729xDXWm0J3p5f1it0rI5mnzxpBZkoCvTKS+f7GCR7jxvQyZfAe+2ZdIz+tEIlorc3kAZg8IJNeGcabS+qQCt4QpV4PXdKM6cNZsR1sJTuyexqA6w/OnQ05RZz7ir2w5Sw3lu5jdvXIaUPJbF27qEaX9ERSEry/UPXOSOa+k0xk7AMnD+HlC0YD8MrPm7nr0xVezxGaHo+4/UifNqoL/zz5EO46YSCZrf1IFy20OESp10NKoqcS/umWSRzW08yIO1kK//aPPRXosYPsAh73njSIKyYY98mYKMXvdx7jqpPq7hs/qGMKP906iV9vm+waDzDv5kmoOlwlLzqyJ1sePoHzx3SndYIt638XbGPL3uKGfFQhQnlh7kYAThjakWmHdGRs77ZcOr5XmKUSIhVR6vUwtIsdUPTS+aM80gqM7t7GY+zkASZJ2MT+Jl3A9cf04+IjezKgg0kt0K99a2Kio/jXaUO5+8RBfHzlEdx1wkAun9CLwZ1SiI+JplNaIvFuC7VRdXjGeOOPu4/l75P7ADBrRf31vz9ftov8ksp6xwGsyy6kstrRIHmEg+Pb1Xtc7YuO6BE+QYQmgyyU1kOntES6t01ia14Jh/bwVOKJNbwRnjl7BEu3HWBc33acc1g3V/9Iy8XwwiO6A9A5LZFLxvUE8DrjumJCb75emc09lmmlIaS3iuPGKf35YNEO1u8xSrhmoqf5G/Oocjj4bfM+/j0niymD2nNUvww+WLSd9y8fW2uRVWvN6S/OZ9HW/Vw+oRe3Hz+wwXIJDeebVdlc/p/FAKQkxNT6/gmCN0Sp+8F/LzmcX7L21rswlRwfw7i+xrTibjLp1jaJdQ9OJT7GP4+UxLhovrn+qMYLDAzpnMJnS3fx2dJdzPz7ODqlGdv8xtxizn55gcfY2av3MNuaEf6x7QBjerXh65XZTOyfSWJcNA/OWsOirfsBeOnHTUzol8HIbuniYRNknAod4IqJvesYKQg2Smtd/6ggMHr0aL1o0aKw3DuQ/LpxL9n5ZZw6sku4RfFg+74Sxj/yg0ffsYPae7zO+2JAh9aszS7kb5P6cPwhHTjhmZ8BiI+JorzKNr/cdcJAduwvJSZKUa01m/cWc+HYHkyyzFBNmefnZvFL1l7evnRM2GTocdsswGSf2PR/0+pcWxFaDkqpxVrr0T6Pi1JvvuQUlvHJkp089NXaWse+vGY8l7z5O69ffChamxn6xtwiXv15s9drPX3WcEZ0TWf26mwenLWmzvtuedjkxvlu9R6Gdkkl05+i3iGkrLKap77bwIeLtzPtkI7cP31IrTFOhfrsOSNc6SNCSUWVg353fQXAb3ccHXHPUAgf9Sl1v8wvSqmpwNNANPCK1vrhGse7AW8CadaY27TWXzZWaCEwZLZO4PIJvRnfN4Npz/zk6t/4f9OIjlLMv93OWzOwYwqlFdW0aRXHo9+sY3T3dJfJZWL/DP40rBNKKS4d34tW8TEuj5/YaMWYXm09UhRf8sbvrM8pZPu+Us4+rBsPnXpIiD5x/dz5yQpXBC7AW/O3elXqTu79bFXIlXp+aSWHPvgdAMO7polCFxpEvUpdKRUNPAccC+wAfldKfa61Xu027C7gf1rrF5RSg4AvgR5BkFdoBIM6pTCqezqLt+5n1T+O85lrJjEumqsn9eHqSX2odmhmLt9FWlIcE/p5Fv84+7BujOqezuOz1/HX8b0Y3aMNDofmqe/W88ycLL5fm+Ma+/uWfTwwczUnDevE8K5pwfyYtdBasyWvhJ7tTLDOvuIKD4XupK4IXGeKiFCyeOs+Kiwvo1NGdA75/YWmjT8z9cOALK31JgCl1HvAdMBdqWsgxWqnArVj14Ww8uJ5o8gtLKdVvH9r49FRiunDfSuUfu1b89L59htgVJTihin9Gd8vg6XbDnBozzY8MHM1i7fuJyvHmHWioxTPnj2C4w/peNCfpz7+2LbflQdnzo0T6JWRzJu/bvE6Nr+00qdSb6BHaUD4yxu2WXLqkA51jBSE2vjzF94Z2O62vwM4vMaY+4DZSqm/A62AYxAiiozW8WR4iVgNNIf2aONyvRvaJZXFlgkHoNqhufa9pXROT2RolzSv52ut+W5NDv3aJ9O9bSuvY2qyMbeIv73zB2t2F3g9/tXKbEZ1T+fp7zcAZtHx1415rlS2xVYiNTB++7mFdjK2mOgotNZoDf/8cg2TB2Qyolsat320guMGd+CEocH7gXrp/FGuGrqC4C+Bcmk8G3hDa/24Umos8B+l1BCttUekilLqMuAygG7dunm5jNCcuHJCb4rLq7jrxEFEKUVuYTmTHpvLF8t2+VTqs1fvcbnyHdG7LYd0SeXao/uSFOf5VS2tqObbNXtQGDt5QVlVrWvdMW0A/56TxaNuYfYzzh9FVJSiVbw9M3dPlHXNu3+42snxMRSVV5FfWsnFb/zOH9sO8OrPm7nnxEF8vmwXny/bxdXvwGsXjWbyADuKOFBMGRT4awrNH3+U+k6gq9t+F6vPnUuAqQBa6/lKqQSgHZDjPkhrPQOYAcb7pZEyC02EzJQEHjltmGs/OT6G0d3TefmnzbSKj+G6Y/qhteajJTv5Y9t+ThrWiTs/sVMu/Loxj1835pGaGMtVE02UrNaa2z9ewXu/b/e418y/j2Plznwe/WYd3dsmccqIzpw/tgd9M1vzzJwN/LHtAABTrIpWQzqn0j4lnj0F5Tw2ex1ZOUXMu3mSxzWvP7YfD8xczexVe1zng12gwskjX68LmFLXWhMdpbj8qF7iwig0Cn/SBPwO9FVK9VRKxQFnAZ/XGLMNOBpAKTUQSAByAymo0Dxw5gD/aMkOKqsdnDVjATd9sIy3F27jrBkL2FtkL0we0tmkaHh89nqy88sA+HJFtkuhj7Ry55wwtCNDOqdy1mHdWHz3sXx81ZGcP7YHAJMGZPLJVUdywiEdeerM4a5rx0ZH8eJ5owCTQXPH/lJ63WEcttqnxPPPU4a4rn/LR8sB6JNpF6SIiVI8MH0wCbFRrlz5Wmsqqx1UHUQqheKKaqodmtRESasrNI56Z+pa6yql1N+AbzDuiq9prVcppe4HFmmtPwduBF5WSl2PWTS9SIfLAV6IaA7v1ZZbpvbnka/X0fdO44fdOj6GL68dz12fruTH9bncdcJA/nJkT6KiFL9k7eXcVxYy5qHvXddITYzll9smkxwfw479JbSOr18BPnfuyFp9HVO9Fx/57oYJtE6IdRUtcfLp1Ufy/Zo9XPveUh49fSinjOhCXnEFT323gU//2Mk7v21zpVF+8sxhtEuOZ3zfDG+38El+aaXrMwpCY/DLpm75nH9Zo+8et/Zq4MjAiiY0V4bVsKf/ftcxJMRG8+ZfDqO0opqE2CiX6WFU93Q6pCSQXVDmGv/0WcNJtrx4uqQn0Vh8LRw7s10mx5tSg9VWGcPk+BimD+/s4RXUv71J1nbd+0s9rnH9+8vMZ7vzGNZlF9IhNcFjpu+LP7aZhWVR6kJjkdwvQsgZ2S2dvpnJdGuTxONnDPNwJ6yZJC0hNppvbziKmKgoEuOiKSirJCUhMArP3V//3MO78fbCbbz5l8M8xozqns5vm/cx96aJXq/Rz8rACSZtwjsLt7HJLeXxOS8vYINVW3TzQ9MAOy+Qw6F5+7dtHNW3ncvT5z/ztwKmOIsgNAZJEyCEBa11RCwE3vf5KorKq3jw5CH8uD6X4wZ7+oXnl1by4/pc/jTMe1RptUNzzXt/cPah3RjXt50raOvWj5ZTVundtv7bnUezYY9nEZWnzxrOP2etIaewnCsm9Oa24wcE7kMKzQrJ/SIIYeLzZbv4asVu4mOiPGrJHt6zDev3FLLfRx77hXccLf7pgk8CkvtFEISG86dhnVwz/E5piazeXcDcdbkstBZTnTPyKU/+yPo9RVx+VC9unNKfuBipXSM0HlHqghACbplqzCn/nLWal38ymTCdhVReu+hQFm7ax6kjO0eESUpo2ohSF4QQctGRPXFouHFKP1eUbJf0JLqMarwXjyC4I0pdEEJI57RE7j6x4WUKBcFfxHgnCILQjBClLgiC0IwQpS4IgtCMEKUuCILQjBClLgiC0IwQpS4IgtCMEKUuCILQjBClLgiC0IwIW0IvpVQusLWRp7cD9gZQnGDT1OSFpiezyBtcRN7g0hB5u2utfVZfCZtSPxiUUovqylIWaTQ1eaHpySzyBheRN7gEUl4xvwiCIDQjRKkLgiA0I5qqUp8RbgEaSFOTF5qezCJvcBF5g0vA5G2SNnVBEATBO011pi4IgiB4QZS6IAhCM6LJKXWl1FSl1DqlVJZS6rZwywOglOqqlPpBKbVaKbVKKXWt1d9GKfWtUmqDtU23+pVS6hnrMyxXSo0Mk9zRSqk/lFIzrf2eSqmFllzvK6XirP54az/LOt4jDLKmKaU+VEqtVUqtUUqNjeTnq5S63vourFRKvauUSoik56uUek0plaOUWunW1+DnqZS60Bq/QSl1YYjlfdT6PixXSn2ilEpzO3a7Je86pdRxbv0h0R/e5HU7dqNSSiul2ln7gX2+Wusm8w+IBjYCvYA4YBkwKALk6giMtNqtgfXAIOAR4Dar/zbgX1Z7GvAVoIAxwMIwyX0D8A4w09r/H3CW1X4RuNJqXwW8aLXPAt4Pg6xvApda7TggLVKfL9AZ2Awkuj3XiyLp+QJHASOBlW59DXqeQBtgk7VNt9rpIZR3ChBjtf/lJu8gSzfEAz0tnREdSv3hTV6rvyvwDSbwsl0wnm9I/zAD8KDGAt+47d8O3B5uubzI+RlwLLAO6Gj1dQTWWe2XgLPdxrvGhVDGLsD3wGRgpvWF2uv2R+J61taXcKzVjrHGqRDKmmopSVWjPyKfL0apb7f+GGOs53tcpD1foEcNJdmg5wmcDbzk1u8xLtjy1jh2CvC21fbQC87nG2r94U1e4ENgGLAFW6kH9Pk2NfOL84/FyQ6rL2KwXp1HAAuB9lrr3dahbKC91Y6Ez/EUcAvgsPbbAge01lVeZHLJax3Pt8aHip5ALvC6ZS56RSnVigh9vlrrncBjwDZgN+Z5LSZyn6+Thj7PSPgeO/kLZrYLESqvUmo6sFNrvazGoYDK29SUekSjlEoGPgKu01oXuB/T5qc2IvxHlVInAjla68XhlsVPYjCvsi9orUcAxRjzgIsIe77pwHTMj1EnoBUwNaxCNZBIep71oZS6E6gC3g63LL5QSiUBdwD3BPteTU2p78TYpJx0sfrCjlIqFqPQ39Zaf2x171FKdbSOdwRyrP5wf44jgT8ppbYA72FMME8DaUqpGC8yueS1jqcCeSGUdwewQ2u90Nr/EKPkI/X5HgNs1lrnaq0rgY8xzzxSn6+Thj7PcD9nlFIXAScC51o/RNQhVzjl7Y35kV9m/d11AZYopTrUIVej5G1qSv13oK/lRRCHWVT6PMwyoZRSwKvAGq31E26HPgecK9YXYmztzv4LrFXvMUC+22tv0NFa36617qK17oF5hnO01ucCPwCn+ZDX+TlOs8aHbBantc4Gtiul+ltdRwOridDnizG7jFFKJVnfDae8Efl83Wjo8/wGmKKUSrfeTqZYfSFBKTUVY0L8k9a6xO3Q58BZlldRT6Av8Bth1B9a6xVa60ytdQ/r724Hxrkim0A/32AtEgRx8WEaxrtkI3BnuOWxZBqHeVVdDiy1/k3D2EW/BzYA3wFtrPEKeM76DCuA0WGUfSK290svzJc/C/gAiLf6E6z9LOt4rzDIORxYZD3jTzHeABH7fIF/AGuBlcB/MJ4YEfN8gXcx9v5KS8Fc0pjnibFlZ1n/Lg6xvFkYm7Pzb+5Ft/F3WvKuA4536w+J/vAmb43jW7AXSgP6fCVNgCAIQjOiqZlfBEEQhDoQpS4IgtCMEKUuCILQjBClLgiC0IwQpS4IgtCMEKUuCILQjBClLgiC0Iz4f2gDGN8mFyelAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_return(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExxonMobil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which stock to predict\n",
    "ticker = \"XOM\"\n",
    "\n",
    "# use numbers of previous days to predict\n",
    "N_STEPS = 100\n",
    "\n",
    "# which day to predict, 1 is for next day, 10 is for 10 days after today\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.5\n",
    "\n",
    "### training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 100\n",
    "\n",
    "### other parameters\n",
    "TEST_SIZE = 0.2\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save data to csv locally\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/102 [..............................] - ETA: 13s - loss: 0.0520 - mean_absolute_error: 0.1979WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0259s vs `on_train_batch_end` time: 0.2393s). Check your callbacks.\n",
      "100/102 [============================>.] - ETA: 0s - loss: 0.0056 - mean_absolute_error: 0.0579\n",
      "Epoch 00001: val_loss improved from inf to 0.00069, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 41ms/step - loss: 0.0056 - mean_absolute_error: 0.0575 - val_loss: 6.9226e-04 - val_mean_absolute_error: 0.0211\n",
      "Epoch 2/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0324\n",
      "Epoch 00002: val_loss improved from 0.00069 to 0.00049, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 0.0016 - mean_absolute_error: 0.0324 - val_loss: 4.8685e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 3/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0303\n",
      "Epoch 00003: val_loss did not improve from 0.00049\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 0.0014 - mean_absolute_error: 0.0303 - val_loss: 0.0012 - val_mean_absolute_error: 0.0295\n",
      "Epoch 4/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0293\n",
      "Epoch 00004: val_loss did not improve from 0.00049\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 0.0013 - mean_absolute_error: 0.0293 - val_loss: 0.0013 - val_mean_absolute_error: 0.0324\n",
      "Epoch 5/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0288\n",
      "Epoch 00005: val_loss did not improve from 0.00049\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 0.0012 - mean_absolute_error: 0.0287 - val_loss: 5.6430e-04 - val_mean_absolute_error: 0.0205\n",
      "Epoch 6/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0281\n",
      "Epoch 00006: val_loss did not improve from 0.00049\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 0.0011 - mean_absolute_error: 0.0280 - val_loss: 5.8667e-04 - val_mean_absolute_error: 0.0193\n",
      "Epoch 7/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 9.7995e-04 - mean_absolute_error: 0.0262\n",
      "Epoch 00007: val_loss improved from 0.00049 to 0.00048, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 9.8668e-04 - mean_absolute_error: 0.0263 - val_loss: 4.8324e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 8/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0269 \n",
      "Epoch 00008: val_loss improved from 0.00048 to 0.00042, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 9.9969e-04 - mean_absolute_error: 0.0269 - val_loss: 4.1675e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 9/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 8.7334e-04 - mean_absolute_error: 0.0252\n",
      "Epoch 00009: val_loss improved from 0.00042 to 0.00027, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 8.7057e-04 - mean_absolute_error: 0.0252 - val_loss: 2.7385e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 10/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 8.6397e-04 - mean_absolute_error: 0.0251\n",
      "Epoch 00010: val_loss did not improve from 0.00027\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 8.6157e-04 - mean_absolute_error: 0.0251 - val_loss: 3.6634e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 11/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 8.2722e-04 - mean_absolute_error: 0.0248\n",
      "Epoch 00011: val_loss improved from 0.00027 to 0.00020, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 8.2699e-04 - mean_absolute_error: 0.0248 - val_loss: 2.0126e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 12/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 7.5370e-04 - mean_absolute_error: 0.0240\n",
      "Epoch 00012: val_loss did not improve from 0.00020\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 7.5145e-04 - mean_absolute_error: 0.0240 - val_loss: 2.4706e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 13/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 7.5516e-04 - mean_absolute_error: 0.0240\n",
      "Epoch 00013: val_loss did not improve from 0.00020\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 7.5458e-04 - mean_absolute_error: 0.0240 - val_loss: 3.3589e-04 - val_mean_absolute_error: 0.0192\n",
      "Epoch 14/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.9114e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00014: val_loss improved from 0.00020 to 0.00016, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 6.9181e-04 - mean_absolute_error: 0.0233 - val_loss: 1.6432e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 15/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.4400e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00015: val_loss improved from 0.00016 to 0.00014, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 6.4262e-04 - mean_absolute_error: 0.0224 - val_loss: 1.3958e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 16/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.7534e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 00016: val_loss did not improve from 0.00014\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 6.7408e-04 - mean_absolute_error: 0.0227 - val_loss: 1.7083e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 17/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.8445e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00017: val_loss did not improve from 0.00014\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 6.8599e-04 - mean_absolute_error: 0.0232 - val_loss: 2.1538e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 18/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.9091e-04 - mean_absolute_error: 0.0236\n",
      "Epoch 00018: val_loss did not improve from 0.00014\n",
      "102/102 [==============================] - 3s 31ms/step - loss: 6.9266e-04 - mean_absolute_error: 0.0237 - val_loss: 1.5253e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 19/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.4754e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00019: val_loss did not improve from 0.00014\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 6.5058e-04 - mean_absolute_error: 0.0231 - val_loss: 1.4489e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 20/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.2036e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00020: val_loss improved from 0.00014 to 0.00011, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 6.2000e-04 - mean_absolute_error: 0.0224 - val_loss: 1.1129e-04 - val_mean_absolute_error: 0.0091\n",
      "Epoch 21/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.8191e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00021: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.8109e-04 - mean_absolute_error: 0.0215 - val_loss: 1.5571e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 22/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 6.1615e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 00022: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 32ms/step - loss: 6.1676e-04 - mean_absolute_error: 0.0227 - val_loss: 1.8945e-04 - val_mean_absolute_error: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.9066e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00023: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 5.8931e-04 - mean_absolute_error: 0.0221 - val_loss: 1.3210e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 24/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.4049e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00024: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.4272e-04 - mean_absolute_error: 0.0214 - val_loss: 1.1743e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 25/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.3646e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00025: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.3513e-04 - mean_absolute_error: 0.0212 - val_loss: 1.1276e-04 - val_mean_absolute_error: 0.0086\n",
      "Epoch 26/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.1039e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00026: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.1029e-04 - mean_absolute_error: 0.0207 - val_loss: 1.1663e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 27/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.0502e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00027: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.0378e-04 - mean_absolute_error: 0.0209 - val_loss: 1.7514e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 28/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.8919e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00028: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.8992e-04 - mean_absolute_error: 0.0205 - val_loss: 1.4276e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 29/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.1563e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00029: val_loss did not improve from 0.00011\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.1476e-04 - mean_absolute_error: 0.0210 - val_loss: 2.0281e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 30/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.0260e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00030: val_loss improved from 0.00011 to 0.00010, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.0385e-04 - mean_absolute_error: 0.0213 - val_loss: 9.9048e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 31/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.0439e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00031: val_loss did not improve from 0.00010\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 5.0604e-04 - mean_absolute_error: 0.0213 - val_loss: 1.7597e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 32/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.8513e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00032: val_loss improved from 0.00010 to 0.00009, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.8426e-04 - mean_absolute_error: 0.0206 - val_loss: 8.6236e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 33/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.8538e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00033: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.8450e-04 - mean_absolute_error: 0.0208 - val_loss: 9.6143e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 34/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.5805e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00034: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.5685e-04 - mean_absolute_error: 0.0202 - val_loss: 1.1067e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 35/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.8167e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00035: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.8072e-04 - mean_absolute_error: 0.0208 - val_loss: 2.4042e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 36/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.6267e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00036: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.6391e-04 - mean_absolute_error: 0.0205 - val_loss: 1.9481e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 37/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.9124e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00037: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 4.9008e-04 - mean_absolute_error: 0.0212 - val_loss: 1.5829e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 38/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 5.0644e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00038: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 5.0605e-04 - mean_absolute_error: 0.0215 - val_loss: 1.7627e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 39/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.8813e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 00039: val_loss did not improve from 0.00009\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.8694e-04 - mean_absolute_error: 0.0214 - val_loss: 1.2654e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 40/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.5509e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00040: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.5655e-04 - mean_absolute_error: 0.0209 - val_loss: 8.2831e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 41/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.3403e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00041: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.3514e-04 - mean_absolute_error: 0.0204 - val_loss: 9.4702e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 42/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.6054e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00042: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.6217e-04 - mean_absolute_error: 0.0210 - val_loss: 8.9224e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 43/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.5261e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00043: val_loss improved from 0.00008 to 0.00008, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.5271e-04 - mean_absolute_error: 0.0211 - val_loss: 7.9923e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 44/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.3869e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00044: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.3804e-04 - mean_absolute_error: 0.0207 - val_loss: 2.6559e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 45/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.2328e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00045: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.2246e-04 - mean_absolute_error: 0.0202 - val_loss: 1.2272e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 46/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.3144e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00046: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.3098e-04 - mean_absolute_error: 0.0205 - val_loss: 1.8470e-04 - val_mean_absolute_error: 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.7565e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00047: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.7414e-04 - mean_absolute_error: 0.0217 - val_loss: 1.7865e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 48/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.4834e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00048: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.4851e-04 - mean_absolute_error: 0.0209 - val_loss: 1.2913e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 49/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.2996e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00049: val_loss did not improve from 0.00008\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.3023e-04 - mean_absolute_error: 0.0206 - val_loss: 1.0131e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 50/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.5288e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00050: val_loss improved from 0.00008 to 0.00007, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.5304e-04 - mean_absolute_error: 0.0214 - val_loss: 6.5806e-05 - val_mean_absolute_error: 0.0065\n",
      "Epoch 51/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.5094e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00051: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.4999e-04 - mean_absolute_error: 0.0211 - val_loss: 3.4603e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 52/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.3817e-04 - mean_absolute_error: 0.0208\n",
      "Epoch 00052: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.3760e-04 - mean_absolute_error: 0.0208 - val_loss: 8.6237e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 53/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.0687e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00053: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 3s 33ms/step - loss: 4.0719e-04 - mean_absolute_error: 0.0201 - val_loss: 1.4503e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 54/100\n",
      "102/102 [==============================] - ETA: 0s - loss: 4.1577e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00054: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.1577e-04 - mean_absolute_error: 0.0205 - val_loss: 8.6687e-05 - val_mean_absolute_error: 0.0109\n",
      "Epoch 55/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9301e-04 - mean_absolute_error: 0.0201- ETA: 2s - loss: 4.1309e-04\n",
      "Epoch 00055: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 3.9236e-04 - mean_absolute_error: 0.0201 - val_loss: 7.2259e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 56/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.0585e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00056: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 4s 34ms/step - loss: 4.0571e-04 - mean_absolute_error: 0.0203 - val_loss: 1.1125e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 57/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.1931e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00057: val_loss did not improve from 0.00007\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.1849e-04 - mean_absolute_error: 0.0207 - val_loss: 9.1195e-05 - val_mean_absolute_error: 0.0104\n",
      "Epoch 58/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.0535e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00058: val_loss improved from 0.00007 to 0.00006, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.0546e-04 - mean_absolute_error: 0.0204 - val_loss: 6.3611e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 59/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.4980e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 00059: val_loss did not improve from 0.00006\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.5016e-04 - mean_absolute_error: 0.0218 - val_loss: 1.4418e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 60/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.3647e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00060: val_loss did not improve from 0.00006\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.3626e-04 - mean_absolute_error: 0.0211 - val_loss: 1.6901e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 61/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.1011e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00061: val_loss did not improve from 0.00006\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.0863e-04 - mean_absolute_error: 0.0204 - val_loss: 1.8996e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 62/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.2871e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00062: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.2827e-04 - mean_absolute_error: 0.0210 - val_loss: 5.3190e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 63/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9909e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00063: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.0151e-04 - mean_absolute_error: 0.0205 - val_loss: 1.0154e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 64/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9147e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00064: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.9141e-04 - mean_absolute_error: 0.0202 - val_loss: 6.2538e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 65/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8581e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00065: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.8594e-04 - mean_absolute_error: 0.0199 - val_loss: 8.5954e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 66/100\n",
      "102/102 [==============================] - ETA: 0s - loss: 4.1033e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00066: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.1033e-04 - mean_absolute_error: 0.0205 - val_loss: 1.7085e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 67/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9584e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00067: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 3.9628e-04 - mean_absolute_error: 0.0203 - val_loss: 8.6033e-05 - val_mean_absolute_error: 0.0107\n",
      "Epoch 68/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.0362e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00068: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 4.0273e-04 - mean_absolute_error: 0.0205 - val_loss: 1.3047e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 69/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9091e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00069: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 3.9130e-04 - mean_absolute_error: 0.0202 - val_loss: 7.9152e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 70/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.1895e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00070: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 38ms/step - loss: 4.1896e-04 - mean_absolute_error: 0.0208 - val_loss: 1.1434e-04 - val_mean_absolute_error: 0.0085\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/102 [============================>.] - ETA: 0s - loss: 4.2496e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00071: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 4.2478e-04 - mean_absolute_error: 0.0211 - val_loss: 1.8169e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 72/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8744e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00072: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 3.8770e-04 - mean_absolute_error: 0.0201 - val_loss: 1.4808e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 73/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.0917e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00073: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 38ms/step - loss: 4.1055e-04 - mean_absolute_error: 0.0205 - val_loss: 5.7835e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 74/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7377e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00074: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.7454e-04 - mean_absolute_error: 0.0200 - val_loss: 9.7078e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 75/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8980e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00075: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.8944e-04 - mean_absolute_error: 0.0201 - val_loss: 8.5349e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 76/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8988e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00076: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.9039e-04 - mean_absolute_error: 0.0204 - val_loss: 6.8316e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 77/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7244e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00077: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.7341e-04 - mean_absolute_error: 0.0198 - val_loss: 7.4588e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 78/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9703e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00078: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.9651e-04 - mean_absolute_error: 0.0204 - val_loss: 7.0395e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 79/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9582e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00079: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.9623e-04 - mean_absolute_error: 0.0206 - val_loss: 8.7701e-05 - val_mean_absolute_error: 0.0090\n",
      "Epoch 80/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8228e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00080: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.8180e-04 - mean_absolute_error: 0.0199 - val_loss: 6.7087e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 81/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7849e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00081: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 3.7969e-04 - mean_absolute_error: 0.0200 - val_loss: 1.0448e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 82/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9110e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00082: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 3.9151e-04 - mean_absolute_error: 0.0202 - val_loss: 6.8841e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 83/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.6993e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00083: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.7075e-04 - mean_absolute_error: 0.0198 - val_loss: 7.6915e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 84/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7050e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00084: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 3.7079e-04 - mean_absolute_error: 0.0198 - val_loss: 7.6135e-05 - val_mean_absolute_error: 0.0106\n",
      "Epoch 85/100\n",
      "102/102 [==============================] - ETA: 0s - loss: 3.8216e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00085: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.8216e-04 - mean_absolute_error: 0.0200 - val_loss: 1.1765e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 86/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8013e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00086: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 3.7999e-04 - mean_absolute_error: 0.0199 - val_loss: 5.0450e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 87/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.8446e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00087: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 3.8456e-04 - mean_absolute_error: 0.0202 - val_loss: 8.6948e-05 - val_mean_absolute_error: 0.0112\n",
      "Epoch 88/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9233e-04 - mean_absolute_error: 0.0203\n",
      "Epoch 00088: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 3.9350e-04 - mean_absolute_error: 0.0203 - val_loss: 8.0546e-05 - val_mean_absolute_error: 0.0108\n",
      "Epoch 89/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.0941e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00089: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.0850e-04 - mean_absolute_error: 0.0207 - val_loss: 8.5298e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 90/100\n",
      "102/102 [==============================] - ETA: 0s - loss: 3.6844e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00090: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.6844e-04 - mean_absolute_error: 0.0196 - val_loss: 1.2610e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 91/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7056e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00091: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 38ms/step - loss: 3.7095e-04 - mean_absolute_error: 0.0201 - val_loss: 4.8922e-05 - val_mean_absolute_error: 0.0060\n",
      "Epoch 92/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9461e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00092: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.9523e-04 - mean_absolute_error: 0.0204 - val_loss: 1.5835e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 93/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9705e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00093: val_loss did not improve from 0.00005\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.9694e-04 - mean_absolute_error: 0.0207 - val_loss: 8.2072e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 94/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.9151e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00094: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-12-10_XOM-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 3.9071e-04 - mean_absolute_error: 0.0204 - val_loss: 4.4856e-05 - val_mean_absolute_error: 0.0063\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - ETA: 0s - loss: 3.8762e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00095: val_loss did not improve from 0.00004\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.8762e-04 - mean_absolute_error: 0.0202 - val_loss: 1.3863e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 96/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7106e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00096: val_loss did not improve from 0.00004\n",
      "102/102 [==============================] - 3s 34ms/step - loss: 3.7082e-04 - mean_absolute_error: 0.0200 - val_loss: 2.1008e-04 - val_mean_absolute_error: 0.0170\n",
      "Epoch 97/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 4.2507e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 00097: val_loss did not improve from 0.00004\n",
      "102/102 [==============================] - 4s 35ms/step - loss: 4.2444e-04 - mean_absolute_error: 0.0210 - val_loss: 5.0921e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 98/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7829e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00098: val_loss did not improve from 0.00004\n",
      "102/102 [==============================] - 4s 36ms/step - loss: 3.7796e-04 - mean_absolute_error: 0.0199 - val_loss: 1.2826e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 99/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7393e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00099: val_loss did not improve from 0.00004\n",
      "102/102 [==============================] - 4s 37ms/step - loss: 3.7328e-04 - mean_absolute_error: 0.0201 - val_loss: 5.6451e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 100/100\n",
      "101/102 [============================>.] - ETA: 0s - loss: 3.7865e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00100: val_loss did not improve from 0.00004\n",
      "102/102 [==============================] - 4s 38ms/step - loss: 3.7870e-04 - mean_absolute_error: 0.0202 - val_loss: 8.0652e-05 - val_mean_absolute_error: 0.0103\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 1 days is 40.04$\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Accuracy Score: 0.5200945626477541\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.7783100075479825, 0.8987959190446589)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_return(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data, 2600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQQ 300 - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which stock to predict\n",
    "ticker = \"QQQ\"\n",
    "\n",
    "# use numbers of previous days to predict\n",
    "N_STEPS = 300\n",
    "\n",
    "# which day to predict, 1 is for next day, 10 is for 10 days after today\n",
    "LOOKUP_STEP = 30\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.5\n",
    "\n",
    "### training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 200\n",
    "\n",
    "### other parameters\n",
    "TEST_SIZE = 0.2\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save data to csv locally\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 2/83 [..............................] - ETA: 9s - loss: 0.0161 - mean_absolute_error: 0.1138WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0213s vs `on_train_batch_end` time: 0.2082s). Check your callbacks.\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0555\n",
      "Epoch 00001: val_loss improved from inf to 0.00155, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 0.0036 - mean_absolute_error: 0.0555 - val_loss: 0.0016 - val_mean_absolute_error: 0.0417\n",
      "Epoch 2/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0426\n",
      "Epoch 00002: val_loss improved from 0.00155 to 0.00084, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 0.0020 - mean_absolute_error: 0.0425 - val_loss: 8.4342e-04 - val_mean_absolute_error: 0.0283\n",
      "Epoch 3/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.0384\n",
      "Epoch 00003: val_loss did not improve from 0.00084\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 0.0017 - mean_absolute_error: 0.0382 - val_loss: 9.5312e-04 - val_mean_absolute_error: 0.0344\n",
      "Epoch 4/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0358\n",
      "Epoch 00004: val_loss did not improve from 0.00084\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 0.0014 - mean_absolute_error: 0.0359 - val_loss: 9.1811e-04 - val_mean_absolute_error: 0.0340\n",
      "Epoch 5/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0334\n",
      "Epoch 00005: val_loss improved from 0.00084 to 0.00061, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 0.0013 - mean_absolute_error: 0.0333 - val_loss: 6.1249e-04 - val_mean_absolute_error: 0.0245\n",
      "Epoch 6/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.0014 - mean_absolute_error: 0.0342\n",
      "Epoch 00006: val_loss did not improve from 0.00061\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 0.0014 - mean_absolute_error: 0.0342 - val_loss: 8.5606e-04 - val_mean_absolute_error: 0.0256\n",
      "Epoch 7/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0325\n",
      "Epoch 00007: val_loss improved from 0.00061 to 0.00050, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 0.0013 - mean_absolute_error: 0.0326 - val_loss: 4.9683e-04 - val_mean_absolute_error: 0.0209\n",
      "Epoch 8/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0299\n",
      "Epoch 00008: val_loss did not improve from 0.00050\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 0.0011 - mean_absolute_error: 0.0299 - val_loss: 5.9648e-04 - val_mean_absolute_error: 0.0224\n",
      "Epoch 9/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 9.8401e-04 - mean_absolute_error: 0.0289\n",
      "Epoch 00009: val_loss improved from 0.00050 to 0.00038, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 9.8037e-04 - mean_absolute_error: 0.0289 - val_loss: 3.8388e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 10/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0304\n",
      "Epoch 00010: val_loss did not improve from 0.00038\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 0.0011 - mean_absolute_error: 0.0305 - val_loss: 4.5678e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 11/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 9.4507e-04 - mean_absolute_error: 0.0270\n",
      "Epoch 00011: val_loss did not improve from 0.00038\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 9.4686e-04 - mean_absolute_error: 0.0271 - val_loss: 5.9963e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 12/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 9.1763e-04 - mean_absolute_error: 0.0278\n",
      "Epoch 00012: val_loss did not improve from 0.00038\n",
      "83/83 [==============================] - 2s 22ms/step - loss: 9.1544e-04 - mean_absolute_error: 0.0278 - val_loss: 4.0039e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 13/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 8.8262e-04 - mean_absolute_error: 0.0267\n",
      "Epoch 00013: val_loss did not improve from 0.00038\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 8.8832e-04 - mean_absolute_error: 0.0268 - val_loss: 5.9879e-04 - val_mean_absolute_error: 0.0225\n",
      "Epoch 14/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 9.9371e-04 - mean_absolute_error: 0.0285\n",
      "Epoch 00014: val_loss did not improve from 0.00038\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 9.9228e-04 - mean_absolute_error: 0.0285 - val_loss: 8.1552e-04 - val_mean_absolute_error: 0.0277\n",
      "Epoch 15/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 9.0474e-04 - mean_absolute_error: 0.0273\n",
      "Epoch 00015: val_loss improved from 0.00038 to 0.00031, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 9.0295e-04 - mean_absolute_error: 0.0272 - val_loss: 3.1263e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 16/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 7.8024e-04 - mean_absolute_error: 0.0253\n",
      "Epoch 00016: val_loss improved from 0.00031 to 0.00031, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 25ms/step - loss: 7.8024e-04 - mean_absolute_error: 0.0253 - val_loss: 3.0619e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 17/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 7.0218e-04 - mean_absolute_error: 0.0238\n",
      "Epoch 00017: val_loss improved from 0.00031 to 0.00029, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 7.0218e-04 - mean_absolute_error: 0.0238 - val_loss: 2.8588e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 18/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 7.1017e-04 - mean_absolute_error: 0.0244\n",
      "Epoch 00018: val_loss did not improve from 0.00029\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 7.0620e-04 - mean_absolute_error: 0.0244 - val_loss: 3.1432e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 19/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 6.6323e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00019: val_loss did not improve from 0.00029\n",
      "83/83 [==============================] - 2s 24ms/step - loss: 6.6817e-04 - mean_absolute_error: 0.0234 - val_loss: 3.2389e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 20/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.4395e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00020: val_loss improved from 0.00029 to 0.00027, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 6.4395e-04 - mean_absolute_error: 0.0233 - val_loss: 2.7332e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 21/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.0157e-04 - mean_absolute_error: 0.0231\n",
      "Epoch 00021: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 23ms/step - loss: 6.0157e-04 - mean_absolute_error: 0.0231 - val_loss: 3.5971e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 22/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.8921e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00022: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 6.8921e-04 - mean_absolute_error: 0.0239 - val_loss: 3.3735e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/83 [============================>.] - ETA: 0s - loss: 5.6660e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00023: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 5.6621e-04 - mean_absolute_error: 0.0217 - val_loss: 2.8715e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 24/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 5.7607e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00024: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 5.7645e-04 - mean_absolute_error: 0.0225 - val_loss: 5.2346e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 25/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 6.5462e-04 - mean_absolute_error: 0.0234\n",
      "Epoch 00025: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 6.5504e-04 - mean_absolute_error: 0.0234 - val_loss: 4.5890e-04 - val_mean_absolute_error: 0.0196\n",
      "Epoch 26/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 6.4501e-04 - mean_absolute_error: 0.0238\n",
      "Epoch 00026: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 6.4507e-04 - mean_absolute_error: 0.0238 - val_loss: 4.0967e-04 - val_mean_absolute_error: 0.0184\n",
      "Epoch 27/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 7.2019e-04 - mean_absolute_error: 0.0251\n",
      "Epoch 00027: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 7.1828e-04 - mean_absolute_error: 0.0251 - val_loss: 7.3797e-04 - val_mean_absolute_error: 0.0254\n",
      "Epoch 28/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 6.2953e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00028: val_loss did not improve from 0.00027\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 6.2953e-04 - mean_absolute_error: 0.0233 - val_loss: 3.2678e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 29/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 5.5355e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00029: val_loss improved from 0.00027 to 0.00023, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 5.5086e-04 - mean_absolute_error: 0.0216 - val_loss: 2.3150e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 30/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 5.5211e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00030: val_loss improved from 0.00023 to 0.00023, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 5.5937e-04 - mean_absolute_error: 0.0218 - val_loss: 2.2698e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 31/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 6.3570e-04 - mean_absolute_error: 0.0243\n",
      "Epoch 00031: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 6.3374e-04 - mean_absolute_error: 0.0242 - val_loss: 2.8743e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 32/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 5.1490e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00032: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 5.1309e-04 - mean_absolute_error: 0.0211 - val_loss: 3.2921e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 33/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 5.9569e-04 - mean_absolute_error: 0.0221\n",
      "Epoch 00033: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 5.9569e-04 - mean_absolute_error: 0.0221 - val_loss: 2.3554e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 34/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 5.6309e-04 - mean_absolute_error: 0.0224\n",
      "Epoch 00034: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 5.6309e-04 - mean_absolute_error: 0.0224 - val_loss: 2.8801e-04 - val_mean_absolute_error: 0.0169\n",
      "Epoch 35/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 5.6211e-04 - mean_absolute_error: 0.0222\n",
      "Epoch 00035: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 5.6211e-04 - mean_absolute_error: 0.0222 - val_loss: 4.6980e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 36/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.6809e-04 - mean_absolute_error: 0.0205\n",
      "Epoch 00036: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 4.6815e-04 - mean_absolute_error: 0.0204 - val_loss: 2.5065e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 37/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 4.9609e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00037: val_loss did not improve from 0.00023\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 4.9667e-04 - mean_absolute_error: 0.0213 - val_loss: 4.7822e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 38/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 5.0894e-04 - mean_absolute_error: 0.0216\n",
      "Epoch 00038: val_loss improved from 0.00023 to 0.00020, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 5.0894e-04 - mean_absolute_error: 0.0216 - val_loss: 2.0426e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 39/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 5.5532e-04 - mean_absolute_error: 0.0225\n",
      "Epoch 00039: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 5.5455e-04 - mean_absolute_error: 0.0225 - val_loss: 3.2601e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 40/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 5.4259e-04 - mean_absolute_error: 0.0220\n",
      "Epoch 00040: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 5.3869e-04 - mean_absolute_error: 0.0219 - val_loss: 2.9878e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 41/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.9593e-04 - mean_absolute_error: 0.0209\n",
      "Epoch 00041: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.9319e-04 - mean_absolute_error: 0.0209 - val_loss: 2.1682e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 42/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 5.4528e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00042: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 5.4908e-04 - mean_absolute_error: 0.0212 - val_loss: 6.5630e-04 - val_mean_absolute_error: 0.0275\n",
      "Epoch 43/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 4.9547e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00043: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.9659e-04 - mean_absolute_error: 0.0212 - val_loss: 3.3301e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 44/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 5.0022e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00044: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.9914e-04 - mean_absolute_error: 0.0202 - val_loss: 3.6276e-04 - val_mean_absolute_error: 0.0206\n",
      "Epoch 45/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.9062e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00045: val_loss did not improve from 0.00020\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 4.8793e-04 - mean_absolute_error: 0.0212 - val_loss: 2.4794e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 46/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.4721e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00046: val_loss improved from 0.00020 to 0.00019, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 4.4721e-04 - mean_absolute_error: 0.0204 - val_loss: 1.9499e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/83 [============================>.] - ETA: 0s - loss: 4.5841e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00047: val_loss did not improve from 0.00019\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 4.5811e-04 - mean_absolute_error: 0.0202 - val_loss: 2.0664e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 48/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.3007e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00048: val_loss did not improve from 0.00019\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.3007e-04 - mean_absolute_error: 0.0193 - val_loss: 2.1347e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 49/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 4.4422e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00049: val_loss did not improve from 0.00019\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 4.4421e-04 - mean_absolute_error: 0.0200 - val_loss: 1.9598e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 50/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 4.2381e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00050: val_loss did not improve from 0.00019\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 4.2353e-04 - mean_absolute_error: 0.0194 - val_loss: 2.1808e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 51/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 5.1367e-04 - mean_absolute_error: 0.0217\n",
      "Epoch 00051: val_loss did not improve from 0.00019\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 5.1604e-04 - mean_absolute_error: 0.0217 - val_loss: 2.1963e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 52/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 4.4254e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00052: val_loss improved from 0.00019 to 0.00019, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.4111e-04 - mean_absolute_error: 0.0194 - val_loss: 1.8645e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 53/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.7013e-04 - mean_absolute_error: 0.0206\n",
      "Epoch 00053: val_loss did not improve from 0.00019\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.7022e-04 - mean_absolute_error: 0.0206 - val_loss: 2.3915e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 54/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.2148e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 00054: val_loss improved from 0.00019 to 0.00018, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.2148e-04 - mean_absolute_error: 0.0196 - val_loss: 1.7585e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 55/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.0705e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00055: val_loss improved from 0.00018 to 0.00017, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 4.0512e-04 - mean_absolute_error: 0.0189 - val_loss: 1.6966e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 56/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.0208e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00056: val_loss did not improve from 0.00017\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 4.0208e-04 - mean_absolute_error: 0.0194 - val_loss: 2.4685e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 57/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.6566e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00057: val_loss did not improve from 0.00017\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 4.6503e-04 - mean_absolute_error: 0.0200 - val_loss: 2.2854e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 58/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.0605e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00058: val_loss improved from 0.00017 to 0.00016, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 4.0605e-04 - mean_absolute_error: 0.0191 - val_loss: 1.6449e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 59/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.5337e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00059: val_loss did not improve from 0.00016\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.5337e-04 - mean_absolute_error: 0.0181 - val_loss: 3.1607e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 60/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.9108e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00060: val_loss did not improve from 0.00016\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.9108e-04 - mean_absolute_error: 0.0191 - val_loss: 2.0215e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 61/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 4.2977e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 00061: val_loss did not improve from 0.00016\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 4.3065e-04 - mean_absolute_error: 0.0200 - val_loss: 2.0327e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 62/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.8018e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00062: val_loss improved from 0.00016 to 0.00016, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.7931e-04 - mean_absolute_error: 0.0189 - val_loss: 1.6091e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 63/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.8521e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00063: val_loss did not improve from 0.00016\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.8521e-04 - mean_absolute_error: 0.0188 - val_loss: 2.7988e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 64/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.7794e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00064: val_loss did not improve from 0.00016\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.7755e-04 - mean_absolute_error: 0.0191 - val_loss: 2.9572e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 65/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.5708e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00065: val_loss improved from 0.00016 to 0.00011, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 3.5708e-04 - mean_absolute_error: 0.0185 - val_loss: 1.1384e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 66/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.9680e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00066: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.9839e-04 - mean_absolute_error: 0.0186 - val_loss: 1.1767e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 67/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.7103e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00067: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.7103e-04 - mean_absolute_error: 0.0186 - val_loss: 1.2706e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 68/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.6100e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00068: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.5919e-04 - mean_absolute_error: 0.0179 - val_loss: 4.2874e-04 - val_mean_absolute_error: 0.0181\n",
      "Epoch 69/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.9464e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00069: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.9296e-04 - mean_absolute_error: 0.0186 - val_loss: 1.2246e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 70/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.4302e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00070: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.4194e-04 - mean_absolute_error: 0.0180 - val_loss: 1.7641e-04 - val_mean_absolute_error: 0.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.7040e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00071: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.7128e-04 - mean_absolute_error: 0.0183 - val_loss: 1.3743e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 72/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.5864e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00072: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.5830e-04 - mean_absolute_error: 0.0185 - val_loss: 1.4334e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 73/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.1555e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00073: val_loss improved from 0.00011 to 0.00011, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.1508e-04 - mean_absolute_error: 0.0174 - val_loss: 1.1210e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 74/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.3125e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00074: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.3063e-04 - mean_absolute_error: 0.0174 - val_loss: 1.3724e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 75/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.6426e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 00075: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.6430e-04 - mean_absolute_error: 0.0185 - val_loss: 2.1104e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 76/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.4634e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00076: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 3.4628e-04 - mean_absolute_error: 0.0181 - val_loss: 2.5391e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 77/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.1267e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00077: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.1238e-04 - mean_absolute_error: 0.0172 - val_loss: 1.1616e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 78/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.1003e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00078: val_loss improved from 0.00011 to 0.00011, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.1171e-04 - mean_absolute_error: 0.0172 - val_loss: 1.0964e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 79/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 4.3815e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00079: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 4.3815e-04 - mean_absolute_error: 0.0198 - val_loss: 2.3978e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 80/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.3001e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00080: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 3s 38ms/step - loss: 3.2945e-04 - mean_absolute_error: 0.0178 - val_loss: 1.5820e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 81/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.1699e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00081: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 3.1755e-04 - mean_absolute_error: 0.0173 - val_loss: 1.3080e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 82/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.3437e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00082: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.3437e-04 - mean_absolute_error: 0.0178 - val_loss: 2.5742e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 83/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.6503e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00083: val_loss did not improve from 0.00011\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.6503e-04 - mean_absolute_error: 0.0187 - val_loss: 1.7450e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 84/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.6204e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 00084: val_loss improved from 0.00011 to 0.00010, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.6419e-04 - mean_absolute_error: 0.0187 - val_loss: 9.9385e-05 - val_mean_absolute_error: 0.0098\n",
      "Epoch 85/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.4254e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00085: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.4264e-04 - mean_absolute_error: 0.0175 - val_loss: 2.2823e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 86/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.9603e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00086: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.9556e-04 - mean_absolute_error: 0.0191 - val_loss: 1.8549e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 87/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.4505e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00087: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.4505e-04 - mean_absolute_error: 0.0183 - val_loss: 1.0166e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 88/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.4540e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00088: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.4457e-04 - mean_absolute_error: 0.0178 - val_loss: 1.1187e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 89/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.9340e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00089: val_loss improved from 0.00010 to 0.00010, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.9340e-04 - mean_absolute_error: 0.0168 - val_loss: 9.5643e-05 - val_mean_absolute_error: 0.0107\n",
      "Epoch 90/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.9789e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00090: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.9923e-04 - mean_absolute_error: 0.0172 - val_loss: 1.0385e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 91/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.1312e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00091: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 3.1405e-04 - mean_absolute_error: 0.0174 - val_loss: 9.9387e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 92/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.9772e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00092: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.9772e-04 - mean_absolute_error: 0.0170 - val_loss: 1.2020e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 93/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.0931e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00093: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.0859e-04 - mean_absolute_error: 0.0172 - val_loss: 1.3383e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 94/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.2798e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00094: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.2778e-04 - mean_absolute_error: 0.0175 - val_loss: 1.2114e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/83 [============================>.] - ETA: 0s - loss: 3.2670e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00095: val_loss did not improve from 0.00010\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.2580e-04 - mean_absolute_error: 0.0176 - val_loss: 1.2723e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 96/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.8629e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00096: val_loss improved from 0.00010 to 0.00009, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.8590e-04 - mean_absolute_error: 0.0166 - val_loss: 8.5119e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 97/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8723e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00097: val_loss did not improve from 0.00009\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.8621e-04 - mean_absolute_error: 0.0167 - val_loss: 9.7273e-05 - val_mean_absolute_error: 0.0102\n",
      "Epoch 98/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.1444e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00098: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.1444e-04 - mean_absolute_error: 0.0176 - val_loss: 8.2408e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 99/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.9741e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00099: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 3.0181e-04 - mean_absolute_error: 0.0169 - val_loss: 9.3775e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 100/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.7777e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00100: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.7718e-04 - mean_absolute_error: 0.0163 - val_loss: 9.3286e-05 - val_mean_absolute_error: 0.0100\n",
      "Epoch 101/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.0039e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00101: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.9824e-04 - mean_absolute_error: 0.0166 - val_loss: 1.0211e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 102/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.4759e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00102: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.4587e-04 - mean_absolute_error: 0.0180 - val_loss: 9.7712e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 103/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.0569e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00103: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.0569e-04 - mean_absolute_error: 0.0173 - val_loss: 2.4576e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 104/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8458e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00104: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.8586e-04 - mean_absolute_error: 0.0169 - val_loss: 1.5600e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 105/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.8814e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00105: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.8928e-04 - mean_absolute_error: 0.0169 - val_loss: 1.1405e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 106/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.0498e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00106: val_loss did not improve from 0.00008\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.0579e-04 - mean_absolute_error: 0.0172 - val_loss: 1.4915e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 107/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.2706e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00107: val_loss improved from 0.00008 to 0.00007, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.2673e-04 - mean_absolute_error: 0.0177 - val_loss: 7.4166e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 108/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.8779e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00108: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.8779e-04 - mean_absolute_error: 0.0170 - val_loss: 1.3180e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 109/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.1522e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00109: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 3.1522e-04 - mean_absolute_error: 0.0172 - val_loss: 1.1139e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 110/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.0184e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00110: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 3.0191e-04 - mean_absolute_error: 0.0171 - val_loss: 1.7041e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 111/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.7702e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00111: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 3s 34ms/step - loss: 2.7714e-04 - mean_absolute_error: 0.0170 - val_loss: 1.2475e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 112/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.1820e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00112: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 3.1591e-04 - mean_absolute_error: 0.0174 - val_loss: 9.0516e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 113/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5935e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00113: val_loss improved from 0.00007 to 0.00007, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.5900e-04 - mean_absolute_error: 0.0157 - val_loss: 6.9873e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 114/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.9918e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00114: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.9862e-04 - mean_absolute_error: 0.0169 - val_loss: 7.2073e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 115/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8146e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00115: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.8045e-04 - mean_absolute_error: 0.0167 - val_loss: 1.0414e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 116/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8507e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00116: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.8486e-04 - mean_absolute_error: 0.0165 - val_loss: 1.2667e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 117/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8361e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00117: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.8408e-04 - mean_absolute_error: 0.0166 - val_loss: 1.2915e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 118/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.9705e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00118: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.9639e-04 - mean_absolute_error: 0.0169 - val_loss: 7.4145e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/83 [============================>.] - ETA: 0s - loss: 2.7625e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00119: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.7569e-04 - mean_absolute_error: 0.0165 - val_loss: 8.0788e-05 - val_mean_absolute_error: 0.0090\n",
      "Epoch 120/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.8349e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00120: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.8295e-04 - mean_absolute_error: 0.0166 - val_loss: 1.2223e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 121/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.7367e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00121: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.7356e-04 - mean_absolute_error: 0.0164 - val_loss: 1.7554e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 122/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8126e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00122: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.8144e-04 - mean_absolute_error: 0.0164 - val_loss: 1.0189e-04 - val_mean_absolute_error: 0.0100\n",
      "Epoch 123/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.4750e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 00123: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.4701e-04 - mean_absolute_error: 0.0179 - val_loss: 1.1038e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 124/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.7460e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00124: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.7654e-04 - mean_absolute_error: 0.0166 - val_loss: 8.9595e-05 - val_mean_absolute_error: 0.0105\n",
      "Epoch 125/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 3.0633e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00125: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 3.0633e-04 - mean_absolute_error: 0.0169 - val_loss: 9.8973e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 126/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.0177e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00126: val_loss did not improve from 0.00007\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.0089e-04 - mean_absolute_error: 0.0167 - val_loss: 9.9890e-05 - val_mean_absolute_error: 0.0107\n",
      "Epoch 127/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.1029e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00127: val_loss improved from 0.00007 to 0.00006, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 3.0983e-04 - mean_absolute_error: 0.0166 - val_loss: 5.7068e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 128/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.4043e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00128: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.3909e-04 - mean_absolute_error: 0.0153 - val_loss: 7.1024e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 129/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.1057e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00129: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 3.1054e-04 - mean_absolute_error: 0.0173 - val_loss: 1.4726e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 130/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 3.0590e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00130: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.0979e-04 - mean_absolute_error: 0.0172 - val_loss: 1.9850e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 131/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.4554e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00131: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 3.4465e-04 - mean_absolute_error: 0.0182 - val_loss: 1.4886e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 132/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6912e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00132: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 2.6825e-04 - mean_absolute_error: 0.0162 - val_loss: 5.8169e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 133/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.4505e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00133: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 2.4532e-04 - mean_absolute_error: 0.0153 - val_loss: 7.0833e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 134/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.8821e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00134: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.8641e-04 - mean_absolute_error: 0.0167 - val_loss: 6.1187e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 135/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.7535e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00135: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.7495e-04 - mean_absolute_error: 0.0166 - val_loss: 1.6696e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 136/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6628e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00136: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.6629e-04 - mean_absolute_error: 0.0162 - val_loss: 1.1169e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 137/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6472e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00137: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.6319e-04 - mean_absolute_error: 0.0164 - val_loss: 1.1557e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 138/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.6670e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00138: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 2.6670e-04 - mean_absolute_error: 0.0162 - val_loss: 1.0398e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 139/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.7870e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00139: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.7841e-04 - mean_absolute_error: 0.0164 - val_loss: 8.7845e-05 - val_mean_absolute_error: 0.0103\n",
      "Epoch 140/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.4889e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00140: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.4889e-04 - mean_absolute_error: 0.0157 - val_loss: 9.5852e-05 - val_mean_absolute_error: 0.0107\n",
      "Epoch 141/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6584e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00141: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.6527e-04 - mean_absolute_error: 0.0161 - val_loss: 1.2866e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 142/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5623e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00142: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.5592e-04 - mean_absolute_error: 0.0157 - val_loss: 6.7749e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 143/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6816e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00143: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.6696e-04 - mean_absolute_error: 0.0159 - val_loss: 9.6028e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/83 [============================>.] - ETA: 0s - loss: 2.3378e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00144: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.3315e-04 - mean_absolute_error: 0.0152 - val_loss: 7.2133e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 145/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.3527e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00145: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.3485e-04 - mean_absolute_error: 0.0154 - val_loss: 1.4802e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 146/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.7828e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00146: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.7733e-04 - mean_absolute_error: 0.0164 - val_loss: 7.2416e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 147/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5289e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00147: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.5266e-04 - mean_absolute_error: 0.0157 - val_loss: 2.1903e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 148/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 3.0983e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00148: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 3.1151e-04 - mean_absolute_error: 0.0169 - val_loss: 7.0217e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 149/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6170e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00149: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 2.6126e-04 - mean_absolute_error: 0.0158 - val_loss: 9.8772e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 150/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6098e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00150: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 3s 30ms/step - loss: 2.6108e-04 - mean_absolute_error: 0.0160 - val_loss: 7.9650e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 151/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.5392e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00151: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 2.5401e-04 - mean_absolute_error: 0.0158 - val_loss: 5.9196e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 152/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.8747e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00152: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.8728e-04 - mean_absolute_error: 0.0168 - val_loss: 7.0837e-05 - val_mean_absolute_error: 0.0087\n",
      "Epoch 153/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6387e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00153: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.6189e-04 - mean_absolute_error: 0.0161 - val_loss: 6.1818e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 154/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.4831e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00154: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.4783e-04 - mean_absolute_error: 0.0161 - val_loss: 6.0754e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 155/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.7039e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00155: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.7034e-04 - mean_absolute_error: 0.0163 - val_loss: 8.6517e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 156/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.4123e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00156: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.4095e-04 - mean_absolute_error: 0.0154 - val_loss: 7.6868e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 157/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.9137e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00157: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.9251e-04 - mean_absolute_error: 0.0166 - val_loss: 1.1679e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 158/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5872e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00158: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.5889e-04 - mean_absolute_error: 0.0161 - val_loss: 9.7183e-05 - val_mean_absolute_error: 0.0103\n",
      "Epoch 159/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6402e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00159: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.6467e-04 - mean_absolute_error: 0.0161 - val_loss: 1.3965e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 160/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.5763e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00160: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.5763e-04 - mean_absolute_error: 0.0158 - val_loss: 7.6915e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 161/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.9326e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00161: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 25ms/step - loss: 2.9258e-04 - mean_absolute_error: 0.0163 - val_loss: 9.7870e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 162/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.8422e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00162: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.8422e-04 - mean_absolute_error: 0.0164 - val_loss: 1.0505e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 163/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.8173e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00163: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.8126e-04 - mean_absolute_error: 0.0160 - val_loss: 7.9377e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 164/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.3632e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00164: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.3763e-04 - mean_absolute_error: 0.0152 - val_loss: 6.9183e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 165/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.4127e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00165: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.4060e-04 - mean_absolute_error: 0.0154 - val_loss: 8.3974e-05 - val_mean_absolute_error: 0.0088\n",
      "Epoch 166/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.3872e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00166: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.3936e-04 - mean_absolute_error: 0.0153 - val_loss: 6.9423e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 167/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.4900e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00167: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 25ms/step - loss: 2.4760e-04 - mean_absolute_error: 0.0157 - val_loss: 7.4934e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 168/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.2541e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00168: val_loss did not improve from 0.00006\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.2444e-04 - mean_absolute_error: 0.0152 - val_loss: 5.9029e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 169/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.4632e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00169: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.4632e-04 - mean_absolute_error: 0.0152 - val_loss: 5.4832e-05 - val_mean_absolute_error: 0.0074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.4885e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00170: val_loss did not improve from 0.00005\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.4876e-04 - mean_absolute_error: 0.0154 - val_loss: 6.7715e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 171/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.2770e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00171: val_loss did not improve from 0.00005\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 2.2744e-04 - mean_absolute_error: 0.0146 - val_loss: 9.0409e-05 - val_mean_absolute_error: 0.0098\n",
      "Epoch 172/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.3858e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00172: val_loss did not improve from 0.00005\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.3950e-04 - mean_absolute_error: 0.0152 - val_loss: 7.4931e-05 - val_mean_absolute_error: 0.0096\n",
      "Epoch 173/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.3680e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00173: val_loss did not improve from 0.00005\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.3686e-04 - mean_absolute_error: 0.0152 - val_loss: 1.8016e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 174/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.4420e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00174: val_loss did not improve from 0.00005\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.4524e-04 - mean_absolute_error: 0.0156 - val_loss: 8.7118e-05 - val_mean_absolute_error: 0.0102\n",
      "Epoch 175/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.5325e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00175: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-300-step-30-layers-3-units-256.h5\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.5224e-04 - mean_absolute_error: 0.0157 - val_loss: 4.4433e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 176/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.0837e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00176: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.0747e-04 - mean_absolute_error: 0.0142 - val_loss: 6.0732e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 177/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.3399e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00177: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.3448e-04 - mean_absolute_error: 0.0155 - val_loss: 7.4525e-05 - val_mean_absolute_error: 0.0090\n",
      "Epoch 178/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5867e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00178: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.5783e-04 - mean_absolute_error: 0.0158 - val_loss: 5.4779e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 179/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.4491e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00179: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.4424e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3335e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 180/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5661e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00180: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 34ms/step - loss: 2.5666e-04 - mean_absolute_error: 0.0155 - val_loss: 1.3720e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 181/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6719e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00181: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 2.6781e-04 - mean_absolute_error: 0.0159 - val_loss: 1.1324e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 182/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.4584e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00182: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.4593e-04 - mean_absolute_error: 0.0156 - val_loss: 7.1965e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 183/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6463e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00183: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.6556e-04 - mean_absolute_error: 0.0158 - val_loss: 4.9263e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 184/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.3145e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00184: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 2.3069e-04 - mean_absolute_error: 0.0147 - val_loss: 2.0473e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 185/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6466e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00185: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 31ms/step - loss: 2.6320e-04 - mean_absolute_error: 0.0160 - val_loss: 1.1693e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 186/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.2436e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00186: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 2.2398e-04 - mean_absolute_error: 0.0149 - val_loss: 1.2588e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 187/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.2149e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00187: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.2342e-04 - mean_absolute_error: 0.0147 - val_loss: 6.0413e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 188/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.6875e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00188: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.6895e-04 - mean_absolute_error: 0.0158 - val_loss: 5.0630e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 189/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.3963e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00189: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 2.4012e-04 - mean_absolute_error: 0.0152 - val_loss: 7.1400e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 190/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.6015e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00190: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 39ms/step - loss: 2.6013e-04 - mean_absolute_error: 0.0159 - val_loss: 8.0322e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 191/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.3377e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00191: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 2.3347e-04 - mean_absolute_error: 0.0148 - val_loss: 1.9852e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 192/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.3913e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00192: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 30ms/step - loss: 2.4090e-04 - mean_absolute_error: 0.0151 - val_loss: 5.6917e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 193/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.3738e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00193: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.3756e-04 - mean_absolute_error: 0.0151 - val_loss: 1.4781e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 194/200\n",
      "83/83 [==============================] - ETA: 0s - loss: 2.2005e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00194: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.2005e-04 - mean_absolute_error: 0.0145 - val_loss: 5.8102e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/83 [============================>.] - ETA: 0s - loss: 2.3902e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00195: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.3870e-04 - mean_absolute_error: 0.0153 - val_loss: 5.1224e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 196/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.5798e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00196: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.5714e-04 - mean_absolute_error: 0.0151 - val_loss: 6.2653e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 197/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 1.9157e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00197: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 3s 32ms/step - loss: 1.9179e-04 - mean_absolute_error: 0.0138 - val_loss: 8.3171e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 198/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.2797e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00198: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 29ms/step - loss: 2.2756e-04 - mean_absolute_error: 0.0149 - val_loss: 7.0797e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 199/200\n",
      "82/83 [============================>.] - ETA: 0s - loss: 2.2904e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00199: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 28ms/step - loss: 2.2866e-04 - mean_absolute_error: 0.0149 - val_loss: 9.6720e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 200/200\n",
      "81/83 [============================>.] - ETA: 0s - loss: 2.0597e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00200: val_loss did not improve from 0.00004\n",
      "83/83 [==============================] - 2s 27ms/step - loss: 2.0660e-04 - mean_absolute_error: 0.0140 - val_loss: 5.6970e-05 - val_mean_absolute_error: 0.0077\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 30 days is 301.72$\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: Accuracy Score: 0.834\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    money = 1\n",
    "    moneylist = []\n",
    "    long_only = 1\n",
    "    long_only_list = []\n",
    "    for i in range(1,len(y_test)):\n",
    "        if i % 30 == 0:\n",
    "            if y_pred[i] >= y_pred[i-30]:\n",
    "                long_short = 1\n",
    "            else:\n",
    "                long_short = -1\n",
    "            money =  money * (1 + (y_test[i] - y_test[i-30])/y_test[i-30] * long_short) \n",
    "        long_only = long_only * (1 + (y_test[i] - y_test[i-1])/y_test[i-1] * 1) \n",
    "        moneylist.append(money)\n",
    "        long_only_list.append(long_only)\n",
    "    plt.plot(moneylist)\n",
    "    plt.plot(long_only_list)\n",
    "    return money,long_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.994031142364982, 2.6685834680123746)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_return(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQQ prediction using previous 500 day for next 100 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which stock to predict\n",
    "ticker = \"QQQ\"\n",
    "\n",
    "# use numbers of previous days to predict\n",
    "N_STEPS = 500\n",
    "\n",
    "# which day to predict, 1 is for next day, 10 is for 10 days after today\n",
    "LOOKUP_STEP = 100\n",
    "\n",
    "### model parameters\n",
    "N_LAYERS = 3\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.5\n",
    "\n",
    "### training parameters\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 200\n",
    "\n",
    "### other parameters\n",
    "TEST_SIZE = 0.2\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### save data to csv locally\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 2/79 [..............................] - ETA: 9s - loss: 0.0214 - mean_absolute_error: 0.1427WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0364s vs `on_train_batch_end` time: 0.1988s). Check your callbacks.\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0611\n",
      "Epoch 00001: val_loss improved from inf to 0.00149, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 5s 60ms/step - loss: 0.0042 - mean_absolute_error: 0.0611 - val_loss: 0.0015 - val_mean_absolute_error: 0.0377\n",
      "Epoch 2/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0020 - mean_absolute_error: 0.0444\n",
      "Epoch 00002: val_loss did not improve from 0.00149\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 0.0020 - mean_absolute_error: 0.0444 - val_loss: 0.0021 - val_mean_absolute_error: 0.0480\n",
      "Epoch 3/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0021 - mean_absolute_error: 0.0436\n",
      "Epoch 00003: val_loss improved from 0.00149 to 0.00122, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 0.0021 - mean_absolute_error: 0.0436 - val_loss: 0.0012 - val_mean_absolute_error: 0.0373\n",
      "Epoch 4/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0447\n",
      "Epoch 00004: val_loss did not improve from 0.00122\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 0.0023 - mean_absolute_error: 0.0446 - val_loss: 0.0030 - val_mean_absolute_error: 0.0512\n",
      "Epoch 5/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0023 - mean_absolute_error: 0.0438\n",
      "Epoch 00005: val_loss improved from 0.00122 to 0.00086, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.0023 - mean_absolute_error: 0.0438 - val_loss: 8.6308e-04 - val_mean_absolute_error: 0.0285\n",
      "Epoch 6/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.0380\n",
      "Epoch 00006: val_loss did not improve from 0.00086\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.0017 - mean_absolute_error: 0.0380 - val_loss: 0.0024 - val_mean_absolute_error: 0.0506\n",
      "Epoch 7/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0015 - mean_absolute_error: 0.0345\n",
      "Epoch 00007: val_loss improved from 0.00086 to 0.00081, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 0.0015 - mean_absolute_error: 0.0345 - val_loss: 8.0693e-04 - val_mean_absolute_error: 0.0270\n",
      "Epoch 8/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0013 - mean_absolute_error: 0.0324\n",
      "Epoch 00008: val_loss improved from 0.00081 to 0.00052, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.0013 - mean_absolute_error: 0.0324 - val_loss: 5.1523e-04 - val_mean_absolute_error: 0.0217\n",
      "Epoch 9/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0315\n",
      "Epoch 00009: val_loss did not improve from 0.00052\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.0012 - mean_absolute_error: 0.0316 - val_loss: 5.5150e-04 - val_mean_absolute_error: 0.0207\n",
      "Epoch 10/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0315\n",
      "Epoch 00010: val_loss improved from 0.00052 to 0.00051, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 0.0012 - mean_absolute_error: 0.0315 - val_loss: 5.1000e-04 - val_mean_absolute_error: 0.0220\n",
      "Epoch 11/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0288  \n",
      "Epoch 00011: val_loss did not improve from 0.00051\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.0010 - mean_absolute_error: 0.0288 - val_loss: 6.5622e-04 - val_mean_absolute_error: 0.0245\n",
      "Epoch 12/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0293   \n",
      "Epoch 00012: val_loss improved from 0.00051 to 0.00050, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.0010 - mean_absolute_error: 0.0292 - val_loss: 4.9554e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 13/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 9.9210e-04 - mean_absolute_error: 0.0288\n",
      "Epoch 00013: val_loss did not improve from 0.00050\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 9.9160e-04 - mean_absolute_error: 0.0287 - val_loss: 0.0013 - val_mean_absolute_error: 0.0364\n",
      "Epoch 14/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0298\n",
      "Epoch 00014: val_loss did not improve from 0.00050\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 0.0011 - mean_absolute_error: 0.0298 - val_loss: 8.9939e-04 - val_mean_absolute_error: 0.0306\n",
      "Epoch 15/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0289\n",
      "Epoch 00015: val_loss improved from 0.00050 to 0.00039, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 0.0010 - mean_absolute_error: 0.0289 - val_loss: 3.8941e-04 - val_mean_absolute_error: 0.0194\n",
      "Epoch 16/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0294\n",
      "Epoch 00016: val_loss did not improve from 0.00039\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 0.0011 - mean_absolute_error: 0.0294 - val_loss: 7.6558e-04 - val_mean_absolute_error: 0.0249\n",
      "Epoch 17/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 9.3152e-04 - mean_absolute_error: 0.0276\n",
      "Epoch 00017: val_loss improved from 0.00039 to 0.00036, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 9.3152e-04 - mean_absolute_error: 0.0276 - val_loss: 3.6050e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 18/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0010 - mean_absolute_error: 0.0289\n",
      "Epoch 00018: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 0.0010 - mean_absolute_error: 0.0289 - val_loss: 4.5617e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 19/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 9.2000e-04 - mean_absolute_error: 0.0275\n",
      "Epoch 00019: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 9.2000e-04 - mean_absolute_error: 0.0275 - val_loss: 6.4765e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 20/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 8.9869e-04 - mean_absolute_error: 0.0281\n",
      "Epoch 00020: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 8.9832e-04 - mean_absolute_error: 0.0281 - val_loss: 5.6063e-04 - val_mean_absolute_error: 0.0228\n",
      "Epoch 21/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 9.6096e-04 - mean_absolute_error: 0.0286\n",
      "Epoch 00021: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 9.6096e-04 - mean_absolute_error: 0.0286 - val_loss: 0.0019 - val_mean_absolute_error: 0.0364\n",
      "Epoch 22/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0016 - mean_absolute_error: 0.0346\n",
      "Epoch 00022: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 0.0016 - mean_absolute_error: 0.0346 - val_loss: 4.4397e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - ETA: 0s - loss: 8.3851e-04 - mean_absolute_error: 0.0269\n",
      "Epoch 00023: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 8.3851e-04 - mean_absolute_error: 0.0269 - val_loss: 3.8377e-04 - val_mean_absolute_error: 0.0191\n",
      "Epoch 24/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 8.5992e-04 - mean_absolute_error: 0.0262\n",
      "Epoch 00024: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 8.5992e-04 - mean_absolute_error: 0.0262 - val_loss: 0.0031 - val_mean_absolute_error: 0.0505\n",
      "Epoch 25/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.0376\n",
      "Epoch 00025: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 0.0017 - mean_absolute_error: 0.0376 - val_loss: 4.5872e-04 - val_mean_absolute_error: 0.0213\n",
      "Epoch 26/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 9.1186e-04 - mean_absolute_error: 0.0273\n",
      "Epoch 00026: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 39ms/step - loss: 9.1186e-04 - mean_absolute_error: 0.0273 - val_loss: 5.1165e-04 - val_mean_absolute_error: 0.0238\n",
      "Epoch 27/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 7.4054e-04 - mean_absolute_error: 0.0263\n",
      "Epoch 00027: val_loss did not improve from 0.00036\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 7.4022e-04 - mean_absolute_error: 0.0263 - val_loss: 5.9518e-04 - val_mean_absolute_error: 0.0243\n",
      "Epoch 28/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 7.0475e-04 - mean_absolute_error: 0.0250\n",
      "Epoch 00028: val_loss improved from 0.00036 to 0.00025, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 39ms/step - loss: 7.0475e-04 - mean_absolute_error: 0.0250 - val_loss: 2.4856e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 29/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 6.8821e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 00029: val_loss did not improve from 0.00025\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 6.8902e-04 - mean_absolute_error: 0.0239 - val_loss: 5.9682e-04 - val_mean_absolute_error: 0.0200\n",
      "Epoch 30/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0011 - mean_absolute_error: 0.0296\n",
      "Epoch 00030: val_loss did not improve from 0.00025\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 0.0011 - mean_absolute_error: 0.0296 - val_loss: 3.5526e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 31/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 9.7774e-04 - mean_absolute_error: 0.0280\n",
      "Epoch 00031: val_loss did not improve from 0.00025\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 9.7738e-04 - mean_absolute_error: 0.0280 - val_loss: 2.7786e-04 - val_mean_absolute_error: 0.0153\n",
      "Epoch 32/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 6.2627e-04 - mean_absolute_error: 0.0237\n",
      "Epoch 00032: val_loss did not improve from 0.00025\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 6.2627e-04 - mean_absolute_error: 0.0237 - val_loss: 4.8532e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 33/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 7.1698e-04 - mean_absolute_error: 0.0252\n",
      "Epoch 00033: val_loss did not improve from 0.00025\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 7.1679e-04 - mean_absolute_error: 0.0252 - val_loss: 2.7385e-04 - val_mean_absolute_error: 0.0174\n",
      "Epoch 34/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 6.5141e-04 - mean_absolute_error: 0.0241\n",
      "Epoch 00034: val_loss improved from 0.00025 to 0.00024, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 6.5127e-04 - mean_absolute_error: 0.0241 - val_loss: 2.3990e-04 - val_mean_absolute_error: 0.0165\n",
      "Epoch 35/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 6.5563e-04 - mean_absolute_error: 0.0241\n",
      "Epoch 00035: val_loss improved from 0.00024 to 0.00017, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 6.5563e-04 - mean_absolute_error: 0.0241 - val_loss: 1.7412e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 36/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 6.1351e-04 - mean_absolute_error: 0.0233\n",
      "Epoch 00036: val_loss did not improve from 0.00017\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 6.1351e-04 - mean_absolute_error: 0.0233 - val_loss: 3.0432e-04 - val_mean_absolute_error: 0.0180\n",
      "Epoch 37/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 5.2428e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 00037: val_loss did not improve from 0.00017\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 5.2428e-04 - mean_absolute_error: 0.0218 - val_loss: 3.3971e-04 - val_mean_absolute_error: 0.0159\n",
      "Epoch 38/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 4.7592e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 00038: val_loss did not improve from 0.00017\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 4.7574e-04 - mean_absolute_error: 0.0213 - val_loss: 1.7719e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 39/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.3612e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00039: val_loss did not improve from 0.00017\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 4.3612e-04 - mean_absolute_error: 0.0204 - val_loss: 3.7182e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 40/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 4.8717e-04 - mean_absolute_error: 0.0215\n",
      "Epoch 00040: val_loss did not improve from 0.00017\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 4.8474e-04 - mean_absolute_error: 0.0214 - val_loss: 2.8536e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 41/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.4005e-04 - mean_absolute_error: 0.0207\n",
      "Epoch 00041: val_loss improved from 0.00017 to 0.00016, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 4.4005e-04 - mean_absolute_error: 0.0207 - val_loss: 1.5635e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 42/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.3610e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00042: val_loss did not improve from 0.00016\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 4.3610e-04 - mean_absolute_error: 0.0199 - val_loss: 1.6278e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 43/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.6742e-04 - mean_absolute_error: 0.0204\n",
      "Epoch 00043: val_loss improved from 0.00016 to 0.00013, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 4.6742e-04 - mean_absolute_error: 0.0204 - val_loss: 1.3428e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 44/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 4.3360e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00044: val_loss did not improve from 0.00013\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 4.3287e-04 - mean_absolute_error: 0.0201 - val_loss: 1.8998e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 45/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.8724e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00045: val_loss improved from 0.00013 to 0.00012, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.8724e-04 - mean_absolute_error: 0.0194 - val_loss: 1.2356e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 46/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 4.1149e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00046: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 4.1188e-04 - mean_absolute_error: 0.0193 - val_loss: 1.4885e-04 - val_mean_absolute_error: 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 4.4307e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00047: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 4.4323e-04 - mean_absolute_error: 0.0202 - val_loss: 1.3739e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 48/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 4.8193e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 00048: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 4.8171e-04 - mean_absolute_error: 0.0211 - val_loss: 1.5689e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 49/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.0245e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00049: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 4.0245e-04 - mean_absolute_error: 0.0194 - val_loss: 1.5276e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 50/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.9969e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00050: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 3.9969e-04 - mean_absolute_error: 0.0189 - val_loss: 3.3875e-04 - val_mean_absolute_error: 0.0204\n",
      "Epoch 51/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.9448e-04 - mean_absolute_error: 0.0212\n",
      "Epoch 00051: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 4.9448e-04 - mean_absolute_error: 0.0212 - val_loss: 1.2567e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 52/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 4.2221e-04 - mean_absolute_error: 0.0202\n",
      "Epoch 00052: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 4.2201e-04 - mean_absolute_error: 0.0202 - val_loss: 1.8714e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 53/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.1508e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00053: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 4.1508e-04 - mean_absolute_error: 0.0193 - val_loss: 1.3158e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 54/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.5081e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00054: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.5064e-04 - mean_absolute_error: 0.0183 - val_loss: 1.4158e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 55/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.9276e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00055: val_loss improved from 0.00012 to 0.00012, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.9289e-04 - mean_absolute_error: 0.0189 - val_loss: 1.2118e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 56/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.9524e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 00056: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.9546e-04 - mean_absolute_error: 0.0198 - val_loss: 1.2625e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 57/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.7610e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 00057: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 3.7610e-04 - mean_absolute_error: 0.0191 - val_loss: 1.4468e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 58/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.6097e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00058: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.6097e-04 - mean_absolute_error: 0.0184 - val_loss: 1.3860e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 59/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.9676e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 00059: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 3.9676e-04 - mean_absolute_error: 0.0189 - val_loss: 1.2258e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 60/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.5699e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00060: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 41ms/step - loss: 3.5681e-04 - mean_absolute_error: 0.0184 - val_loss: 1.4259e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 61/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 4.0030e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00061: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 4.0050e-04 - mean_absolute_error: 0.0193 - val_loss: 1.5863e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 62/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.7615e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00062: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 3.7615e-04 - mean_absolute_error: 0.0186 - val_loss: 2.1908e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 63/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.8356e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00063: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 3.8356e-04 - mean_absolute_error: 0.0190 - val_loss: 3.7775e-04 - val_mean_absolute_error: 0.0203\n",
      "Epoch 64/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.7400e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00064: val_loss improved from 0.00012 to 0.00012, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 3.7400e-04 - mean_absolute_error: 0.0186 - val_loss: 1.2104e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 65/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.6740e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 00065: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 3.6723e-04 - mean_absolute_error: 0.0184 - val_loss: 1.5132e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 66/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.9669e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 00066: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 3.9668e-04 - mean_absolute_error: 0.0193 - val_loss: 1.9193e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 67/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 3.2373e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00067: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 3.2221e-04 - mean_absolute_error: 0.0179 - val_loss: 1.3306e-04 - val_mean_absolute_error: 0.0111\n",
      "Epoch 68/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.3558e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00068: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 3.3547e-04 - mean_absolute_error: 0.0176 - val_loss: 1.3477e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 69/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.7559e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00069: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 3.7559e-04 - mean_absolute_error: 0.0188 - val_loss: 1.3740e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 70/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.0350e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00070: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.0350e-04 - mean_absolute_error: 0.0170 - val_loss: 1.2783e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 71/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.2697e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 00071: val_loss improved from 0.00012 to 0.00012, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.2697e-04 - mean_absolute_error: 0.0174 - val_loss: 1.1924e-04 - val_mean_absolute_error: 0.0110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 4.4889e-04 - mean_absolute_error: 0.0199\n",
      "Epoch 00072: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 4.4884e-04 - mean_absolute_error: 0.0200 - val_loss: 1.2247e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 73/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.4237e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 00073: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 4.4237e-04 - mean_absolute_error: 0.0201 - val_loss: 3.1632e-04 - val_mean_absolute_error: 0.0198\n",
      "Epoch 74/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 3.7123e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 00074: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.7096e-04 - mean_absolute_error: 0.0187 - val_loss: 1.4521e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 75/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.3773e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00075: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 3.3778e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8764e-04 - val_mean_absolute_error: 0.0199\n",
      "Epoch 76/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.7929e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 00076: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.7911e-04 - mean_absolute_error: 0.0190 - val_loss: 1.6651e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 77/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.4996e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 00077: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.4996e-04 - mean_absolute_error: 0.0181 - val_loss: 5.3841e-04 - val_mean_absolute_error: 0.0231\n",
      "Epoch 78/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 5.4460e-04 - mean_absolute_error: 0.0218\n",
      "Epoch 00078: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 5.4438e-04 - mean_absolute_error: 0.0218 - val_loss: 1.4169e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 79/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 4.0183e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00079: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 4.0168e-04 - mean_absolute_error: 0.0194 - val_loss: 1.5005e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 80/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 3.6629e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00080: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.6304e-04 - mean_absolute_error: 0.0185 - val_loss: 1.7254e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 81/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.0857e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 00081: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.0848e-04 - mean_absolute_error: 0.0173 - val_loss: 2.1481e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 82/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.6846e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00082: val_loss did not improve from 0.00012\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.6847e-04 - mean_absolute_error: 0.0182 - val_loss: 2.0470e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 83/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 3.4321e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00083: val_loss improved from 0.00012 to 0.00012, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.4208e-04 - mean_absolute_error: 0.0176 - val_loss: 1.1617e-04 - val_mean_absolute_error: 0.0105\n",
      "Epoch 84/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.2230e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00084: val_loss improved from 0.00012 to 0.00009, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 3.2250e-04 - mean_absolute_error: 0.0180 - val_loss: 8.9536e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 85/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.3901e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 00085: val_loss did not improve from 0.00009\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.3886e-04 - mean_absolute_error: 0.0186 - val_loss: 1.6710e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 86/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.2586e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00086: val_loss did not improve from 0.00009\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.2575e-04 - mean_absolute_error: 0.0175 - val_loss: 1.0704e-04 - val_mean_absolute_error: 0.0109\n",
      "Epoch 87/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.7029e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00087: val_loss did not improve from 0.00009\n",
      "79/79 [==============================] - 2s 29ms/step - loss: 2.7056e-04 - mean_absolute_error: 0.0165 - val_loss: 9.6699e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 88/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.8790e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00088: val_loss did not improve from 0.00009\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.8810e-04 - mean_absolute_error: 0.0167 - val_loss: 2.4411e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 89/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.1607e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 00089: val_loss did not improve from 0.00009\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 3.1607e-04 - mean_absolute_error: 0.0178 - val_loss: 1.2319e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 90/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.9844e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00090: val_loss did not improve from 0.00009\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 2.9844e-04 - mean_absolute_error: 0.0171 - val_loss: 2.3580e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 91/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.9901e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00091: val_loss improved from 0.00009 to 0.00008, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.9889e-04 - mean_absolute_error: 0.0167 - val_loss: 7.8479e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 92/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.1171e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00092: val_loss did not improve from 0.00008\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.1171e-04 - mean_absolute_error: 0.0175 - val_loss: 1.4725e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 93/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.3797e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 00093: val_loss did not improve from 0.00008\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 3.3797e-04 - mean_absolute_error: 0.0177 - val_loss: 1.9564e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 94/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.0474e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 00094: val_loss did not improve from 0.00008\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 3.0474e-04 - mean_absolute_error: 0.0169 - val_loss: 9.4866e-05 - val_mean_absolute_error: 0.0102\n",
      "Epoch 95/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.8650e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 00095: val_loss did not improve from 0.00008\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.8650e-04 - mean_absolute_error: 0.0165 - val_loss: 4.4679e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 96/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.4431e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00096: val_loss did not improve from 0.00008\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.4431e-04 - mean_absolute_error: 0.0175 - val_loss: 2.4222e-04 - val_mean_absolute_error: 0.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.1411e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 00097: val_loss did not improve from 0.00008\n",
      "79/79 [==============================] - 2s 32ms/step - loss: 3.1412e-04 - mean_absolute_error: 0.0166 - val_loss: 1.1613e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 98/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.3759e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 00098: val_loss improved from 0.00008 to 0.00007, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 3.3759e-04 - mean_absolute_error: 0.0176 - val_loss: 7.1844e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 99/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.7628e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00099: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.7614e-04 - mean_absolute_error: 0.0163 - val_loss: 1.2566e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 100/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.9530e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00100: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.9530e-04 - mean_absolute_error: 0.0164 - val_loss: 1.3402e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 101/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.1122e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00101: val_loss improved from 0.00007 to 0.00007, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 3.1122e-04 - mean_absolute_error: 0.0171 - val_loss: 6.5103e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 102/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.5678e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00102: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.5678e-04 - mean_absolute_error: 0.0157 - val_loss: 6.9932e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 103/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.6070e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00103: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 42ms/step - loss: 2.6068e-04 - mean_absolute_error: 0.0157 - val_loss: 1.6415e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 104/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.1617e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 00104: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 3.1617e-04 - mean_absolute_error: 0.0172 - val_loss: 1.2916e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 105/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.6391e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00105: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 2.6391e-04 - mean_absolute_error: 0.0157 - val_loss: 8.9200e-05 - val_mean_absolute_error: 0.0106\n",
      "Epoch 106/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.6974e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00106: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.6974e-04 - mean_absolute_error: 0.0159 - val_loss: 9.8754e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 107/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.7402e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00107: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.7402e-04 - mean_absolute_error: 0.0159 - val_loss: 1.0090e-04 - val_mean_absolute_error: 0.0097\n",
      "Epoch 108/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.5739e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 00108: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.5739e-04 - mean_absolute_error: 0.0155 - val_loss: 7.7957e-05 - val_mean_absolute_error: 0.0083\n",
      "Epoch 109/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.0861e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 00109: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.0855e-04 - mean_absolute_error: 0.0170 - val_loss: 1.0673e-04 - val_mean_absolute_error: 0.0104\n",
      "Epoch 110/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.0023e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00110: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 3.0022e-04 - mean_absolute_error: 0.0167 - val_loss: 1.1134e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 111/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.9261e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00111: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.9427e-04 - mean_absolute_error: 0.0163 - val_loss: 7.0722e-05 - val_mean_absolute_error: 0.0089\n",
      "Epoch 112/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.9960e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00112: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 2.9960e-04 - mean_absolute_error: 0.0164 - val_loss: 2.8237e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 113/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.1477e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00113: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 3.1477e-04 - mean_absolute_error: 0.0168 - val_loss: 6.6077e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 114/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.4235e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00114: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.4235e-04 - mean_absolute_error: 0.0152 - val_loss: 1.4085e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 115/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 3.0748e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00115: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 3.0748e-04 - mean_absolute_error: 0.0171 - val_loss: 8.2120e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 116/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.7584e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00116: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 2.7584e-04 - mean_absolute_error: 0.0160 - val_loss: 7.4007e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 117/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.7124e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00117: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.7139e-04 - mean_absolute_error: 0.0159 - val_loss: 2.1456e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 118/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.7935e-04 - mean_absolute_error: 0.0162\n",
      "Epoch 00118: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.7930e-04 - mean_absolute_error: 0.0162 - val_loss: 6.8656e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 119/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 3.2192e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 00119: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.1924e-04 - mean_absolute_error: 0.0170 - val_loss: 1.0099e-04 - val_mean_absolute_error: 0.0099\n",
      "Epoch 120/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.6665e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00120: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.6665e-04 - mean_absolute_error: 0.0158 - val_loss: 1.3576e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 121/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.3091e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 00121: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 3.3079e-04 - mean_absolute_error: 0.0175 - val_loss: 1.1220e-04 - val_mean_absolute_error: 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.4016e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00122: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.3945e-04 - mean_absolute_error: 0.0153 - val_loss: 7.9610e-05 - val_mean_absolute_error: 0.0091\n",
      "Epoch 123/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.4989e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00123: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 38ms/step - loss: 2.4985e-04 - mean_absolute_error: 0.0151 - val_loss: 8.5134e-05 - val_mean_absolute_error: 0.0099\n",
      "Epoch 124/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.6679e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 00124: val_loss did not improve from 0.00007\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.6679e-04 - mean_absolute_error: 0.0163 - val_loss: 1.5877e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 125/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.4777e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00125: val_loss improved from 0.00007 to 0.00006, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.4775e-04 - mean_absolute_error: 0.0154 - val_loss: 5.9320e-05 - val_mean_absolute_error: 0.0079\n",
      "Epoch 126/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.6522e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00126: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.6528e-04 - mean_absolute_error: 0.0157 - val_loss: 1.1079e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 127/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.8094e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00127: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.7921e-04 - mean_absolute_error: 0.0164 - val_loss: 8.6468e-05 - val_mean_absolute_error: 0.0100\n",
      "Epoch 128/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.4161e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00128: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.4161e-04 - mean_absolute_error: 0.0148 - val_loss: 7.6189e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 129/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.9582e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00129: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.9616e-04 - mean_absolute_error: 0.0164 - val_loss: 2.0652e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 130/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.5714e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 00130: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 3.5700e-04 - mean_absolute_error: 0.0183 - val_loss: 1.1720e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 131/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.6332e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00131: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 2.6323e-04 - mean_absolute_error: 0.0158 - val_loss: 1.0151e-04 - val_mean_absolute_error: 0.0112\n",
      "Epoch 132/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.7177e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00132: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.7177e-04 - mean_absolute_error: 0.0159 - val_loss: 1.4608e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 133/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.4289e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 00133: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.4270e-04 - mean_absolute_error: 0.0153 - val_loss: 9.3398e-05 - val_mean_absolute_error: 0.0106\n",
      "Epoch 134/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.7043e-04 - mean_absolute_error: 0.0161\n",
      "Epoch 00134: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 2.7043e-04 - mean_absolute_error: 0.0161 - val_loss: 2.6400e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 135/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.7557e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00135: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.7557e-04 - mean_absolute_error: 0.0156 - val_loss: 7.5938e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 136/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.3223e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00136: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.3212e-04 - mean_absolute_error: 0.0145 - val_loss: 8.0317e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 137/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.2076e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00137: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.2066e-04 - mean_absolute_error: 0.0144 - val_loss: 7.0355e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 138/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.3058e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00138: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.3056e-04 - mean_absolute_error: 0.0148 - val_loss: 6.3508e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 139/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.3223e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00139: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.3223e-04 - mean_absolute_error: 0.0148 - val_loss: 6.9097e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 140/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.4774e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00140: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.4671e-04 - mean_absolute_error: 0.0150 - val_loss: 1.1237e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 141/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.5435e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00141: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 2.5366e-04 - mean_absolute_error: 0.0152 - val_loss: 1.0873e-04 - val_mean_absolute_error: 0.0095\n",
      "Epoch 142/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.6209e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00142: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.6218e-04 - mean_absolute_error: 0.0157 - val_loss: 1.2793e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 143/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 3.0610e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00143: val_loss improved from 0.00006 to 0.00006, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.0604e-04 - mean_absolute_error: 0.0167 - val_loss: 5.5827e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 144/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.4981e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00144: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.4981e-04 - mean_absolute_error: 0.0149 - val_loss: 1.2815e-04 - val_mean_absolute_error: 0.0096\n",
      "Epoch 145/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.8409e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00145: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.8428e-04 - mean_absolute_error: 0.0158 - val_loss: 1.0864e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 146/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.9041e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00146: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.9028e-04 - mean_absolute_error: 0.0168 - val_loss: 1.1716e-04 - val_mean_absolute_error: 0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.5987e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00147: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.5979e-04 - mean_absolute_error: 0.0158 - val_loss: 7.5498e-05 - val_mean_absolute_error: 0.0082\n",
      "Epoch 148/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.2229e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00148: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.2247e-04 - mean_absolute_error: 0.0144 - val_loss: 1.0005e-04 - val_mean_absolute_error: 0.0106\n",
      "Epoch 149/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.8110e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 00149: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.8121e-04 - mean_absolute_error: 0.0167 - val_loss: 9.1659e-05 - val_mean_absolute_error: 0.0103\n",
      "Epoch 150/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.6036e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 00150: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.6091e-04 - mean_absolute_error: 0.0158 - val_loss: 1.4717e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 151/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.9454e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 00151: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.9456e-04 - mean_absolute_error: 0.0168 - val_loss: 1.4226e-04 - val_mean_absolute_error: 0.0108\n",
      "Epoch 152/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.6003e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00152: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.5994e-04 - mean_absolute_error: 0.0156 - val_loss: 7.3762e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 153/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.7249e-04 - mean_absolute_error: 0.0159\n",
      "Epoch 00153: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.7241e-04 - mean_absolute_error: 0.0159 - val_loss: 6.7845e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 154/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.0777e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00154: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.0775e-04 - mean_absolute_error: 0.0142 - val_loss: 7.2787e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 155/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.4313e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00155: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 2.4313e-04 - mean_absolute_error: 0.0148 - val_loss: 6.4938e-05 - val_mean_absolute_error: 0.0085\n",
      "Epoch 156/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.4921e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00156: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.4920e-04 - mean_absolute_error: 0.0151 - val_loss: 8.1946e-05 - val_mean_absolute_error: 0.0101\n",
      "Epoch 157/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.3394e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00157: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.3385e-04 - mean_absolute_error: 0.0147 - val_loss: 6.0637e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 158/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.2992e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00158: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.2982e-04 - mean_absolute_error: 0.0146 - val_loss: 1.5714e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 159/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.2938e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00159: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.2969e-04 - mean_absolute_error: 0.0147 - val_loss: 8.1779e-05 - val_mean_absolute_error: 0.0094\n",
      "Epoch 160/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.5534e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00160: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.5421e-04 - mean_absolute_error: 0.0158 - val_loss: 1.4592e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 161/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.4677e-04 - mean_absolute_error: 0.0152\n",
      "Epoch 00161: val_loss did not improve from 0.00006\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.4677e-04 - mean_absolute_error: 0.0152 - val_loss: 8.6360e-05 - val_mean_absolute_error: 0.0092\n",
      "Epoch 162/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.3878e-04 - mean_absolute_error: 0.0147\n",
      "Epoch 00162: val_loss improved from 0.00006 to 0.00005, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.3878e-04 - mean_absolute_error: 0.0147 - val_loss: 5.3804e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 163/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.4293e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 00163: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.4283e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4451e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 164/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.5544e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 00164: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.5539e-04 - mean_absolute_error: 0.0158 - val_loss: 6.0834e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 165/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.1558e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00165: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.1462e-04 - mean_absolute_error: 0.0141 - val_loss: 5.2240e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 166/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.5386e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 00166: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.5251e-04 - mean_absolute_error: 0.0154 - val_loss: 6.5461e-05 - val_mean_absolute_error: 0.0093\n",
      "Epoch 167/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.1331e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00167: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.1321e-04 - mean_absolute_error: 0.0140 - val_loss: 7.7491e-05 - val_mean_absolute_error: 0.0095\n",
      "Epoch 168/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.1465e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00168: val_loss improved from 0.00005 to 0.00005, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 2.1459e-04 - mean_absolute_error: 0.0140 - val_loss: 4.8278e-05 - val_mean_absolute_error: 0.0067\n",
      "Epoch 169/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.1252e-04 - mean_absolute_error: 0.0142\n",
      "Epoch 00169: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 2.1254e-04 - mean_absolute_error: 0.0142 - val_loss: 5.2777e-05 - val_mean_absolute_error: 0.0074\n",
      "Epoch 170/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.1933e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00170: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.1928e-04 - mean_absolute_error: 0.0141 - val_loss: 5.5806e-05 - val_mean_absolute_error: 0.0071\n",
      "Epoch 171/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.2696e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00171: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.2696e-04 - mean_absolute_error: 0.0144 - val_loss: 8.0482e-05 - val_mean_absolute_error: 0.0092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.0900e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00172: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.0900e-04 - mean_absolute_error: 0.0138 - val_loss: 5.8922e-04 - val_mean_absolute_error: 0.0232\n",
      "Epoch 173/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 4.4540e-04 - mean_absolute_error: 0.0194\n",
      "Epoch 00173: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 4.4540e-04 - mean_absolute_error: 0.0194 - val_loss: 5.3297e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 174/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 3.2872e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 00174: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 3.2687e-04 - mean_absolute_error: 0.0164 - val_loss: 5.9174e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 175/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.8533e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 00175: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.8530e-04 - mean_absolute_error: 0.0156 - val_loss: 8.6259e-05 - val_mean_absolute_error: 0.0078\n",
      "Epoch 176/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.9635e-04 - mean_absolute_error: 0.0160\n",
      "Epoch 00176: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.9635e-04 - mean_absolute_error: 0.0160 - val_loss: 6.0536e-05 - val_mean_absolute_error: 0.0076\n",
      "Epoch 177/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.5923e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00177: val_loss did not improve from 0.00005\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.5913e-04 - mean_absolute_error: 0.0149 - val_loss: 6.7621e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 178/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.5482e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00178: val_loss improved from 0.00005 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.5482e-04 - mean_absolute_error: 0.0151 - val_loss: 4.3173e-05 - val_mean_absolute_error: 0.0064\n",
      "Epoch 179/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.5209e-04 - mean_absolute_error: 0.0150\n",
      "Epoch 00179: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 2.5209e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3242e-04 - val_mean_absolute_error: 0.0102\n",
      "Epoch 180/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.5933e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 00180: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 2.5933e-04 - mean_absolute_error: 0.0151 - val_loss: 7.5302e-05 - val_mean_absolute_error: 0.0077\n",
      "Epoch 181/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.3298e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00181: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.3298e-04 - mean_absolute_error: 0.0146 - val_loss: 5.3027e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 182/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.0610e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00182: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.0527e-04 - mean_absolute_error: 0.0140 - val_loss: 5.4735e-05 - val_mean_absolute_error: 0.0080\n",
      "Epoch 183/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.3681e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00183: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.3480e-04 - mean_absolute_error: 0.0145 - val_loss: 5.0369e-05 - val_mean_absolute_error: 0.0070\n",
      "Epoch 184/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.1299e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00184: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.1291e-04 - mean_absolute_error: 0.0140 - val_loss: 5.3484e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 185/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.1282e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00185: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 36ms/step - loss: 2.1282e-04 - mean_absolute_error: 0.0145 - val_loss: 9.7771e-05 - val_mean_absolute_error: 0.0104\n",
      "Epoch 186/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 1.8518e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00186: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 1.8518e-04 - mean_absolute_error: 0.0134 - val_loss: 1.1426e-04 - val_mean_absolute_error: 0.0098\n",
      "Epoch 187/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.6600e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 00187: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 37ms/step - loss: 2.6600e-04 - mean_absolute_error: 0.0149 - val_loss: 4.8366e-05 - val_mean_absolute_error: 0.0069\n",
      "Epoch 188/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.1489e-04 - mean_absolute_error: 0.0138\n",
      "Epoch 00188: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.1481e-04 - mean_absolute_error: 0.0138 - val_loss: 7.2229e-05 - val_mean_absolute_error: 0.0097\n",
      "Epoch 189/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.1041e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00189: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.1041e-04 - mean_absolute_error: 0.0141 - val_loss: 6.8929e-05 - val_mean_absolute_error: 0.0075\n",
      "Epoch 190/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.4474e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00190: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.4461e-04 - mean_absolute_error: 0.0143 - val_loss: 4.9816e-05 - val_mean_absolute_error: 0.0072\n",
      "Epoch 191/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.3377e-04 - mean_absolute_error: 0.0140\n",
      "Epoch 00191: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.3376e-04 - mean_absolute_error: 0.0140 - val_loss: 9.1926e-05 - val_mean_absolute_error: 0.0113\n",
      "Epoch 192/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.3785e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00192: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.3785e-04 - mean_absolute_error: 0.0145 - val_loss: 7.8028e-05 - val_mean_absolute_error: 0.0086\n",
      "Epoch 193/200\n",
      "77/79 [============================>.] - ETA: 0s - loss: 2.2053e-04 - mean_absolute_error: 0.0141\n",
      "Epoch 00193: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.2121e-04 - mean_absolute_error: 0.0141 - val_loss: 6.6134e-05 - val_mean_absolute_error: 0.0073\n",
      "Epoch 194/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.2812e-04 - mean_absolute_error: 0.0143\n",
      "Epoch 00194: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.2812e-04 - mean_absolute_error: 0.0143 - val_loss: 6.6502e-05 - val_mean_absolute_error: 0.0081\n",
      "Epoch 195/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.0043e-04 - mean_absolute_error: 0.0136\n",
      "Epoch 00195: val_loss improved from 0.00004 to 0.00004, saving model to results/2020-12-10_QQQ-huber_loss-adam-LSTM-seq-500-step-100-layers-3-units-256.h5\n",
      "79/79 [==============================] - 3s 35ms/step - loss: 2.0034e-04 - mean_absolute_error: 0.0136 - val_loss: 4.2130e-05 - val_mean_absolute_error: 0.0066\n",
      "Epoch 196/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.0171e-04 - mean_absolute_error: 0.0134\n",
      "Epoch 00196: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.0181e-04 - mean_absolute_error: 0.0134 - val_loss: 7.3676e-05 - val_mean_absolute_error: 0.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.3466e-04 - mean_absolute_error: 0.0145\n",
      "Epoch 00197: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.3466e-04 - mean_absolute_error: 0.0145 - val_loss: 8.1015e-05 - val_mean_absolute_error: 0.0084\n",
      "Epoch 198/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.3925e-04 - mean_absolute_error: 0.0146\n",
      "Epoch 00198: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 2.3914e-04 - mean_absolute_error: 0.0146 - val_loss: 7.6122e-05 - val_mean_absolute_error: 0.0090\n",
      "Epoch 199/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 2.3185e-04 - mean_absolute_error: 0.0144\n",
      "Epoch 00199: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 34ms/step - loss: 2.3185e-04 - mean_absolute_error: 0.0144 - val_loss: 4.8400e-05 - val_mean_absolute_error: 0.0068\n",
      "Epoch 200/200\n",
      "78/79 [============================>.] - ETA: 0s - loss: 2.0690e-04 - mean_absolute_error: 0.0139\n",
      "Epoch 00200: val_loss did not improve from 0.00004\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 2.0685e-04 - mean_absolute_error: 0.0139 - val_loss: 7.3955e-05 - val_mean_absolute_error: 0.0102\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 100 days is 287.05$\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: Accuracy Score: 0.9863013698630136\n"
     ]
    }
   ],
   "source": [
    "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    money = 1\n",
    "    moneylist = []\n",
    "    long_only = 1\n",
    "    long_only_list = []\n",
    "    for i in range(1,len(y_test)):\n",
    "        if i % 100 == 0:\n",
    "            if y_pred[i] >= y_pred[i-100]:\n",
    "                long_short = 1\n",
    "            else:\n",
    "                long_short = -1\n",
    "            money =  money * (1 + (y_test[i] - y_test[i-100])/y_test[i-100] * long_short) \n",
    "        long_only = long_only * (1 + (y_test[i] - y_test[i-1])/y_test[i-1] * 1) \n",
    "        moneylist.append(money)\n",
    "        long_only_list.append(long_only)\n",
    "    plt.plot(moneylist)\n",
    "    plt.plot(long_only_list)\n",
    "    return money,long_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.4269236313316087, 2.476917979651391)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAArjUlEQVR4nO3dd5xU1d3H8c/ZTq9LkSqCDVTQDYKaqFEMIrbYe0t44oNGjUleGhONJYk1xvaoPOqjxt6DWBDFiEZBFwUUAUHpICydBbbMzHn+OHec2d3Z3dndmZ2ZO9/36zWv287ce+4O/ObMuacYay0iIuJPOanOgIiIJI+CvIiIjynIi4j4mIK8iIiPKciLiPhYXqou3L17dztw4MBUXV5EJCPNnj17g7W2ON70jQZ5Y0wRMAMo9NK/ZK29oVaaC4E7gNXervuttY80dN6BAwdSWloabz5FRAQwxixvSvp4SvKVwE+tteXGmHzgI2PMW9bambXSPW+tvawpFxcRkeRqNMhb11uq3NvM917qQSUikgHievBqjMk1xswB1gPTrLWzYiQ7xRgzzxjzkjGmXz3nmWCMKTXGlJaVlTU/1yIiEpe4gry1NmitHQ70BUYaY4bVSvI6MNBauz8wDXiinvNMstaWWGtLiovjfm4gIiLN1KQmlNbaLcD7wNha+zdaayu9zUeAgxKSOxERaZFGg7wxptgY09lbbwOMARbWStM7avMEYEEC8ygiIs0UT+ua3sATxphc3JfCC9baKcaYm4BSa+1k4NfGmBOAALAJuDBZGRYRkfiZVA01XFJSYtVOXkTSzcbySp6ZtYLqYChp1ygZ2JWf7Nm855LGmNnW2pJ406esx6uISDqaOn8dd037BgBjknONXx2+R7ODfFMpyIuIRAmGXAn+s+uOprhDYYpz03IaoExEJIrfenoqyIuIRAk/pkxWVU1rU5AXEYnBJzFeQV5EJFq4xaHxSVFeQV5EJEq4Tt4fIV5BXkSkhhR1HUoaBXkRkSg/lOR9UpRXkBcRicH4pMJGQV5EJMoPQ734I8YryIuIxKLqGhERH/JZQV5BXkQkmvXZwAYK8iIiUSLDGvijLK8gLyISgz9CvIK8iEgNaicvIuJjkQev/ojyCvIiIlHCD15VkhcR8SGNXSMiIhlDQV5EJAZV14iI+NAPk4bowauIiP9ojlcRER/TzFAiIj6m1jUiIllAY9eIiPjQD52hUpyPRFGQFxGJogevIiI+FhmgzB9RXkFeRCSaz568KsiLiPhYo0HeGFNkjPnUGDPXGDPfGHNjjDSFxpjnjTFLjDGzjDEDk5JbEZEks/inPh7iK8lXAj+11h4ADAfGGmNG1UpzCbDZWjsYuBu4LaG5FBFpJdb6p2UNxBHkrVPubeZ7r9qVVicCT3jrLwFHGb88tRCRrGKxvnnoCnHWyRtjco0xc4D1wDRr7axaSfoAKwGstQFgK9AtxnkmGGNKjTGlZWVlLcq4iEgyZF1JHsBaG7TWDgf6AiONMcOaczFr7SRrbYm1tqS4uLg5pxARSSp/ta1pYusaa+0W4H1gbK1Dq4F+AMaYPKATsDEB+RMRaXU+qq2Jq3VNsTGms7feBhgDLKyVbDJwgbd+KjDdWp81NhWRrOCqa/wT5fPiSNMbeMIYk4v7UnjBWjvFGHMTUGqtnQw8CvzTGLME2AScmbQci4gkkcVflfKNBnlr7TxgRIz910etVwCnJTZrIiIp4K8Yrx6vIiLRsrEzlIiIZCgFeRGRKNZaXz14VZAXEYliraprRER8y6IHryIivuVK8v4J8wryIiJRLFYleRERyQwK8iIiUazPKuUV5EVEavFRjFeQFxGJZm0WThoiIpItNKyBiIjP+SjGK8iLiETz20wYCvIiIlGyciJvEZFskZUTeYuIZAs9eBUR8TFXJ++fKK8gLyJSi0ryIiK+5a/mNQryIiJR9OBVRMTHNDOUiIiPufHk/RPlFeRFRGpRSV5ExKc0rIGIiI/5bM4QBXkRkWiayFtExMes2smLiPiYmlCKiPibgryIiE/5q7ImjiBvjOlnjHnfGPO1MWa+MeaKGGmOMMZsNcbM8V7XJye7IiLJZW0SO0NVlsOWlck5dz3y4kgTAK621n5ujOkAzDbGTLPWfl0r3YfW2vGJz6KISOtJ6njy9w6HHWXw561JukBdjZbkrbVrrbWfe+vbgQVAn2RnTEQkFZI2QFnldhfgwxdpJU2qkzfGDARGALNiHB5tjJlrjHnLGDO0nvdPMMaUGmNKy8rKmp5bEZFWkJR28l9PjqxX7Uj8+esRd5A3xrQHXgautNZuq3X4c2CAtfYA4D7gtVjnsNZOstaWWGtLiouLm5llEZHk+aHHa9ki+PyfiTnp9nUw5crI9tIPEnPeOMQV5I0x+bgA/7S19pXax62126y15d76m0C+MaZ7QnMqItIKbLgq5YGRMPmyxJy0bAEEq2DYqW77i6cSc944NPrg1bjfLY8CC6y1f68nTS9gnbXWGmNG4r48NiY0pyIiraDOFK+hIOTktuykm5e5ZcnFsH0tHHt7y87XBPG0rjkUOA/40hgzx9v3B6A/gLX2IeBU4FJjTADYBZxprd/GchORrFD7wWugEgratuyc81+Ftt2h30i46M2WnauJGg3y1tqPaORhs7X2fuD+RGVKRCSVcqK7RAUqWhbkrYXVn8P+p0Nufssz10Tq8SoiEsVi6egeMTqByvoT79oMweq6+8vLIs0kNy+Fym3QM2ajw6RTkBcRiWIt9GJ9ZEegInbCim1w+yB4+rSa++c8C3cOhm/fg43fwvJP3P7+hyQnw42Ip05eRCRrWAvnVDwX2VFfSX5HGdgQfPd+zdm/V3hB/ZMH4NvpkfTdBicnw41QSV5EJEqHwAYOCXwa2VFfSb5ye2Q93HoGwHhhNTrAA+SmpkytIC8iEqVDYLNbKerslsGq2Amjg/zKWbH3hw06IhFZaxYFeRGRKB2C3uBhoye6ZX0l+aqoh7PfRfVgrag1+Niln8DZLyYug02kOnkRkSjtQl6Q7tzfLStqj+LiCZfY2/WArVHDB0cH+XNfgZ77Jj6TTaCSvIhIlK7VXsuaPge55dZVsROGg3nXQbDsQ3jnT1C9C6p3RtL0PiB5GY2TSvIiIlEGV37Fqpw+9O02GPLb1iylR9u2BnLyoONubvvje6HX/jWDfLheP4VUkhcRCQuFGFLxFfPzh7omkW26wK4tNdOsmQN/7gTLP4YOu8GYG+Ggi9yxV34B1RXQbQj8+OqUtaiJpiAvIhJWtpB2oe18nefVo+cVQWBXzTSlj7rlypnQZYCrux97a+R49U7Y40g4Kj1mQVWQFxEJWzkTgG/y93bbeUVQtROeOTPSgiZ6wo9Bh7tlflFkX/UuyG/TCpmNj4K8iAhAKARTrgJgXW4vt2/9fPjmLfd67mzX+/WrlyPv6Twgsn7oFa6OPljp6vLThIK8iAjAtkgrmoCJMVqkDdVsDw+upB+WkwehgFtXSV5EJE0sfAMePAzWzQfgnfYnxR5b3VqorjU3a7c9IuuVUZ2j8tInyKf+0a+ISC3WWjbuqGc4gQTr+soEcqrKCU65mlzgxfbnQDBGmA/sikzbt+9JMOSYmsMHRw9n0K5bMrPcJAryIpJ2/vLGAh75aGmrXOuLQuhiIHf7aiptPtOWVTNyYD2VHEvedctTHqk7AUh0FU2H3snJbDMoyItI2lm5eSc9OxZy2ZHJHZ63TcV6unwQqWb5+MA7uLnHUA4a0LXhN8aa4enoP0eaV7bvmbhMtpCCvIiknapAiB4dijhv9MDkXmjW1BqbRx5yKBRHXfOit2DxNFcyf+t3DZ+rqCOMOA8WvwNdBjacthUpyItI2qkKhijIS1K7kNWzYcl0OPx3sGWF1+HJG2myba0S/IBD3Kvsm6idDUx5fcJ9rhVOTm7Cs91cCvIiknaqAiEKcpMU5J88yc25uucxsG01dOwDm751x+oba6brIOi+J2z4BnIL6j+3MWDSJ8CDmlCKSBqqCoTIT3RJfs0X8OxZkUlAHh8PW1dDpz5wxlOw74n1jzWTmwenPubW+x+c2HwlmUryIpJ2KpNRkn/8eKja7kru21ZDz2Fu2r49joR9jnevhvQcBuPuhP1OazhdmlGQF5G0UxUMUZjoknyV145922q33PQtlK9zQT8exsDIXyY2T61A1TUiknaqAgl+8BqsrruvfJ1b9h+VuOukIQV5EUk7CX/w+tBh9R9TkBcRaV0Jb0JZttAtx98N12+CfU6IHCtol7jrpCHVyYtITC98tpIvVm5JybV3VAYSG+S7DHQPWQ+8EHJyYK9xsGAyjL0tcddIUwryIhLT7VMXUl4ZoENRjC78Sda1XQEH9u+SuBOGgnDAWS7AAww/yzWF7DoocddIUwryIhJTZSDEWSP7c8PxQxtPnM5CQdixAdrU6s2aBQEeVCcvIvVIaq/T1rR2rhsmeLcRqc5JSqgkLyIxVSdz/JjWULUDJl8emaKv349Sm58UaTTIG2P6AU8CPQELTLLW3lMrjQHuAcYBO4ELrbWfJz67ItIaAsEQIQv5mVySn/9aZD7WvDbQqX9Ks5Mq8XyCAeBqa+2+wChgojFm31ppjgWGeK8JwIMJzaWItKrqoAXIzJL8tOth+cewbU1kX+d+kYeuWabRkry1di2w1lvfboxZAPQBvo5KdiLwpLXWAjONMZ2NMb2994pIhqkKhoAMLMnv2gL/uQdmPQzte0T2t0lgS50M06RP0BgzEBgBzKp1qA+wMmp7lbev9vsnGGNKjTGlZWVlTcyqiLSWqoAL8gW5DYydno7KFrlloMKNFR+W0/rNQNNF3EHeGNMeeBm40lq7rTkXs9ZOstaWWGtLiouLm3MKEWkF1V5JPuOqa1bOjKwX7w0neTXHhe1Tk580ENcnaIzJxwX4p621r8RIshroF7Xd19snIhmoOlOra777ILLea38o6uTW+2ZnyxqII8h7LWceBRZYa/9eT7LJwPnGGQVsVX28SOb6obom00rya6Ia9XXq44YvOP1JOOyq1OUpxeJpJ38ocB7wpTFmjrfvD0B/AGvtQ8CbuOaTS3BNKC9KeE5FstC2imp2VgZb/bprt7o5TxNekg+FYP186LVfYs8LbmyaXZsj2112d2PA73ti4q+VQeJpXfMRDc5cC16rmomJypSIwIbySg752/QfWrqkQruCBPeX/OQ+18Txl9Ohz0GJPfd9tc5XvHdiz5+h1ONVJE1t2lFFVdCNH7N/306tfv22BbkcPKhr4wnjVfaNC/AAa+YkNsjv2gKhQM19vYYl7vwZTEFeJE0FvA5Jh+9ZzNhhvVKcmwR494bI+uZliT3350+65QVTYOsqWP+178eJj5eCvEiaCoZckM/LybC26rFsXg6L3oT+o11P1O3fJ+a8a+fCc+dC+2Jo2x12/3FizusjGfboXCR7BEKuLj430zokxfLE8W45+jLo0Bu2J6jx3X/uha0rYPXsrG4L3xAFeZE05ZuSfHUFbFnu1occA+26w85NiTn3jqie82NvTcw5fUbVNSJpKuAF+dxMD/LhkSDH3Ax5BVDUGSq2tOycs5+AL/4Jqz5z239Yozr4eijIi6SpSEk+Q39wByoht8BNop1bCIdc7vYXdYKKrS079+u/jqwP/LECfAMy9F+PiP9ldEl+2UdwSw/46G4oXw/te7qOSeCCfFU5BAMNnyNeo/47MefxKZXkRdJU0HvwmnF18pXb4Zkz3fp7N7plr/0jx8PjyVRug7bNaIdftbPmdvc9m36OLKKSvEiaCreTz6iSfKDS9Tyt2k6NjvLVUYG5TWe3jB6CoCnKFtbc7j64eefJEirJi6SpcHVNXiY1oZz9OJSvc+v7HA8LJrt1kxtJEy7JN7Ve3lqYeh2sndPSXGYVleRF0lQgE5tQvvV7tzzreSjsENkfPQpkOMhv+AbuHQEbv43v3BVbYeYDsPw/0K4HnPY4XPpxQrLtZyrJi6SpSJ18BpTFKsthxu1uvf9o2Gss7DYCCjvCmJtc08mwcJB/9b/cctZDMO6O+s9dXQE7N9T8Mhh8NAw9ObH34FMK8iJpKqPq5Gfc7uZWBTjeW3boCcfG6KDUptbD1mB1w+d+8UL45q2a+3rvHzOp1JUBRQSR7BTMlDr5WQ9HAnyXgVC8V8PpO/auuR1qJMjXDvAA3fSwNV4K8iJpKiPaye/YGKmHB5j4aXzv2+f4yHqogUlRvnwp9v5+B8d3HVGQF0lXGdHj9d0bwOTAHkfBL96DvML43pffNrJevbP+dG9fW3O78wC4bh0UdWx6XrNUGv/rEcluaV+Sn/eCGz/moAvhvFegb0n8741uUtnQsMO1695/PQfyi5qSy6ynB68iDdhVFeS9heuoTsEUfHNXbgHStAmltW6Wp7bd4Se/bzx93RNEVresrD9Zh15Q0N4NgwCQzr9q0pSCvEgDpsxbw+9empey63cozKMgL00C28ZvYVUpHHAGLJ3hxoQff3fdB6nxMN49FbSHqh31p6vcDp361u3lKnFTkBdpwI5KN4jWvyYeSqc2+a1+/S5tC8jPTZMg/9QpsHkprJwFpY+6IYMPOLt55xp6sjtP1z1g8VTYtBS67l43XeV290UAMORnzc56NlOQF2lAuF589+J2dCxq/SCfFmY/Dp894gI8uAAPsO+Jza8fHzLGvd7/qwvyDx0Gf1hdN92uzdCmC/xxPeQoXDWH/moiDajy6uIL0qU03ZrK18Mrv4Tv/l332IDD4JibW36NcLVNuM492s5NsOYLN2VgvK12pA4FeZEGhHudpuXDz2Sa94IL8AD57WDI0W4CkOK9Yd7zcP5rkJuAXzYNtZGf94JbRreplyZTkBdpQCAYwpg0bsaYDNW7IuPKjJoIP/tLZMIPgJ/8NnHXsg20Wtq60rWnV8enFlGQF2lAVdCSn5ODMVkS5JfOgCe8kvPZL8CeSX7YaespyVsLn9wPXXav+QUjTZaFFY0i8QsEQ+k9dkxluXslypu/i6zv8dPEnbc+oXqmAAzX0atna4upJC/SgEDIpk8TxrDy9bBxCezYAC+c5/Zdt65lPUFXlbqHm+HZmn6zIDF17o0J1VNdU13hliPOS34efE5BXqQBVcEQ+elUkq/YBncOqbv/yxfgwPPrf9/KT2H6za5+e69joc9BkWOrZ8MjR0W2j7sLOu6WuDw35KAL3EQgAGvmuNEl79obir15W/M0hEFLpVkRRSS9BIKhxA8QFgq6kmqgqmnv+2aqmz81ljVz6n/fSxfDo2NcffuMO+CxY12dd9j7f6uZfvCYpuWrJYr3gqE/d+tPnABzn3Xzw66e7fblt2m9vPiUgrxIA6qDlvy8BJXky9fDbQPhpq7wl57w2DHxvzcYgGdOhx3r3fbRf3bLi6fCoCNg8Tuxqz52boKvXnbrhV79drASbuzshgnesQGWfeR6r467Ey4rhS4DmnV7zZbjDVaWkxNpNx+mIN9ijVbXGGMeA8YD6621w2IcPwL4F+B1h+MVa+1NCcyjSMpUB0PkJ6okP/e5SJ03uI4+wQDkNvDfcM0c1168XbfIvtOegKEnQckl7sHk0JPh9Stg5v/A5mXQtht8cCtc+GZkzJfDrnIDiVWVR6p77hgUOefPH3Y9WFMh/KvC5NZ9DqDqmhaLp07+ceB+4MkG0nxorR2fkByJpJFAMEEPXndtga+8CTDG3Qlvem3Nt61uuOT8we2w6A23nlsA166OzJcabnnSY6hbvnNdzfc+Pi6yPuI8KGjrXvscDwter5l2wKFNvqXECQf5nLrt5lWSb7FGg7y1doYxZmAr5EUkpqUL57Bm1kss6HUiFfmdaxzrtmMJucEK1ncYmpT21IvXb6coP7fxhPUJBeHLF+GN37rJMU5+GPY/Ayq3wXs3wZJ34aCLXN5r53/bGvjm7cj24KNrTogdFs9D0s5RXyRnPAXv/Ak+vjeyr133pt1XIv0Q2K1rNRQtV8MZtFSiWteMNsbMBdYAv7XWzk/QeUWw/5rIobu+YsuSWUysvhKwdGAXR+fM5rKCBwG4sOp3/Ds0IinXP2l4M1uaVFe4B57fe0MVH38PHHCmWx99Gcx8CN74jXsdfSMcdmXkvdbCik9cZ6G+I2HVp3DS/8S+TtuusfeHjb21bpXQYVdFgvwFr9d9T2sKV9eEgvDxfTWP5bTgC1aAxAT5z4EB1tpyY8w44DUgRhsvMMZMACYA9O/fPwGXFt/bsIRBu74CYFzRfL49ZjE5n9yL2bmxRrJHjykg9ONjk5KFJo1bEwzA7P9z1SMz7ogE+DOehn2iajTzCuGo62HyZW773RsiQT4YcPXmuza57XNfcvXVhe1jX7OhKo2TJ7nx32tr29Xlqd/B0L44/vtLCi/Ih+83rGNf2G14q+fGb1pc2Wit3WatLffW3wTyjTExf/tZaydZa0ustSXFxan+hyVpY9Hb8L9HQbDaaw3yCgQq3bGvXwXgiaJzMdU7yH3vhpoBfsIH0L4nuVuWk5+bk5RXk4Y0mPuMq2//8C5XKu37I7h+U80AHzbgkJrbn3lD+K77KhLw9jkBijrVH+Br+8V0Nx3f2FvddkMPU/cZnwYBnprNOQFGnAt/2ghXfpma/PhMi0vyxphewDprrTXGjMR9cWxs5G0i7j/3xm/hxQshsAvKFrlu9Ss+dsePuwum38Kq/IFMLxrDBRVP1Xz/tatd8OuxD6yd2+rZj2njt2457znXVPGw39Rf5VC7muWN38CPLomU/i/9GHoOje+6w89xMyj1Pci9AEZd2vT8p0Lth61FnRtucSRNEk8TymeBI4DuxphVwA1APoC19iHgVOBSY0wA2AWcaW3tr2aRWrZ/D3ftVXPfO3+MBHiAN64GYFr7E9iaV+ymmsstdK1KuuweKd32Hw3//hu8++dI+/FU2PgtfP6EW9+ywi37/qj+9IWd6u7bsgI2fQc5+dB9r7rH61NffX0mqN1MUs0mEyqe1jVnNXL8flwTS5HY1i90rULa94SCdq4EP+OOyPHwRM3fve+C93/PhJcvcW3KD7uKdz/oQF51CEoujn3+gy50QX7RW25i6aEnQ6c+rXJrPwgG4L4D3fqBF7hgX3JJw9UhOTlw4gPQpwR2boDHj4N/7Ae9D3BT4WVLafbY2yLNS6FlY/BIHVnyr0hSorLcdU9/8gS3vddxcNYzsPpzN50cuGndAO4rga0rXHvt/CI48+kfTlM9/ZOGx3Pv0At6D4e1c1xb8S3LYZz3JRKogmUzYI+jkjdkbSgIC6e49UFHwgn3upY08VxvxLmR9UFHuFmY1s6FQ69MQkbTVO3mm3lqG59ICvKSHJu+g3trNWlc9Ibr2r/8P257xHmRad3Cy9H/XedUwZClKL+RNgJFUVUfSz90D3Fz812J+s3fuvbp4eaLibJ6Nrw2EcoWuO2OfeFcbwiB5nyhnPa4G/YAYK9xDaX0N5XkE0pj10hN793sut+31Dt/iqyPuQnO8X6Or/zUtc/usjucGFXLd/qTcOztMR80BkK28UHCCqJan5QtcN38rXWdjgCWfVgz/eZlkRY8TVW10z0g/t+fRgI8QJ8DW9auu02XyHrnLG5irJJ8QqkkLxGbl8GHd7r1/U5rfsAKVMLiabD3eDj8966OeZM3tNHz57hlz1rDIPXc171iCIZCjbdVD9YK2HOedqX5L715QretjRyr3A73HOB+SZzYhMdJH97lfjF88ZQbdwbcl1PZN/D+La7aKFHapUHTxlTpHrObjTSTgny2qNgGb/0ejvxD3VLi6tmuZL1+YWTfC+fXqBdvVOV2V5q21v0SCFbCsFNcgAfXvC9arzpj3dUrELSNz7Ga39YtD7/GdUYqXxcJ8ABbV0XWNyx2y6Uz4s4DOze5YQjCjrrezZy02wio2gGVWxMzwcWeY91QBtny0DVs6MluRMzzJ7sH0pIwWfYvKUtVbINb+7n1uc/C75dG2mgHqly1Q499XckbYMBhLtA0NkJi2Ku/cudt3wt2boRQtdsfPTFFbr57GPn6FXDsHQ1PcFFLIGQbn4LvkMthwWTY/3SY/Xjd4xsWwdzn3fH/PdLta8rEGA/9uOb2j6+OrBe0g2Nuif9cDTnjaQg2cZx5Pzjt8VTnwLcU5LPBojdrbs97PtJRZpPXeWf917DhGzcI1t7jYflHUPoYHDyh4XMHq2H+a269/PvI/nF31h1d8cALXDVQQbsmZT8YsuQ2VrrrNxL+vLXu/sFjXHPETyfBqxPcKIxhKz5xLWMaq5YKBWGb90tg9GUwJImTauTmZV8pXpJKv4v8rrI80l3+9CchJw/Wej0qN34LS96LpA0FXNPDPl5772l/qnkua+GZM13JPWzaDa636ojzoNsQ1w3/+k0w8pd182JMkwM8QCAUIr8p48eEf0mAm14uehjdFTNrpn3zt65JZyzLP4G/7wt/86qaxt8NP/uLa+ookiEU5P3u0TFuBMMj/+jGMek3KlJ6v+/AumOQdx3k6tFHTYRARWRWIYDta+Gbt1zVzK4tLuiHO7GMvRUuL4Uz/pnwkQOD8dTJRzv7xch68d7uvg/w+vR9742H0mt/tyx9zLWUieWVCW689+qdkXOJZBgFeb+yFmY+6KphAA73Alnn/q4Ne/WuSNrRl7mgd/LDkX3hqpaXonqZLo8acmBVqXsQWb7OBfh4B9Bqhrjq5KP1PQiGe52Mug5yvyBOfgja9YClH7j9h10VSV9rREtCIZg1yXXOitaxlXvRiiSAKv/8auaDMPVat/7rOZH9hR1gRxk877UEOeVR2O/Uuu+Pbg0TDMCSaW6ogbCnT4msd0tukzdXJ9/EzkXH/8NVrUT/qujQ082R2n2vmiNAlq93X4rhDkwf3+PGwQH3y6dNZzd4WlMe1IqkCQV5P6ra4Ya5bVfsZgHqunvkWKDCLZdMc8viegbBii613hw1v2jHvq49++J33Pbgo2H3nyQu7zFUB0ONd4aqLTffBedo4UHDNixybdoveB1Wfeb9IlnvvgTAtYMPu/jt5A2HINIKFOT9YvNyN0jXz/7qmj9uXwOn/h/0H1UzXbh+GSC/Xf2l8HqqJt4Z8zZ7f3EL/YEFw69jxZDzYdGmmGkTpTIQanpJPpaTJ8GzZ7i/C7gvpx0b3PrTp8CvPnIPozcucc8khoxRgJeMpyCfqQKVrvliuC78P/9wD0TnPuu2Ta7rYFLbvie5OUfBTehc3zgh7bq7AcXCk0gD80K7M+HpeexthnNB7jr+OnN3ts+cnbBbakjXdjHmNm2qvcbWbWbZ72C33ODNLRqe4HrkL1x9vkiGU5DPRJ8/CZMvd6XwiZ/C9JtdKxFwPVc3L3Vzg8Yqhe4z3k0wMefphnudGuN6vN7YGYAqm8uH+9zAG4cf5iU4mwSMcBOXHGMY0iNJD3Y79XHNPssWwsrP3DR84P6OIj6gIJ8JKsth2vVs3LSBorJ5tNvujQOzcTE7/j6CduXLAPjix4+wuXgkfb57gR2d9mDT1+tinm749yvoDszd0ZWyetKEtTn2HTosfYsT5pRw6x77M3S3GBNdZLpug11v2UePjuxTNY34hIJ8utu1GR7+CWxZQdTjT+4LnMTlea/RrnwZK0PFHFf1V7ZNawt8BYQH+iqNecpzcwdzS/6HTHgvxLp60tTkZjca2L3pHZkywm4jGk8jkqEU5NPZzk0w6XDYsoJNe5/Fn+YVc1OXN9k05h6O6L4f360+nR5f3EPFqBt4unv8A35hD+Wr4DU80oQhXdsU5LBHcfLawqeU6t7FxxTk09Gmpa66YPXnrtnfwb9ixR6X8sac+Zx2wuUcsVcPl67vcXDwcWhg1haKnirwxAdqjusukuEU5NPJkndh9hOufjjaEdewY3UQgDb5iR0yQICizpH14eeoPl58RUG+Ne3c5EY0zG/jxjLvPsRVFeTkujFiwkMIdN/TjQgZVtSZimo3F2qRgnziRQd1BXjxGQX5RKksd22sBx/t2pjvKIP2PWqmeeZ018My2p7HQrc94JP7obAT/Ne/I833Vn/uBhMzhorqEABtChTkRSR+CvINCQZg42I3FEDv4fWX8pbOgCdPBBuqdcC4oQVOvB+67lE3wHce4EZ1BBfYL3yjZv1w34PcC9hV7aprivIU5JPihPthy/JU50Ik4RTkGzLzAZh2vVs/71U33Zu1bjsc8Levc9UshR2gsCNsXRl1AusGxHrm9MiuM59x3eZLLnFjq9//I/dFctFb0LF3vVmpCAf5fA0cmhQHJmDqPpE0pCAfi7VuHJgPbovsW/4xWxZ+QJvSh1heMIR7ev6FCtOGq9Zdw7CKMm7veSeL2gynXcetVJsCJpT9jRG7Pq5x2imdzubVmT2AHrB4AQDt296KbQs7XlkFrKI+Kza5MWeKVF0jIk2gIB9t62qYfgv0LXEBvs9BcNxdMOkImHEHnb1ke1Z+yR0rz+D6DjcxrGI2z7U5gxmBvWF7BVAIwP8UXsghZk+W5A5mv8CXvNDmdKpMoZcmYp2XHmrur60wP4dx+/WifYE+MhGJn7Hh6odWVlJSYktL4+ltmWTVu2DZf2DOUzD/1cj+Tv1h4kxXpTL1OvdgFFhlu9PXbIika9MFLit1D1tFRJLMGDPbWlsSb/rsruC1Fl7+hRtmdv6rMCxq8oyzn4/MR/qzv8A1K3hvrxv4SeU/CPU9OJLu0CsU4EUkbWVvkLcWnjgeFk5xozle9Bac+qhrt77f6W5ijGhFnZjbfTwhcjCXTHUPTjvsBiUXxz6/iEgayJ4K3optbvjdcJf1Je/Csg+h534w4d+Q6/0pLv+83lNUBoIU5uVgjIHxf3cvEZE05q8gX74eTI7rUfrm76ByG+w51o3k+M4fXZrivd2MPx/f5ybWuHBKJMBDgz0eK6tDFOZl748fEck8/gjyS951E2l8/S+3bXJdqR0iM/30P8TNgrR1lQvwAGNuqjsPaAMqAyEKNayAiGSQRoO8MeYxYDyw3lpbZzxbY4wB7gHGATuBC6219dd5JNrMB+Hta2ruG3IMDD7KTeb8+hUw9Odw/D1Q1BEqtrpS/f5nwsBDm3SpcHWNiEimiKck/zhwP/BkPcePBYZ4r4OBB71l8s1/Fab+AXoOg9Med2PFFEXNXGStC+bR85gWdYIT7mvW5SoDIQoU5EUkgzQa5K21M4wxAxtIciLwpHUN7mcaYzobY3pba9cmKpPR5qzYzIfT3+DgLW9QsuUtlrcZxqQud1I1fSewLBmX/MEXyzfTqW0CJpQWEWkliaiT7wNED9iyyttXJ8gbYyYAEwD69+/frIuZxVO5fNlEAF7JHcuDofPYuWwnrqYouYwxHL5ncdKvIyKSKK364NVaOwmYBK7Ha3POccCIkVD1Kxh8ND8fMoafJzSHIiL+koggvxroF7Xd19uXHF0HwbG3NZ5OREQS0uN1MnC+cUYBW5NVHy8iIk0TTxPKZ4EjgO7GmFXADUA+gLX2IeBNXPPJJbiK8YuSlVkREWmaeFrXnNXIcQtMTFiOREQkYdToW0TExxTkRUR8TEFeRMTHFORFRHxMQV5ExMdSNserMaYMWN7Mt3cHNjSayp+y9d6z9b4he+89W+8bGr73AdbauMdXSVmQbwljTGlTJrL1k2y992y9b8jee8/W+4bE3ruqa0REfExBXkTExzI1yE9KdQZSKFvvPVvvG7L33rP1viGB956RdfIiIhKfTC3Ji4hIHBTkRUR8LOOCvDFmrDFmkTFmiTHmmlTnJ5GMMf2MMe8bY742xsw3xlzh7e9qjJlmjFnsLbt4+40x5l7vbzHPGHNgau+gZYwxucaYL4wxU7zt3Y0xs7z7e94YU+DtL/S2l3jHB6Y04y3kzYv8kjFmoTFmgTFmdBZ95ld5/9a/MsY8a4wp8uPnbox5zBiz3hjzVdS+Jn/GxpgLvPSLjTEXxHPtjAryxphc4AHgWGBf4CxjzL6pzVVCBYCrrbX7AqOAid79XQO8Z60dArznbYP7OwzxXhOAB1s/ywl1BbAgavs24G5r7WBgM3CJt/8SYLO3/24vXSa7B3jbWrs3cADub+D7z9wY0wf4NVBirR0G5AJn4s/P/XFgbK19TfqMjTFdcfN5HAyMBG4IfzE0yFqbMS9gNDA1avta4NpU5yuJ9/svYAywCOjt7esNLPLWHwbOikr/Q7pMe+GmjXwP+CkwBTC4Hn95tT97YCow2lvP89KZVN9DM++7E7C0dv6z5DPvA6wEunqf4xTgZ3793IGBwFfN/YyBs4CHo/bXSFffK6NK8kT+UYSt8vb5jvdTdAQwC+hpI1Mqfg/09Nb99Pf4B/B7IORtdwO2WGsD3nb0vf1w397xrV76TLQ7UAb8n1dV9Ygxph1Z8Jlba1cDdwIrgLW4z3E22fG5Q9M/42Z99pkW5LOCMaY98DJwpbV2W/Qx677CfdXu1RgzHlhvrZ2d6rykQB5wIPCgtXYEsIPIz3bAn585gFfVcCLui243oB11qzSyQjI/40wL8quBflHbfb19vmGMyccF+Ketta94u9cZY3p7x3sD6739fvl7HAqcYIxZBjyHq7K5B+hsjAlPURl9bz/ct3e8E7CxNTOcQKuAVdbaWd72S7ig7/fPHOBoYKm1tsxaWw28gvu3kA2fOzT9M27WZ59pQf4zYIj39L0A95BmcorzlDDGGAM8Ciyw1v496tBkIPwk/QJcXX14//ne0/hRwNaon38Zw1p7rbW2r7V2IO4znW6tPQd4HzjVS1b7vsN/j1O99BlZ0rXWfg+sNMbs5e06Cvgan3/mnhXAKGNMW+/ffvjeff+5e5r6GU8FjjHGdPF+BR3j7WtYqh9GNOPhxTjgG+Bb4LpU5yfB93YY7ifbPGCO9xqHq3d8D1gMvAt09dIbXGujb4Evca0UUn4fLfwbHAFM8dYHAZ8CS4AXgUJvf5G3vcQ7PijV+W7hPQ8HSr3P/TWgS7Z85sCNwELgK+CfQKEfP3fgWdxzh2rcr7dLmvMZAxd7978EuCiea2tYAxERH8u06hoREWkCBXkRER9TkBcR8TEFeRERH1OQFxHxMQV5EREfU5AXEfGx/wdb5F+DNHkPrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_return(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABG+0lEQVR4nO3dd3iT5dfA8e9p2XujbJAhZVUoSxkyBEQcqAioCAoiTtx7742irwoCggPkJ0NRVEQBERGUJasgW0BmGWWU0fa8f9xJk0JbOpKGtudzXbny5H5G7qeBnNxbVBVjjDEmNWGhzoAxxpizmwUKY4wxabJAYYwxJk0WKIwxxqTJAoUxxpg05Qt1BrKiXLlyWqNGjVBnwxhjcpTFixfvVdXy6T0+RweKGjVqsGjRolBnwxhjchQR2ZKR463qyRhjTJosUBhjjEmTBQpjjDFpytFtFCk5efIk27Zt49ixY6HOismAQoUKUaVKFfLnzx/qrBhjTpHrAsW2bdsoXrw4NWrUQERCnR2TDqpKTEwM27Zto2bNmqHOjjHmFLmu6unYsWOULVvWgkQOIiKULVvWSoHGnKVyXaAALEjkQPaZGXP2ypWBwhhjcqtdu+B//8ve97RAESRff/01IsKaNWvOeOw777zD0aNHM/1eY8eO5a677koxvXz58kRGRhIREcHHH3+c4vnTpk3j1VdfzfT7G2OyT7Nm0Ls3ZOErI8MsUATJhAkTaNOmDRMmTDjjsVkNFGnp3bs3y5YtY86cOTz++OPs2rUr2f74+HiuuOIKHn300aC8vzEmsLZvd8+HD2ffe1qgCILDhw8zb948Ro8ezZdffpmUnpCQwIMPPkjDhg1p3Lgx7733HsOHD+e///6jQ4cOdOjQAYBixYolnTNp0iQGDBgAwLfffkvLli254IIL6Ny582lf+mmpUKEC5513Hlu2bGHAgAEMGTKEli1b8vDDDycrkezatYuePXvSpEkTmjRpwvz58wH4/PPPadGiBZGRkdx2220kJCRk9c9kjMmgEyd820eOZN/75rrusf7uvReWLQvsNSMj4Z130j7mm2++oVu3btStW5eyZcuyePFimjVrxsiRI9m8eTPLli0jX7587Nu3jzJlyvD2228ze/ZsypUrl+Z127Rpw4IFCxARRo0axeuvv85bb72Vrnxv3LiRjRs3Urt2bcB1I54/fz7h4eGMHTs26bh77rmH9u3bM3XqVBISEjh8+DDR0dFMnDiR33//nfz583PHHXfwxRdfcNNNN6XrvY0xgbF0qW/bAkUON2HCBIYOHQpAnz59mDBhAs2aNePnn39myJAh5Mvn/uxlypTJ0HW3bdtG79692bFjBydOnEjXmIOJEycyb948ChYsyIgRI5Les1evXoSHh592/KxZs/j0008BCA8Pp2TJknz22WcsXryY5s2bAxAXF0eFChUylHdjTNb5/y7MzqqnXB0ozvTLPxj27dvHrFmzWLFiBSJCQkICIsIbb7yR7mv4dxX1H1tw9913c//993PFFVcwZ84cnn322TNeq3fv3rz//vunpRctWjTd+VFV+vfvzyuvvJLuc4wxEB0NL7wA48ZBVicdiI2Fr76CFi3gzz9h1Cho1Sow+TyToLVRiEghEflTRP4WkVUi8pwnvaaILBSR9SIyUUQKeNILel6v9+yvEay8BdOkSZPo168fW7ZsYfPmzWzdupWaNWvy22+/cckllzBixAji4+MBF1QAihcvzqFDh5KuUbFiRaKjo0lMTGTq1KlJ6QcPHqRy5coAjBs3Lij579SpEx9++CHg2lQOHjxIp06dmDRpErt3707K95YtGZql2Jg8qXdvmDABVq3K+rX27nXPzZq552+/zfo10yuYjdnHgY6q2gSIBLqJSCvgNWCYqtYG9gMDPccPBPZ70od5jstxJkyYQM+ePZOlXXPNNUyYMIFBgwZRrVo1GjduTJMmTRg/fjwAgwcPplu3bkmN2a+++io9evTgwgsv5Nxzz026zrPPPkuvXr1o1qzZGdszMuvdd99l9uzZNGrUiGbNmrF69WoiIiJ48cUX6dKlC40bN+aSSy5hx44dQXl/Y3ITz28rihTJ+rW8gaJ+fff80ENZv2Z6iaoG/01EigDzgNuB6cA5qhovIq2BZ1W1q4jM8Gz/ISL5gJ1AeU0jg1FRUXrqwkXR0dHU9/4lTY5in53JbcLCQBVWroQGDbJ2renToUcPWLAAqleHihUhsxMaiMhiVY1K7/FB7R4rIuEisgzYDcwENgAHVDXec8g2oLJnuzKwFcCz/yBQNoVrDhaRRSKyaM+ePcHMvjHGZIn3Z+7Jk1m/1uzZkC8f1KwJ55yT+SCRGUENFKqaoKqRQBWgBXB+AK45UlWjVDWqfPl0L/lqjDEhE4hAsWyZa58IRYfDbBlwp6oHgNlAa6CUp2oJXADxjDNkO1AVwLO/JBCTHfkzxphgCkSg2LYNqlbN+nUyI5i9nsqLSCnPdmHgEiAaFzCu9RzWH/jGsz3N8xrP/llptU8YY0xOkdVAoQpbt+bCQAGcC8wWkeXAX8BMVf0OeAS4X0TW49ogRnuOHw2U9aTfD9jkQ8aYHMv/Z25agWLNGti06fRz58/3DarbutVNAlivXuDzmR5BG3CnqsuBC1JI34hrrzg1/RjQK1j5McaY7HTggG87rUBRv75rpPY/ZvZs6NQJnnsOnn4a1q516ednuZU3c2xSwCAIDw8nMjKShg0b0qtXryzNDDtgwAAmTZoEwKBBg1i9enWqx86ZMydpEr+MqFGjBnu9nbRPSW/UqBGNGzemS5cu7Ny5M8Xzu3fvzgH//xXGmGSlhNQChbfUER+f/Jh//3XPc+e6Buxp09zrcyoqxMUFPrNnYIEiCAoXLsyyZctYuXIlBQoU4KOPPkq23zsyO6NGjRpFREREqvszGyjSMnv2bJYvX05UVBQvv/xysn2qSmJiIt9//z2lSpUK6Psak9M98IBvO7VA4T8brP9kB94J//76C5Ysgfffh7bM5bxrI6FMGThl/FiwWaAIsrZt27J+/XrmzJlD27ZtueKKK4iIiCAhIYGHHnqI5s2b07hxY0aMGAG4L9+77rqLevXq0blz56RpMwAuvvhivAMMf/zxR5o2bUqTJk3o1KkTmzdv5qOPPmLYsGFERkby22+/sWfPHq655hqaN29O8+bN+f333wGIiYmhS5cuNGjQgEGDBpGePgPt2rVj/fr1bN68mXr16nHTTTfRsGFDtm7dmqxE8umnnyaNPO/Xrx9AqvkwJjebM8e3nVqg8C8cbNjg2/ZWQsTGuud8nORjbiU8ZjccOwbZvNBYrp4UMGTzjHvEx8fzww8/0K1bNwCWLFnCypUrqVmzJiNHjqRkyZL89ddfHD9+nIsuuoguXbqwdOlS1q5dy+rVq9m1axcRERHccsstya67Z88ebr31VubOnUvNmjWTpisfMmQIxYoV48EHHwTg+uuv57777qNNmzb8+++/dO3alejoaJ577jnatGnD008/zfTp0xk9evRpeT/Vd999R6NGjQBYt24d48aNo9UpM5KtWrWKF198kfnz51OuXLmkuayGDh2aYj6Mya1OrTTIbKDwup0Pqcc/8OFU+OMPeOMNV9zwzOgcbLk7UIRIXFwckZGRgCtRDBw4kPnz59OiRYukqcF/+uknli9fntT+cPDgQdatW8fcuXPp27cv4eHhVKpUiY4dO552/QULFtCuXbuka6U2XfnPP/+crE0jNjaWw4cPM3fuXKZMmQLAZZddRunSpVO9lw4dOhAeHk7jxo158cUXOXDgANWrVz8tSICborxXr15J81B585VaPvwXaDImN/HO8XnffTBsWOqBwm9y6GQTB/qvNSEk8nj4a+yNuJhyV10F7du7H6sTJ1qgCIhQzDOOr43iVP5Te6sq7733Hl27dk12zPfffx+wfCQmJrJgwQIKFSqU6WucuqDSgQMHMjRFeaDyYUxO4g0U3t9g/m0R/vxLFH//7dv2L1G0ZCHnJPwHj73pu2jfvpCN7YLWRhEiXbt25cMPP+Sk56fGP//8w5EjR2jXrh0TJ04kISGBHTt2MHv27NPObdWqFXPnzmWTp1tFatOVd+nShffeey/ptTd4tWvXLmnm2h9++IH9+/cH5J46duzIV199RUxMTLJ8pZYPY3Ir7/gHb2H/TFVPhQuD/9R1/oHincZj3GIWl17qSxw7Fp58MmD5PRMLFCEyaNAgIiIiaNq0KQ0bNuS2224jPj6enj17UqdOHSIiIrjpppto3br1aeeWL1+ekSNHcvXVV9OkSRN69+4NwOWXX87UqVOTGrOHDx/OokWLaNy4MREREUm9r5555hnmzp1LgwYNmDJlCtWqVQvIPTVo0IAnnniC9u3b06RJE+6//36AVPNhTG7l/b3mWT6GgwdTPs4bKM49N/m4C2+gaMIyWqwYDXfema0liNOoao59NGvWTE+1evXq09JMzmCfncktvv9eFVR//121RAnVoUNTPm7GDHdcmzaq+fOr7t2rmpio2rOnakHi9Hda64mSZVX37Qto/oBFmoHvWitRGGNMgP3zj3uuXRvKlUtereTPW4qoUMFVT5Ur5wbXJR47wTSu4EL+4L+hr/saO0LEAoUxxgTY2rWupqh8effln8LEBwB4mvGSzeEUHQ3tNo+jCzO5h3fZd9UtKZ+cjXJloFCbdDbHsc/M5CZr17ovfxG3DOqps24kJsLrr8P69e71Y4/B0qVuO/qxcdwfPZh1+etT5OG7adIke/OeklzXPbZQoULExMRQtmxZJDuXgDKZpqrExMRY91mTa6xd6yb1AyhQ4PTG7Fmz4JFH3HaRIlC8OESW2swhGlIMN4hids2BvPra2fEdlusCRZUqVdi2bRu2TGrOUqhQIapUqRLqbBiTZYcOwfbtvuqkAgVO7x7r38OpenVcEePOOynGET6lHzsqXsDC825ncHZl+gxyXaDInz9/0ohlY4zJbt6pOOrUcc/587sBd5s3Q8mSrl3af0R2kybACy/A99/zZJkPeGnf7VQtAM0LZ3fOU5cr2yiMMSZUvNOLN1w7GQYOpNbRlZw4ATVruqniwNdmEcVfPPTH1W7hif79OefZIYArlRQsmP15T03QShQiUhX4FKgIKDBSVd8VkYmAt42/FHBAVSNFpAZuqVTPEh0sUNUhwcqfMcYEw9VXw90Mp/5TQwF4Kv83fHlyGVAlaZ2J6dOhBAf5nu4U2gUMHQovv0zBz12bRGysq7I6WwSzRBEPPKCqEUAr4E4RiVDV3qoaqaqRwGRgit85G7z7LEgYY3KiuqzldR7mRNfLYeVKCsQf5SOGEEYCADEx8M03yqfcRGn2M7zbD27mwMKFk0oRiYl5JFCo6g5VXeLZPoQrLVT27hfXJek6YEKw8mCMMdnpeFwiYxnAUYoQPnokNGjA8Aov0YPpDOceQDlwAJ7mea5kGg/wFv9WiEo637+6KU9UPfnzVCtdACz0S24L7FLVdX5pNUVkKRALPKmqv2VH/owxJit27oQnnoAnOy2kNQv49vKRXF75HAAmnHMfxXet404+oCpbKfr6BTzH8/yPXgznHj5v57uOf3A4m0oUQQ8UIlIMV8V0r6rG+u3qS/LSxA6gmqrGiEgz4GsRaXDKOYjIYHC9xgI1mZ0xxmTFk0/CmDFw+fJZ1ASK3tgzaV98PNzDcHZTgWd5DkZ+y3dcRl8mUK2acMMNJDvWq0iR7Mv/mQQ1UIhIflyQ+EJVp/il5wOuBpp501T1OHDcs71YRDYAdYFki8Oq6khgJEBUVJQN5zXGhJz3C77k0tksownl65dLti+BfDzHM2ylKtUaleKlFZfzx8Lw09Yd8h/BHeLpnZIJZq8nAUYD0ar69im7OwNrVHWb3/HlgX2qmiAitYA6wMZg5c8YYwIlPByKcphWCb/zIbdzS1XfPl8pQRjDQFjhXhUv7qb48He2Bopg9nq6COgHdBSRZZ5Hd8++PpzeiN0OWC4iy4BJwBBV3RfE/BljTEAcOwY38jmFOcaxy69LtnTEqetnexUvfnra1Vf7ts+mQBG0EoWqzgNSnKhEVQekkDYZV01ljDE5xrFj8P33MJYf2EQNqvVqmWx/6dKwZcvp56UUKMqV8802G8p1ik5lI7ONMSYLFi6EIwdO0JFZzKArRYsl/308bRoMH+5bo8KrWLGUr+ctVTRrlvL+ULBAYYwxWbBrF7TmD4pzmBl0PS0AVK0Kd9/tSgr+wsNTvt5777l1KlIqcYSKBQpjjMmgPXvg5ZfdCOp9+6ArM4gnnFl0pGjRlM8pWTJ91y5Q4OxqnwALFMYYk2FDh7oBdrNmuSk5ujKD5UUvJJaSp/Vk8goLg2uvzd58BooFCmOMyaAEN20T77wDx7fvpRlLWF+rC5D2iOqRI4Oft2CwQGGMMel08CAMGeJrX5g+HSqung3AZcM6M2YMNG2a+vne0dY5bTHHXLdwkTHGBMtLL8GIESRrh6i5eRaHw4pTrH0UN5/hG7VgQXd+587BzWegWYnCGGPSaaNnrghvieC886D+7rmsLn0R5Evf7+7Bg6FWrSBlMEgsUBhjTDrt88wVERPjngse2kvNuNVsOLdt6DKVDSxQGGNMOh06lPx11O7pAOxrmsPqkjLIAoUxxqTTgQPJX/dmIpupzrlXNE/x+NzCAoUxxqTD+PGwfr3vdccmMVzCTP7HdVSvkcrgiVzCej0ZY0w69O8PZYjhu5I3ckHnsvxZoA35/45nIr3pVynUuQsuCxTGGJMO1arBbRtfo/XBH2EytOML1nMeS2hKhQqhzl1wWaAwxph0aNMGum38kcSLOxAWd5STazcw5MBH9OkjqU7wl1tYG4UxxqRD2N7dNGYFYV0ugd9/J//Obdz2v858/HGocxZ8VqIwxph0qLN1ltvo3NnN4REeTq9eoc1TdglaiUJEqorIbBFZLSKrRGSoJ/1ZEdmewvKoiMhjIrJeRNaKSNdg5c0YYzKqwa5fOJSvVNqTOeVSwSxRxAMPqOoSESkOLBaRmZ59w1T1Tf+DRSQCt5Z2A6AS8LOI1FXVhCDm0RhjzkyVZjE/sebci2me2xskUhC0EoWq7lDVJZ7tQ0A0UDmNU64EvlTV46q6CVgPtAhW/owxJr0O/7maKgn/sieq+5kPzoWypTFbRGoAFwALPUl3ichyERkjIt61nCoDW/1O20YKgUVEBovIIhFZtGfPnmBm2xhjUIVDE78HILGbBYqgEJFiwGTgXlWNBT4EzgMigR3AWxm5nqqOVNUoVY0qX758oLNrjDFJEhMhMuxvSr77HEuJpGzjtCpFcq+gBgoRyY8LEl+o6hQAVd2lqgmqmgh8jK96aTtQ1e/0Kp40Y4wJiV274Eq+oUjiEe5hOJVy+Qjs1ASz15MAo4FoVX3bL/1cv8N6Ais929OAPiJSUERqAnWAP4OVP2OMOZMtGxO4kc9ZTX3m0TbXj8BOTTB7PV0E9ANWiMgyT9rjQF8RiQQU2AzcBqCqq0Tkf8BqXI+pO63HkzEmFHr2dIsLXV54NnVZx02MA6Bw4RBnLEREVUOdh0yLiorSRYsWhTobxphcRjyTwa5o2p+qS76mIrs4TiFy8NdlMiKyWFWj0nu8TeFhjDF+EhPdc31WU2/JeCbSm+MUCm2mQswChTHG+Nm71z3fxggSCeMpXghths4CFiiMMcbP2rXQlMXcxgh+CL+cGUsrhjpLIWeBwhhj/Gz57V8WE0UhjrOiVFvKlHHpebUhGyxQGGNMMkd/X5q0varSJZQs6bavvDJEGToL2DTjxhjjJ2KR6wrbl/EcrBxByZKwfDnUqRPijIWQlSiMMcZr+XJa7/6GV3mEL+lLuXIuuVEjKJSHOz5ZoDDGGK9ZswgnkQ+4AyDPTtlxKgsUxpgc5eBBaNEC/v478NeO/nELRyjCVs+0c5Xz5hyAp7FAYYzJUX75Bf76Cx59NPDX/m/Bv2yhOuCGZlepEvj3yIksUBhjcpT5891zMKbTqBC3xRMonPPPD/x75EQWKIwxOcbUqfCWZwWbQK9IunkznHNiC+WbVeeRR1zaeecF9j1yKgsUxpgcw1uaKFsWDh0K7LWX/rSH8uylUvs6vPwyHDkCBQsG9j1yKgsUxpgcQRVGj3a/8tu2hf37A3Pd+Hh46ik48qubibpIuyjCwqBIkcBcPzewQGGMyRH27nXBoX59KF06cIFixgx48UX4Z/xfJCKUuLhpYC6ci1igMMbkCI895p5vuQXKlAlcoPCuPRHFItZwPmEliwfmwrmIBQpjTI4werR7LlcOihWDo0d9a0dkxeHD7rkpS9CmzbJ+wVwomGtmVxWR2SKyWkRWichQT/obIrJGRJaLyFQRKeVJryEicSKyzPP4KFh5M8bkLP4BoXVrKFrUbcfFZf3aEydCBXZRhe1E3GDVTikJZokiHnhAVSOAVsCdIhIBzAQaqmpj4B/gMb9zNqhqpOcxJIh5M8bkIAsXuufPP4d8+XwNzUeOZP6aM2e6aqcpU1xpAkCirESRkqAFClXdoapLPNuHgGigsqr+pKrxnsMWADb20RiTpv/+c8+NGgEffMCVIy6lLms5ejTz13z+ed/2hbLAbURGZv6CuVi2tFGISA3gAmDhKbtuAX7we11TRJaKyK8i0jaVaw0WkUUismjPnj3BybAx5qxw/DicPAkHDrjXpQodg/vvp8qKH7mT/8tSoPCNw1DuLD4OOnSAEiWymOPcKeiBQkSKAZOBe1U11i/9CVz11BeepB1ANVW9ALgfGC8ip31qqjpSVaNUNap8+fLBzr4xJkRUoWpVuPRSNxEgQOkdq130ABqxIktVT7Geb6P+jKNM7Bbo3TuLOc69ghooRCQ/Lkh8oapT/NIHAD2AG1TdjC2qelxVYzzbi4ENQN1g5s8Yc/a6917Ys8dNArhvn2tPKLphOQCxdZtRg80BKVE8yYvsq9II+vXLeqZzqWD2ehJgNBCtqm/7pXcDHgauUNWjfunlRSTcs10LqANsDFb+jDFnL1XXcO310ksuLWzeXChThkOtu1CVrRyNjU/9Imdw6BAUIo5abOR496ttKHYa0hUoRKSuiPwiIis9rxuLyJNnOO0ioB/Q0a/La3fgfaA4MPOUbrDtgOUisgyYBAxR1X2ZuCdjTA43bZorRTRqdMqOxYuhVSu05nnkIwHdui1T1z9+3D3qsZYwlHJt62c907lYetfM/hh4CBgBoKrLRWQ88GJqJ6jqPLyTuif3fSrHT8ZVUxlj8rDERLjqKrfdogWsWOG2z2O9W7z60ksJq1UDgLB/NwM1Mvwe2zzxpQGrAMgf2SArWc710lv1VERV/zwlLfNlPmOMScXy5b7tDh182zd4+70MGkT+86oBkH/n1gxf/4EH4JJL3HYkyzhJPqhrzaFpSW+g2Csi5wEKICLX4nopGWNMQF16qXueOtVN1wFQlr08x7PQqhXUrk2h89wapQV2b2XMGNiagXjx9tuwaRN05Uce4k3CKpSDAgUCexO5THqrnu4ERgLni8h2YBNwY9ByZYzJs/Lnd88dO/rmYVpb70pYC9x2GwCFyxZhL2UJ37GNgQNdgWDt2oy9T69C38ExCH/7zcBlPpdKV6BQ1Y1AZxEpCoR5RlobY0xATZoE27fD0KFu7FuJg1vRQc/DqPluIQpPF9Z8+WAbVYhZ5ooS29LZpu2dG6oBKxl47P+gWTO44YZg3Equkt5eTy+LSClVPaKqh0SktIik2pBtjDEZtXgx9OrlGrMbNcKNiGvYEEaNgvbtITo62fqnW6lKVVygSEhI+9rTp7txGBddBGEk8D+ucztO61ZlUpLeNopLVfWA94Wq7ge6ByVHxpg8KTbWt92jU5wbcedNvO46X52UxzaqUAVXlDhToPCus710KUziWiKIZl3p5vDKKwHKfe6W3kARLiJJq8eKSGHAVpM1xgTErFmuTQKgcmWo+NX78MknLuG332DI6ZNJ12pflXLEUJijZwwUnlk/AKUnXwPwUdev4ZxzApH9XC+9jdlfAL+IiOeT42ZgXHCyZIzJa2bM8G2vePEbuPlh9+Kpp6BNmxTPOVraTTxdme2s1zqpXvvECZg/323XYR0AQ/iQ626tlPWM5xHpbcx+TUSWA508SS+o6oy0zjHGmPSIjYX333fbtWtDqcdvdy8++iipl1NK4spVBaAqW1lP6oHi//7PPRfjEMtpDMD5D/RIKsGYM0tviQJV/YHkU4IbY0yWffmlW9b0+++hcydFzj3uBlOkESQATlRwJQpvO0Vqjh2Dq5jKVK4GYFaFPtz1qi2DkxFptlGIyDzP8yERifV7HBKR2LTONcaY9Jg+HapVg27dIP/mdW6SJ+8cHmnwBooPuCPN48qXh6G8m/S66ZJR5Ev3T2QDZwgUqtrG81xcVUv4PYqrqq3wYYzJks8/dxMA9ujhuq/ytmei6S5dznhuvuKF+YsoinGECFal2qCdcDKRhqwEYBHNKFW5aIByn3ecsdeTiISLyJrsyIwxJm+ZMgWKF4eXX8bN+z12LPTvDzVqnPHcggWhD18CcBG/s39/yscVX7+UcsQwgE9oxYKA5T0vOWOgUNUEYK2IVMuG/Bhj8pC1a93EfyVL4rrDHj+eYlfYlBQsCBupxXEKcB4bOHYs5eMqrfoJgB+4lIT0N8saP+kdR1EaWOVZk2Ka9xHMjBljcp+DB+HVV12X1bVrYfVqaNvWs+PJJ+HCC93Ef+lQsCCA8B+VqMR/nDiR8nFltq9kIzXZTcVA3Uaek97w+lRQc2GMyRNef91VMyUmwuzZLq1HD+DXX13V09NPp/taTZu6KTl0YyUq79jOyZMpH1c4die7w86hRLHko79N+qUZKESkEDAEqA2sAEarqq1DYYzJlAWeJoInnvCl1a2j0OdpqFQJ2rVL97UqV4Z582Bb60pU2rGC5s1TDgRFD+9iTXhdduxwy6majDtT1dM4IAoXJC4F3krvhUWkqojMFpHVIrJKRIZ60suIyEwRWed5Lu1JFxEZLiLrRWS5iDTN5D0ZY84yf/3lFguaNSt5+i23QNimDfD33y56FC6c4WufKOeqng6lMqd1sSO7OFCwIkWKQFHr8JQpZ6p6ilDVRgAiMho4dZW7tMQDD6jqEhEpDiwWkZnAAOAXVX1VRB4FHgUewQWiOp5HS+BDz7MxJoe78krYccpSZ506wYjhx+GDqS4hA6UJfyfKV6YEhyjGIaB48p3Hj1Pi+F5OnGvTdWTFmUoUSbV+Ga1yUtUdqrrEs30IiAYqA1fimydqHHCVZ/tK4FN1FgClROTcjLynMebssm2bGx/hHyTq1XPPz18wlXzn14aHH4YyZaBB5tatTqjogsC5KSy6uWe5S8tfvXKmrm2cM5UomviNwBagsOe1AJreQXciUgO4AFgIVFRV7ye6E5K6IlQG/Bc03OZJS/bpi8hgYDBAtWrWY9eYs9W6db6lqFu1gg8+cJ2batWCbx/9nQvfvNp3cP/+nhF3GecNFJX4D0i+9vWmedspD9TvbIEiK9IMFKoantb+9BCRYsBk4F5VjRW/fwyqqiKSoeYlVR2JW5aVqKgoa5oy5ix1333u+dJL3TxOSRITuXO6ZzmbRo1g+PB0d4lNSeI5/oEiuRObXVrZRlb1lBXpHUeRKSKSHxckvlDVKZ7kXd4qJc/zbk/6dqCq3+lVPGnGmBxo2TKIiHDLmyYza5brnvTEE7B8OVx8MRQqlOn30XNTDxS63X2FFD/fShRZEbRAIa7oMBqIVtW3/XZNA/p7tvsD3/il3+Tp/dQKOOhXRWWMySFiYlzX1e3b4dZboUgRzw5V2LIFBg92Bzz5ZEDeL7xUcQ5TNMVAsXHudo5RkFI1SwfkvfKqYI5nvwjoB6wQkWWetMeBV4H/ichAYAt4F6/le9zyquuBo7jFkYwxOcw338B/nu/sZGsO9ekD//uf2/766yyVIvzly+8bnX2qYrH/sZ3KnFcgc+0fxglaoFDVebhG75R0OjVBVRW4M1j5McakLDER4uICNMbgiy/o+dDTNKY0t9aeQ1RUMZf+668uSBQp4qaMvfLKALyZowq7qEhFdnHyZPKltSuxnfiKVu2UVUFtozDGnN327YPwcChWzFUZZcmOHXDrrSTGHSeKxSxqfItbNSgx0dVB1a4NO3dCz54BybtXQgLspzSl2X/afE/l43dwqKj1ss8qCxTG5EF//um6qX72mS/t008zfz1VmNbyRU7GnaRF3K/MqDqI8ClfQbly0KKF6yv7wgtuTvEAK1vWFyjiTxntVSpxH0eLlgv4e+Y1FiiMyWNU3WR6mzbBvff60n/9Ne3zNmxw4+J69ICPP06+L3bZRi7dOpJRDGIj5zGrz0i3ItHFF7tpXh96CK67LsXrZlXlylCtsQsUQ4e6EoYIhEkipXUfx4uUCcr75iUWKIzJQ+Lj3ff3qb+869WDrVtTPgfgjz/g2mth/363dOngwZ4J9vbtg0OHOP7QE5wkPy94Jppu3kLg8svhu+/g99/dtLFhwfu6KVa1NCU4xOfj4pMmBizOIcJJ5HhRCxRZZYHCmBzon39g8WLfbKzp9e67vuWoH33UPVep4n74b96c+uyq/fu7cRFe+TnB8utfhYoVoUQJKvzyJW/xAPUursTTT8M112QsX1l1spjr/lqKA0nTjZfFNbqcKG6BIqtsuSdjchBV12Ho2299aceOeRfxOTNv71SAF190495uuQWWLoURI2DFCmjc+PTzDh92z3de/i+PtJ3P+odH0OTLOS6xfn1eOnofnzCQjbMzdVtZFl/CBYrS7OfYMdcm0ZQlAMRWrJvqeSZ9LFAYk4Ns2pQ8SID7pd/yDPMsx8e7LrB//+1eX3ut6+30f//nXnuHNERHu0Ch6kotZcu6R0wMvNZ3GQ991w759hAVyc8nDOAOPqB+ocIsjYa77w7orWZIQrFSgDdQuLRWLOAYBdlbs3noMpZLWNWTMTnIpk3uuVEjX9qp03efavFiN7agY0e3JPVXXyUvWYCrfgIYNMjTEBwG558P5cvDK69AxxM/cN+kC5FSpeDPPynOIW7hE45RmKVL3bn16wfkFjMlIVmJAgoRx4O8xW4qkL+w/R7OKgsUxuQgM2e656lTfWl79qR9jrc306JF7rlGDb+JWlVh4kRKPDiYroV+Tapi8vfLE7/wA93JfzIOJk+G5s1Ztvr0uq5QBorEUq4dogz7OHYMWniWzvmdi8hncSLLLFAYk0OsWAGvvQa9esF55/kGyJ0pUPivKlewoBv3Bri6qIcfhj59kFEfM+5EH4pwJNm5hYhjBLcBsHTYHGjuqnHq13clFX+hDBTx5d2gunPZQVycL1DczXvs2xe6fOUWFihMnrBlC0RFpTyobM0aV1ef0nrLZwNVt5ZD48ZuBowPP3TpZcq4KqNPP3VtEImJKZ8726+B+eaboVQpXBentm3hzTehc2eYNYuKiTu5l3eSnf8Yr1CbDXTiZ4pd1j7ZvqanLFZcoUJW7zTztGQpjlKYymznv3kbeJyX2UhNYijHrl2hy1euoao59tGsWTM1Jj3efVfVfW2qHj3qS9+zx5c+YEDo8peWTz7x5fGdd5Lve/ll376mTU8/NybG7evbV7VMGdVNKw6pvvKKaokS7jFhgmpioqqqzi55pcYTpj/TUZ/hGR3ESFXQL+irCxaknDfveyckBPaeM+rbb1XXcZ5OoLfeyXuqoIP5SEH1xRdDm7ezEbBIM/BdG/Iv+6w8LFCY9Lr1Vt+X2qJFqmPHqn79tepFF/nSo6JCncvT7d3ry9/ttyd9pyc5cMC3302t6bNmjWrdui592U1vqVar5juwWzfVpUuTHd+g+BbdQ9lkFzxACW1W4d9U8zdvnuqyZQG62Sz49lvVObTTX2mrb3GfHqaIQqKC6okToc7d2SejgcKaeUyuFx8PEya4sWG7drlqnDFjfPsrVIAmTc7ceyg7vfmmqxlavdq9HjYs+XQbXiVLuh5K3mqnuDgoXNhtX3CBe30BS2j82YNQoIBvRbn27U9benTVoWrUZj133RPOluFf07z0Bh7Z/wiDriucaj4vuigANxsAqrCdyrRkIfsow0Zq8eefQqFCyWeTNZmUkahytj2sRGG8jh1THTRIdcuW0/dFR7sfyB99pCqS/Bc4uHNuu021fPnsz3dKTpzw5a1TJ9XatVUTTsS7aqLOnV1Gu3VTHTxYtU8fnUknXUZjfZpntV8/d43ERHf+dXypR8KKunP27z+9SOLH+56HD3uqctapVqmiunZt9tx3VnzzjerzPKkKmoDo3NJXhDpLZzUyWKKwxmyTK/zyC4waBffcAxs3uglLBw1y8xetWOGOad7cNQD7++gjqFYNKlVyvYe8g7VCaedO3/aaNXB1g7WENbsA+vZ1Aym8821MngwzZ1KCWApwgud4ls2fzUXV9YhqyArGFrmDIuWLua5PpUqdVorw5x2bUbSom/ivdm3396ubAwY2q8JIBgMQhrKzaK0Q5yh3saonk+PFxcGhQ2772DHXhfSvv9xj9GjfcfXrQ4MGMHeu+75UdZPbgetuCu57OJTdPAH+/de7pdyw/Q1e2vkUlC7h5gTv29cNqfbTUlw31vXU5nFeZv/+duzYAZ9zo5uHb84cN3ruDObNO3t7fp2JKmyjKtGcT33WsK70GYaqmwwJ5prZY0Rkt4is9EubKCLLPI/N3iVSRaSGiMT57fsoWPkyucusWa7LaJ8+7vXx426VzVMnKo2MdHX348e7BdZiY2HbNt8P7Hr13HPHjtmV89R98ol7vov3eY1H2BXZzQ1auPHG04KE1zEK81u9QXThJ3b8+DcHonfQhOVsv+GRdAUJgBIlfCO0cxr1TGbYgdm8wqMsqXx5aDOU22SkniojD6Ad0BRYmcr+t4CnPds1UjsurYe1UeRt3nr4lB7ffqt64YVuu1mzM/d8OXnSd24orVvn2lEqskPjKKjT6KHb/k277+nzz7s2luW/7tNjFNBva9+rvw/5VBV04+Ql2ZTz0Nq2Lfnn37NnqHN0duNs6fWkqnNFpEZK+0REgOuAs+D3m8kJvL8YwVcK2L3bPZcpA1df7XoIzZ/v0po3d+smLFlySinh+HHX+8d7kdhYyJ+ffIUL07s3TJwIP/3kekJFRgb7rk53xx3uXpff9gEFR54gfNjbVK6adsH/qae8W6X5q0p3Lln/AdG7W7KHclS4pEnQ83w2qHzKstgFCoQmH7lVqBqz2wK7VHWdX1pNEVkqIr+KSNvUThSRwSKySEQW7TnT3AUm12jRwlUnVarkS/PWqAwf7lZcu7TjccJIIDzcfdGXKgUdmx5wjb633OK+TQoVct8ipUq5OquSJd3Bt91G06ru31PXrtClS3bfoeviOnMmFOQY5Sd/iPToQfehdTJ0jf3db6QgJ4iM/Y2vKw6haPG82V8ljTZ7kwmhaszuC0zwe70DqKaqMSLSDPhaRBqo6mlNa6o6EhgJEBUVlcoyKya32LXL9eDxTmi3cyccPOjaGw4cAFAa7foF+ozisclTuZ9wFhTtgozu4YoYH3zgShElS8Kll7rW7Lg4t8BCgQKuOPLPPzBuHANL/Mp7zGQbVZPNn6QKjz3mmggaNgzOfcbG+qYAn9n/C2TcXrjvvgxfp/27V3PLyNGs4XwuuLp1gHOZc3gXLzIBkpF6qow+SKHtARecdgFV0jhvDhB1putbG0Xul1L7w/DhbmxECQ7osgbXu8TixXVdlzv0Q27T/UUru7SwMNWbblL97bczN1LMnavHCxXXk4TrXzTTXkxM2uUdHR0WFvj7O3lS9YMP/O8vUU/Wa6DapEmaYx7S4m2bGTYsoFk96/n/G7nChlGkiRwwjqIzsEZVt3kTRKS8iIR7tmsBdYCNIcibCYCEhDPPaJoe7he20pFfeLza5yQs+ItixVwX1t0/LmE1ETRZNR6efRZ276bat/9HzIsfEb79X7eaz65dMG4ctGlz5uG5bdvy3RMLGMsAChPHZ/TjkatdzejBg+6QlCbdy6oHH3TtEl69mUi+tatcaSKT9Sfe1e5Klw5ABnOoEydCnYNcJiNRJSMPXNXSDuAksA0Y6EkfCww55dhrgFXAMmAJcHl63sNKFGenm25yv+qOH8/adf7vnjW6mvOT/VScVbi73n/FOt0WXk23UlkPTPklMJlWN/AZVM/hPz1Icf2c6/X4cdUFC1LuEbV+veo//2TuvRISVAcOdNcsUUK1Y0fVamzWWIq5CZqOHcv0fVxyibvu1KmZvkSO5F+i6Nw51Lk5u3EW9Xrqm0r6gBTSJgOTg5UXk728U3n/+6/f2geZ0HjKs1QN2w7/96Frzf7ySzq88QYdpn3PQUrQmZ/5q2fglrksUsQ97+Rcvi/Si+5HpzBpYgI33OQbu3DypK9w4r03zUBL2aJFsG6dG1jtHQw4bJhnpu/qzxBOAsyYkf5FsFPgzV8qQy5yvY4d/XuCmYDISFQ52x5Wojg7eX/V9eqVhYvEx+uB8NI6o8otvrTERL2d/9MZXKItWBDwMQ9//OHL+8+DXPGiD+OT/VLdvt13fGbGXVSu7DvvggtU5871TNG9aZPGS7guaTc0y/cxeLC7/k8/ZflSOUqnThn/PPIqckAbhcnBunWDW289PX3jRjc1RkKCL+2rr05fBe1MvO0AR39fSsmE/fxbr7NvpwidJt1BV37iT1pyzz0Zz39a/Es/e5tfylrqMo7+nE90Urq3vSIjpQh/27f7tp9/3s0QG6YJcMcdhBfIxwWfP5C5C/t56y3X2atz5zMfm5tMm+Y//YkJJAsUJt1+/NHViowa5QKDv/r13czV3tXEhgxxz6cel5r5813bbaNGMGIEvNDeLQ6tFycfk+kd3/Dhh/Duu5m9k5SVLeu+XCdPhvzlSnIxcyjASaKJ4H7eAkhaVtO7ylxGeNeu9qpe3bMxYgT88AM89xxUrZr5G/AoVgxuvz3vjSUoUiQgfz6TkowUP862h1U9ZS//KpgiRVLe9+QjJ7Q73+k/vR7Xp3hO1zS/wdVB7diR5rWffNJ3jTDidS11dAEtMttDNMu++87lpQ/jdSlNdH+JqhpGvELyFecyUtWRL5/vnAEDPFVOW7eqnnPO2blqksm1sBXuTDDs25dGoDh2TC9lur7Kw7qeWskO3EYltx0Zqbp5c6rXv/JK32kP86oq6KFxk4N+X6n55htffl6OmqwK2pPJCqqVKqm2aOHbvySd0yl5z0laijU+XrVVKxdBUltr1JggyGigsKonky7LlrnnKVNc3/+EBE89/bp1aKtWfM9l3McwtlKVSTdMhaNHKcIRqrCdf9+YCOvXu7m8W7Zk7ajfuPlmN2Aa4OhR+OYb5e3GY9lfoS6v8SjauzfF+vUM1e0SH+/bXn3e5WjhwkzhGs4nmmLF3OBur6efTr3NIjERnnkG3nsP/vwT7r80msJXdYWrroJ8+WDBAtf9qaVNi23OYhmJKmfbw0oU2SM+XrVxY/fDd+9e1TffVI1kiZ68sJ0qaGy+UnodX2phjiiozp7tzps1S329b5YsUb3nHtXChVVBP+VG3XD766qNG+vu6s30Fzq4g2vWdPVQhw6F8pZ1+XJfiaF/f1Wd7EoVo7lZmzZ1y0/XqeM7pkmTlK+zdKnvmKIc0p2lPeNCatVyy8cNHpx9N2WMB1b1ZAJt0iT3L+Wl7vPcl73nmy++aHEdXfMFrczWZNVShw+781at8n0nxsd7LhYbq1OL9FX/EzaVa6YbpJYmPP+i34Gh98gjLou33+5en+h9oyrok5VHa8mSyf4UCinPEjJmjG//MIZqgoS5CGpMCFmgMAFXjFj9hsuTvvGOlquij/CKLvwhRvv0ccldurg1ABo18p23c6fvS/IXvwHU1aqpVuFfHXP5FNUdO7RrV9XmzbP/vs5kyhSX948+8iRs2aL7irg2l1XU15+7v6UNWJF0jym111cse1L7MU7v4H2No6AuihyYrfdgTEosUJiA+vhj1Vd4RBMQTbz3PtXNm3XePPcvZ+xY1Q4d3CR0KTl+3BcoZsxwaRs2+NIuu8w9QPXyy7PvntIrMVF1zpzkc/O9eGO0/klUUiP9IYrqrYzQohzSFSuSn//O8wd1Fhcn3fBuyukbd2/J3pswJgUZDRTWmG1StXkz3H7rSW7gC2Ja90CGvQ3Vq1OsmNs/YAD891/yNSL8+S8ek5joHt61qcEtLDR9uts+GyewE3FjQ5KNRzj/fFrwF9XZwsGfFrKCRozkNv6iObGrk+a55I8/YPfT79GBOfx6/Qhi569k/NNrue2latl/I8ZkkQUKk6rffoPbGEFVtlHuscFJ6d5AAW4k7Kmri6Vk0qTkcw+9/nry/c89l8XMZpNzz3XPCeSjeKcW1Nk5j+iXplCZ7TQd3AzGjIEtWzjw6kc8zfNMpzt7eg6mROsGDH2uDMWLhzb/xmRGqBYuMjnApBExfCLPkNixM2E9LktK9w8UcXGplyj8eSfAAxg0CBo39r3evRvKlw9AhrOBd+JAcCvulasYTvwtPWn1RD3mFOhFoYEDAbgUWEgLnqs0ku/ahSavxgSKBQqTzKefui791atDr/n3UVJiCXtnWLL6F/9AARlf9e3112HLFt/rnBIkAK69FpYvTz6PUtmyEE0Elfcs4+Svf8DMmXw+PIbpl7zDn5Ns8WaT81mgMEliY6F/f7f9w8it9NHxrO8+lHqnRAL/X9XgpnVOzZgxbrlqf6VKuaWru3d3pYucJF8+ePnl5Gneab3jyQ/t2hEb2Y5+L8JLTbM/f8YEg7VR5HFu3WlnyRIoymH6MIGGg1sTTz6ODrz7tHNE3MynXoUKpX79m28+PU3ErXk9fTr0DN3g64C68073fOSIW4Ib4PzzQ5cfYwLJAkUedfIk1K3rehtt3Qo7dsD1Hf7jT1owges5RiE68zMVW9ZI8fwrr0z/ey1a5Nv+7rus5fts1dRTetizB5p71lIqWTJ0+TEmkIIWKERkjIjsFpGVfmnPish2EVnmeXT32/eYiKwXkbUi0jVY+crr1DMn0QsvuJXWwC3P/NgjiYzneqrLVpgwgbUT/6ZCzzacc07K1ylaNP3v2aCBW8di8WK47LIzH58TVajgnr/6ypd2aluOMTlVMNsoxgLvA5+ekj5MVd/0TxCRCKAP0ACoBPwsInVVNQETMO++C/ffDxdc4L608+d3JYsfJh9hpnThQuajL70CffpwGXDZdalfy/slmJ7lNgsVcsst5GY1arjnhx/2pVmgMLlF0EoUqjoX2JfOw68EvlTV46q6CVgPtAhW3vKi5cvh3nvdoDfvqnN33w1hJPA+d9FK/2DzDY8j992bruuVKOGe77gjKNnNcWrVOj3NxkyY3CIUbRR3ichyT9WUdzxuZWCr3zHbPGmnEZHBIrJIRBbt2bMn2HnNFXbtgiZN3PaXX7rneucc5I3YwSSQj5sZy9iyD1L9s5fSbpn2U7gwxMTAsGFBynQOc2pPMLAShck9sjtQfAicB0QCO8CzvmQGqOpIVY1S1ajyOakDfggNHeqemzSB3r1hxcKjLK3Ti7BRH0P16ux+/B16b34tw0tnlimTvqqnvCoj7TjGnM2ydRyFqu7ybovIx4C3D8x2wH+12yqeNBMAP/4IN94IH32o8PMvNHz+eTc/xwcfwO23UyHUGcwlChZ0izG99pobRFiwYKhzZExgZGugEJFzVXWH52VPwNsjahowXkTexjVm1wH+zM685Tbr1sGsb49w7arnmHfwe+qP/4fwL+Jdt6cCBdxIuJQGOZhMK1LEBYqICOjRI9S5MSZwghYoRGQCcDFQTkS2Ac8AF4tIJKDAZuA2AFVdJSL/A1YD8cCd1uMpcxITYdHkLbxz3e88z9OUZiML6caJDt1p2jK/Gzxx5ZVueLQJqKJFYf9+135jTG4StEChqn1TSB6dQpr3+JeAl4KVnzxh3TqWtL6LFjE/MR7Ymq8GHeJnM5f2jOoLTQeGOoO5mzdAWKAwuY2NzM5h4uNh/nxXcvjsM5gwAf7bchI++4z4thdTN+YP3in/EktGLqLq0X/o9V57wDdy2ASPt+eTd+4nY3ILmxQwh/nkE7hncBzt+ZXy7GEQoziH3wBlbf4mDAr/mg9/ak5kpDv+rrvg9tutd1J28PZyOn48tPkwJtAsUJxFli51v/zDwuDwYXj1VTeKulo1N5V19eqwa/oi1nEVVTydwg5RjHcZyu9cxJSTV/O/r8KSgoSXBYnscf/9rrRXt26oc2JMYFmgyAY7drhGzvLlU197ISYGWrVy24mJpw/gioyE6X0+48Fpg9lNee7kfY41iGLUr3X46vIy/PGHa5+++upg3olJyzXX+ObSMiY3sTaKIIuNdSvANWiQ9nQXY8bAiRNuUrnrrz99f89lT1Pp0ZtYoC0Ze9di3jhyJx8sbomULcOAAe6YceNcacQYYwJJNAf/BIqKitJF/nNYn4VmzYJOndx2jRqwaZMLCH/+CW3auPTDh+Gcc6BqVVi92v0qjYuDDRtgQJMl3Mcw+vE5Y7iZwYxkzbp81K7te4+TJ2H9eqhfP9tvzxiTA4nIYlWNSu/x9vsziH780QWJAgXc9NoJCbBsmRux27YtzJzpjvvkE7fgzauvukV9wvbsouiY96g/6EKW0IyeTOV1HuLziz7i9wXJgwS4XjYWJIwxwWKBIgg2b4b//nNf/GXLwrRp7ot861bXOO3VpQscPAhffw1RdQ5yecxYl1ipEtxzD/mPH2Hz7a8RvvM/Ll74Ol9OKUDLliG6KWNMnmWN2QG2bRvUrOl7/fjj0LUrbNniSyvMUU6Sn/LsYeMbf3D3nPFcJtMJG3jcnfzYY9C3LzRoQA3POS0qZuddGGOMjwWKAImLcyWHPn18acWLw/2DYmH0VwxcuZKoiD3sW72TDjKHcO8MJS/BuVQk9obbKHv39dCiBRmextUYY4LIAkUAbN0KDRu6Hk61a8PGjVA0MZavIt6ibNPhcOAA4UWL0rRiRY43LEVCu7uISSjB8yMqsIbzmcPFxH9uH4Ux5uxk304BMG6cCxIREa4B+/U203jm31sotzAGevaEBx6A1q0hLAzvzNMVgMPH4Jdx0KFDKHNvjDFps8bsLPrlF3jqKbjoIli1+BhVx77Ae/9eSVyFGuhfi2DKFLczhQEOI0bAmjUwY0YIMm6MMelkJYrMUIV//oHDh1lz3yKW8z414s+BKkvdEOvrrqPqxx/7FpZORcGCUK9eNuXZGGMyyQJFBpw4Afv2JlL+4ZsJ/+JTAO4EjhQoTdF/PXNwDB0KnTtbg7QxJtewQJGGkydh5Ej3y79nT2jV6AjDdvSmB9NZ3qQfB0rV4LVfW3LVO1259Xb7UxpjcqdgrnA3BugB7FbVhp60N4DLgRPABuBmVT0gIjWAaGCt5/QFqjokWHlLy5o1rkDw6qvQr58v/Y1nj/DNjuZEEM1DvM5bfz+AEkbBgvD1oFDk1BhjskcwG7PHAt1OSZsJNFTVxsA/wGN++zaoaqTnke1Bwjtqun592L7dFyRur/EDz/E0w7dfTQTR8NVXvBD3EOr50y1YYAvVGGNyt2AuhTrXU1LwT/vJ7+UC4NpgvX9GxMVBu3Zu6g2vofcod598m/M+fDApLf7RJ8l37bUUAqKj4cABTlv7wRhjcptQVqzfAkz0e11TRJYCscCTqvpbdmVk1CgXJO6/H16+ZT3533yFsLGT3OCIa66Bt9+GnTvJ17x50jnnn59duTPGmNAKSaAQkSeAeOALT9IOoJqqxohIM+BrEWmgqrEpnDsYGAxQrVq1LOfl0CF44QVo3x7e7Pc30uZil9i5M/To4RaRCAtzy8wZY0welO2BQkQG4Bq5O6lnMQxVPQ4c92wvFpENQF3gtMUmVHUkMBLcehSZyYMqLFnilqy84QY39GH4zUuRdu2hWDFYuNDWszTGGI9sDRQi0g14GGivqkf90ssD+1Q1QURqAXWAjcHKx+zZvsWEAIa9doLGb/V3A+QWLIAqVYL11sYYk+MEs3vsBOBioJyIbAOewfVyKgjMFDcgzdsNth3wvIicBBKBIaq6L1h5864sB/DmG8q9S2+GFSvgm28sSBhjzCmC2eupbwrJo1M5djIwOVh5OVWBAjB/vusSe92m12H8eHj+ebjiiuzKgjHG5Bh5djhx69bQ+sjP0Ocx6N4dHn001FkyxpizUt6dPXbDBrjqKtdo/eWXNmrOGGNSkTcDxaJF0LKlCw4zZril6IwxxqQobwaKKlXgwgvh22+hevVQ58YYY85qebON4pxz3ALXxhhjzihvliiMMcakmwUKY4wxabJAYYwxJk0WKIwxxqTJAoUxxpg0WaAwxhiTJgsUxhhj0mSBwhhjTJrEs3ZQjiQie4AtWbhEOWBvgLKTk9h95z159d7z6n1D2vdeXVXLp/dCOTpQZJWILFLVqFDnI7vZfec9efXe8+p9Q2Dv3aqejDHGpMkChTHGmDTl9UAxMtQZCBG777wnr957Xr1vCOC95+k2CmOMMWeW10sUxhhjzsAChTHGmDTlyUAhIt1EZK2IrBeRR0Odn0ASkaoiMltEVovIKhEZ6kkvIyIzRWSd57m0J11EZLjnb7FcRJqG9g6yRkTCRWSpiHzneV1TRBZ67m+iiBTwpBf0vF7v2V8jpBnPIhEpJSKTRGSNiESLSOs89Jnf5/m3vlJEJohIodz4uYvIGBHZLSIr/dIy/BmLSH/P8etEpH963jvPBQoRCQf+D7gUiAD6ikhEaHMVUPHAA6oaAbQC7vTc36PAL6paB/jF8xrc36GO5zEY+DD7sxxQQ4Fov9evAcNUtTawHxjoSR8I7PekD/Mcl5O9C/yoqucDTXB/g1z/mYtIZeAeIEpVGwLhQB9y5+c+Fuh2SlqGPmMRKQM8A7QEWgDPeINLmlQ1Tz2A1sAMv9ePAY+FOl9BvN9vgEuAtcC5nrRzgbWe7RFAX7/jk47LaQ+giuc/S0fgO0BwI1PznfrZAzOA1p7tfJ7jJNT3kMn7LglsOjX/eeQzrwxsBcp4PsfvgK659XMHagArM/sZA32BEX7pyY5L7ZHnShT4/mF5bfOk5TqeYvUFwEKgoqru8OzaCVT0bOemv8c7wMNAoud1WeCAqsZ7XvvfW9J9e/Yf9ByfE9UE9gCfeKrdRolIUfLAZ66q24E3gX+BHbjPcTF543OHjH/Gmfrs82KgyBNEpBgwGbhXVWP996n7KZGr+kWLSA9gt6ouDnVeQiAf0BT4UFUvAI7gq4IAcudnDuCpNrkSFywrAUU5vXomTwjmZ5wXA8V2oKrf6yqetFxDRPLjgsQXqjrFk7xLRM717D8X2O1Jzy1/j4uAK0RkM/AlrvrpXaCUiOTzHON/b0n37dlfEojJzgwH0DZgm6ou9LyehAscuf0zB+gMbFLVPap6EpiC+7eQFz53yPhnnKnPPi8Gir+AOp5eEQVwDV/TQpyngBERAUYD0ar6tt+uaYC3h0N/XNuFN/0mTy+JVsBBv6JsjqGqj6lqFVWtgftMZ6nqDcBs4FrPYafet/fvca3n+Bz5i1tVdwJbRaSeJ6kTsJpc/pl7/Au0EpEinn/73nvP9Z+7R0Y/4xlAFxEp7SmNdfGkpS3UjTMhahDqDvwDbACeCHV+AnxvbXDFz+XAMs+jO64e9hdgHfAzUMZzvOB6gW0AVuB6j4T8PrL4N7gY+M6zXQv4E1gPfAUU9KQX8rxe79lfK9T5zuI9RwKLPJ/710DpvPKZA88Ba4CVwGdAwdz4uQMTcO0wJ3GlyIGZ+YyBWzz3vx64OT3vbVN4GGOMSVNerHoyxhiTARYojDHGpMkChTHGmDRZoDDGGJMmCxTGGGPSlO/MhxhjAEQkAdfVMD9u8sVPcRPPJaZ5ojE5nAUKY9IvTlUjAUSkAjAeKIGbjdOYXMuqnozJBFXdjZu++S7P6NcaIvKbiCzxPC4EEJFPReQq73ki8oWIXCkiDUTkTxFZ5lkvoE6IbsWYM7IBd8akk4gcVtVip6QdAOoBh4BEVT3m+dKfoKpRItIeuE9VrxKRkriR8nVwayEsUNUvPFPJhKtqXHbejzHpZVVPxgRGfuB9EYkEEoC6AKr6q4h8ICLlgWuAyaoaLyJ/AE+ISBVgiqquC1XGjTkTq3oyJpNEpBYuKOwG7gN24VaXiwIK+B36KXAjcDMwBkBVxwNXAHHA9yLSMftybkzGWInCmEzwlBA+At5XVfVUK21T1UTPOsThfoePxU1At1NVV3vOrwVsVNXhIlINaAzMytabMCadLFAYk36FRWQZvu6xnwHeqdw/ACaLyE3Aj7jFgwBQ1V0iEo2b1dXrOqCfiJzErUz2ctBzb0wmWWO2MUEmIkVw4y+aqurBUOfHmIyyNgpjgkhEOgPRwHsWJExOZSUKY4wxabIShTHGmDRZoDDGGJMmCxTGGGPSZIHCGGNMmixQGGOMSdP/A274rByiOOf+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
